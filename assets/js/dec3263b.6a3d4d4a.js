"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[6243],{28453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var l=i(96540);const r={},t=l.createContext(r);function s(e){const n=l.useContext(t);return l.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),l.createElement(t.Provider,{value:n},e.children)}},29651:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>l,toc:()=>d});const l=JSON.parse('{"id":"blueprints/inference/Neuron/llama4-trn2","title":"Llama 4 with vLLM on Trainium2","description":"This guide demonstrates deploying Llama 4 models using vLLM with NxD Inference on AWS Trainium2 instances.","source":"@site/docs/blueprints/inference/Neuron/llama4-trn2.md","sourceDirName":"blueprints/inference/Neuron","slug":"/blueprints/inference/Neuron/llama4-trn2","permalink":"/ai-on-eks/docs/blueprints/inference/Neuron/llama4-trn2","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/blueprints/inference/Neuron/llama4-trn2.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"title":"Llama 4 with vLLM on Trainium2","sidebar_position":7},"sidebar":"blueprints","previous":{"title":"Ray Serve High Availability","permalink":"/ai-on-eks/docs/blueprints/inference/Neuron/rayserve-ha"},"next":{"title":"Overview","permalink":"/ai-on-eks/docs/blueprints/inference/"}}');var r=i(74848),t=i(28453),s=i(42450);const a={title:"Llama 4 with vLLM on Trainium2",sidebar_position:7},o="Llama 4 with vLLM on Amazon EKS using Trainium2",c={},d=[{value:"Understanding Trainium2 Requirements",id:"understanding-trainium2-requirements",level:2},{value:"Model Memory Requirements",id:"model-memory-requirements",level:3},{value:"Trainium2 Instance Specifications",id:"trainium2-instance-specifications",level:3},{value:"Model Compilation (Tracing) Requirement",id:"model-compilation-tracing-requirement",level:3},{value:"Prerequisites",id:"prerequisites",level:3},{value:"EKS Cluster Requirements",id:"eks-cluster-requirements",level:3},{value:"Create EKS Cluster with Trainium2 Support",id:"create-eks-cluster-with-trainium2-support",level:3},{value:"Install Neuron Device Plugin",id:"install-neuron-device-plugin",level:3},{value:"Verify Neuron Devices",id:"verify-neuron-devices",level:3},{value:"Option 1: Compile on a Trainium2 Instance",id:"option-1-compile-on-a-trainium2-instance",level:3},{value:"Option 2: Use Pre-compiled Artifacts",id:"option-2-use-pre-compiled-artifacts",level:3},{value:"Neuron Configuration for Compilation",id:"neuron-configuration-for-compilation",level:3},{value:"Deploying Llama 4 Scout with Helm",id:"deploying-llama-4-scout-with-helm",level:2},{value:"Deploying Llama 4 Maverick",id:"deploying-llama-4-maverick",level:2},{value:"Testing the Deployment",id:"testing-the-deployment",level:2},{value:"Text Completion",id:"text-completion",level:3},{value:"Multimodal (Image + Text)",id:"multimodal-image--text",level:3},{value:"Multiple Images",id:"multiple-images",level:3},{value:"Helm Values Configuration",id:"helm-values-configuration",level:2},{value:"Monitoring",id:"monitoring",level:2},{value:"Check Pod Logs",id:"check-pod-logs",level:3},{value:"Check Neuron Device Utilization",id:"check-neuron-device-utilization",level:3},{value:"Cleanup",id:"cleanup",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"References",id:"references",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"llama-4-with-vllm-on-amazon-eks-using-trainium2",children:"Llama 4 with vLLM on Amazon EKS using Trainium2"})}),"\n",(0,r.jsxs)(n.p,{children:["This guide demonstrates deploying ",(0,r.jsx)(n.a,{href:"https://huggingface.co/meta-llama",children:"Llama 4"})," models using ",(0,r.jsx)(n.a,{href:"https://github.com/vllm-project/vllm",children:"vLLM"})," with ",(0,r.jsx)(n.a,{href:"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/",children:"NxD Inference"})," on ",(0,r.jsx)(n.a,{href:"https://aws.amazon.com/machine-learning/trainium/",children:"AWS Trainium2"})," instances."]}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsxs)(n.p,{children:["This blueprint uses ",(0,r.jsx)(n.strong,{children:"AWS Trainium2 (trn2)"})," instances with the Neuron SDK for cost-effective inference. Llama 4 models support both text and image inputs (multimodal)."]})}),"\n",(0,r.jsx)(n.h2,{id:"understanding-trainium2-requirements",children:"Understanding Trainium2 Requirements"}),"\n",(0,r.jsx)(n.p,{children:"Llama 4 models use a Mixture of Experts (MoE) architecture that requires significant compute resources. Trainium2 provides excellent price-performance for these large models."}),"\n",(0,r.jsx)(n.h3,{id:"model-memory-requirements",children:"Model Memory Requirements"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"Active Params"}),(0,r.jsx)(n.th,{children:"Total Params"}),(0,r.jsx)(n.th,{children:"Experts"}),(0,r.jsx)(n.th,{children:"BF16 Memory"}),(0,r.jsx)(n.th,{children:"Instance Required"}),(0,r.jsx)(n.th,{children:"tensor_parallel_size"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Llama 4 Scout"}),(0,r.jsx)(n.td,{children:"17B"}),(0,r.jsx)(n.td,{children:"~109B"}),(0,r.jsx)(n.td,{children:"16"}),(0,r.jsx)(n.td,{children:"~220 GiB"}),(0,r.jsx)(n.td,{children:"trn2.48xlarge"}),(0,r.jsx)(n.td,{children:"64"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Llama 4 Maverick"}),(0,r.jsx)(n.td,{children:"17B"}),(0,r.jsx)(n.td,{children:"~400B"}),(0,r.jsx)(n.td,{children:"128"}),(0,r.jsx)(n.td,{children:"~800 GiB"}),(0,r.jsx)(n.td,{children:"trn2.48xlarge"}),(0,r.jsx)(n.td,{children:"64"})]})]})]}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsxs)(n.p,{children:["Unlike GPU deployments, Trainium2 uses the ",(0,r.jsx)(n.strong,{children:"original BF16/FP16 models"})," without quantization. The Neuron SDK efficiently manages memory across all 64 Neuron cores."]})}),"\n",(0,r.jsx)(n.h3,{id:"trainium2-instance-specifications",children:"Trainium2 Instance Specifications"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Instance Type"}),(0,r.jsx)(n.th,{children:"Neuron Devices"}),(0,r.jsx)(n.th,{children:"Neuron Cores"}),(0,r.jsx)(n.th,{children:"HBM Memory"}),(0,r.jsx)(n.th,{children:"Use Case"})]})}),(0,r.jsx)(n.tbody,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"trn2.48xlarge"}),(0,r.jsx)(n.td,{children:"32"}),(0,r.jsx)(n.td,{children:"64"}),(0,r.jsx)(n.td,{children:"1.5 TiB"}),(0,r.jsx)(n.td,{children:"Scout & Maverick (BF16)"})]})})]}),"\n",(0,r.jsx)(n.admonition,{type:"warning",children:(0,r.jsxs)(n.p,{children:["Llama 4 models require ",(0,r.jsx)(n.code,{children:"tensor_parallel_size=64"})," which means you need a full trn2.48xlarge instance with all 64 Neuron cores. The 1.5 TiB HBM memory is sufficient for both Scout and Maverick models in BF16 precision."]})}),"\n",(0,r.jsx)(n.h3,{id:"model-compilation-tracing-requirement",children:"Model Compilation (Tracing) Requirement"}),"\n",(0,r.jsxs)(n.admonition,{title:"Important",type:"danger",children:[(0,r.jsxs)(n.p,{children:["Before deploying Llama 4 on Trainium2, the model must be ",(0,r.jsx)(n.strong,{children:"pre-compiled (traced)"})," for Neuron. This is a one-time process that converts the model weights to Neuron-optimized format."]}),(0,r.jsxs)(n.p,{children:["The compiled artifacts must be stored in a location accessible to the deployment (e.g., S3 bucket or EFS volume) and referenced via the ",(0,r.jsx)(n.code,{children:"NEURON_COMPILED_ARTIFACTS"})," environment variable."]}),(0,r.jsxs)(n.p,{children:["See the ",(0,r.jsx)(n.a,{href:"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/tutorials/llama4-tutorial.html",children:"NxD Inference Llama 4 Tutorial"})," for detailed compilation instructions."]})]}),"\n",(0,r.jsxs)(s.A,{header:(0,r.jsx)(n.h2,{children:(0,r.jsx)(n.span,{children:"Prerequisites and EKS Cluster Setup"})}),children:[(0,r.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"aws cli"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://helm.sh/docs/intro/install/",children:"Helm"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://eksctl.io/installation/",children:"eksctl"})}),"\n"]}),(0,r.jsx)(n.h3,{id:"eks-cluster-requirements",children:"EKS Cluster Requirements"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"EKS Version"}),": >= 1.30"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Trainium2 Node Group"}),": trn2.48xlarge instances"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Neuron Device Plugin"}),": Installed and configured"]}),"\n"]}),(0,r.jsx)(n.h3,{id:"create-eks-cluster-with-trainium2-support",children:"Create EKS Cluster with Trainium2 Support"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"eksctl create cluster \\\n  --name llama4-trn2-cluster \\\n  --region us-east-1 \\\n  --node-type trn2.48xlarge \\\n  --nodes 1 \\\n  --nodes-min 0 \\\n  --nodes-max 2\n"})}),(0,r.jsx)(n.h3,{id:"install-neuron-device-plugin",children:"Install Neuron Device Plugin"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f https://raw.githubusercontent.com/aws-neuron/aws-neuron-sdk/master/src/k8/k8s-neuron-device-plugin.yml\nkubectl apply -f https://raw.githubusercontent.com/aws-neuron/aws-neuron-sdk/master/src/k8/k8s-neuron-scheduler-eks.yml\n"})}),(0,r.jsx)(n.h3,{id:"verify-neuron-devices",children:"Verify Neuron Devices"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl get nodes -o json | jq '.items[].status.allocatable[\"aws.amazon.com/neuron\"]'\n"})}),(0,r.jsx)(n.p,{children:"Expected output for trn2.48xlarge:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-text",children:'"32"\n'})})]}),"\n",(0,r.jsxs)(s.A,{header:(0,r.jsx)(n.h2,{children:(0,r.jsx)(n.span,{children:"Model Compilation (Required First Step)"})}),children:[(0,r.jsx)(n.p,{children:"Before deploying, you must compile the Llama 4 model for Neuron. This process creates optimized artifacts that can be reused across deployments."}),(0,r.jsx)(n.h3,{id:"option-1-compile-on-a-trainium2-instance",children:"Option 1: Compile on a Trainium2 Instance"}),(0,r.jsx)(n.p,{children:"SSH into a trn2.48xlarge instance with the Neuron SDK installed:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Activate Neuron virtual environment\nsource /opt/aws_neuronx_venv_pytorch/bin/activate\n\n# Install vLLM-Neuron plugin\npip install vllm-neuron --upgrade\n\n# Set compilation output directory\nexport NEURON_COMPILED_ARTIFACTS="/home/ubuntu/llama4/traced_models/Llama-4-Scout-17B-16E-Instruct"\n\n# Run vLLM to trigger compilation (this takes 30-60 minutes)\npython3 -m vllm.entrypoints.openai.api_server \\\n  --model "meta-llama/Llama-4-Scout-17B-16E-Instruct" \\\n  --max-num-seqs 1 \\\n  --max-model-len 16384 \\\n  --tensor-parallel-size 64 \\\n  --port 8000\n'})}),(0,r.jsx)(n.h3,{id:"option-2-use-pre-compiled-artifacts",children:"Option 2: Use Pre-compiled Artifacts"}),(0,r.jsx)(n.p,{children:"If you have access to pre-compiled artifacts, upload them to S3:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"aws s3 sync /home/ubuntu/llama4/traced_models/ s3://your-bucket/llama4-neuron-artifacts/\n"})}),(0,r.jsx)(n.h3,{id:"neuron-configuration-for-compilation",children:"Neuron Configuration for Compilation"}),(0,r.jsx)(n.p,{children:"For optimal performance, use this configuration during compilation:"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'scout_neuron_config = {\n    "text_config": {\n        "batch_size": 1,\n        "is_continuous_batching": True,\n        "seq_len": 16384,\n        "enable_bucketing": True,\n        "context_encoding_buckets": [256, 512, 1024, 2048, 4096, 8192, 10240, 16384],\n        "token_generation_buckets": [256, 512, 1024, 2048, 4096, 8192, 10240, 16384],\n        "torch_dtype": "float16",\n        "async_mode": True,\n        "world_size": 64,\n        "tp_degree": 64,\n        "cp_degree": 16\n    },\n    "vision_config": {\n        "batch_size": 1,\n        "seq_len": 8192,\n        "torch_dtype": "float16",\n        "tp_degree": 16,\n        "dp_degree": 4,\n        "world_size": 64\n    }\n}\n'})})]}),"\n",(0,r.jsx)(n.h2,{id:"deploying-llama-4-scout-with-helm",children:"Deploying Llama 4 Scout with Helm"}),"\n",(0,r.jsx)(n.admonition,{type:"caution",children:(0,r.jsxs)(n.p,{children:["The use of ",(0,r.jsx)(n.a,{href:"https://huggingface.co/meta-llama",children:"Llama 4"})," models requires access through a Hugging Face account. Make sure you have accepted the model license on HuggingFace."]})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Step 1:"})," Add the AI on EKS Helm repository"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"helm repo add ai-on-eks https://awslabs.github.io/ai-on-eks-charts\nhelm repo update\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Step 2:"})," Create a Kubernetes secret for Hugging Face token"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'kubectl create secret generic hf-token \\\n  --from-literal=hf-token=$(echo -n "Your-Hugging-Face-Hub-Token-Value" | base64)\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Step 3:"})," Deploy Llama 4 Scout using Helm"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'helm install llama4-scout ai-on-eks/inference-charts \\\n  -f https://raw.githubusercontent.com/awslabs/ai-on-eks-charts/main/charts/inference-charts/values-llama-4-scout-17b-vllm-neuron.yaml \\\n  --set inference.modelServer.env.NEURON_COMPILED_ARTIFACTS="s3://your-bucket/llama4-neuron-artifacts/Llama-4-Scout-17B-16E-Instruct"\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Step 4:"})," Monitor the deployment"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -w\n"})}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsx)(n.p,{children:"With pre-compiled artifacts, the deployment should be ready in 10-15 minutes. Without pre-compiled artifacts, the first deployment will trigger compilation which takes 30-60 minutes."})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-text",children:"NAME                                      READY   STATUS    RESTARTS   AGE\nllama-4-scout-17b-vllm-nrn-xxxxx-xxxxx    1/1     Running   0          15m\n"})}),"\n",(0,r.jsx)(n.h2,{id:"deploying-llama-4-maverick",children:"Deploying Llama 4 Maverick"}),"\n",(0,r.jsx)(n.p,{children:"For the larger Maverick model with 128 experts:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'helm install llama4-maverick ai-on-eks/inference-charts \\\n  -f https://raw.githubusercontent.com/awslabs/ai-on-eks-charts/main/charts/inference-charts/values-llama-4-maverick-17b-vllm-neuron.yaml \\\n  --set inference.modelServer.env.NEURON_COMPILED_ARTIFACTS="s3://your-bucket/llama4-neuron-artifacts/Llama-4-Maverick-17B-128E-Instruct"\n'})}),"\n",(0,r.jsx)(n.admonition,{type:"warning",children:(0,r.jsx)(n.p,{children:"Maverick model compilation takes significantly longer (60-90 minutes) due to the larger number of experts."})}),"\n",(0,r.jsx)(n.h2,{id:"testing-the-deployment",children:"Testing the Deployment"}),"\n",(0,r.jsx)(n.h3,{id:"text-completion",children:"Text Completion"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward svc/llama-4-scout-17b-vllm-nrn 8000:8000\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",\n    "messages": [{"role": "user", "content": "What is Amazon EKS?"}],\n    "max_tokens": 100\n  }\'\n'})}),"\n",(0,r.jsx)(n.h3,{id:"multimodal-image--text",children:"Multimodal (Image + Text)"}),"\n",(0,r.jsx)(n.p,{children:"Llama 4 supports multimodal inputs with up to 5 images per prompt:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",\n    "messages": [{\n      "role": "user",\n      "content": [\n        {"type": "image_url", "image_url": {"url": "https://httpbin.org/image/png"}},\n        {"type": "text", "text": "Describe this image in detail"}\n      ]\n    }]\n  }\'\n'})}),"\n",(0,r.jsx)(n.h3,{id:"multiple-images",children:"Multiple Images"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",\n    "messages": [{\n      "role": "user",\n      "content": [\n        {"type": "image_url", "image_url": {"url": "https://httpbin.org/image/png"}},\n        {"type": "image_url", "image_url": {"url": "https://httpbin.org/image/png"}},\n        {"type": "text", "text": "Compare these two images"}\n      ]\n    }]\n  }\'\n'})}),"\n",(0,r.jsx)(n.h2,{id:"helm-values-configuration",children:"Helm Values Configuration"}),"\n",(0,r.jsx)(n.p,{children:"The Helm chart uses the following key configuration for Trainium2 deployments:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'# values-llama-4-scout-17b-vllm-neuron.yaml\nmodel: meta-llama/Llama-4-Scout-17B-16E-Instruct\n\nmodelParameters:\n  maxModelLen: 16384\n  tensorParallelSize: 64\n  maxNumSeqs: 1\n\ninference:\n  serviceName: llama-4-scout-17b-vllm-nrn\n  accelerator: neuron\n  framework: vllm\n\n  modelServer:\n    image:\n      repository: public.ecr.aws/neuron/pytorch-inference-neuronx\n      tag: 2.5.1-neuronx-py310-sdk2.21.0-ubuntu22.04\n    deployment:\n      resources:\n        neuron:\n          requests:\n            aws.amazon.com/neuron: 32\n            memory: 512Gi\n          limits:\n            aws.amazon.com/neuron: 32\n            memory: 768Gi\n      nodeSelector:\n        node.kubernetes.io/instance-type: trn2.48xlarge\n    env:\n      VLLM_USE_V1: "1"\n      NEURON_COMPILED_ARTIFACTS: ""  # Set via --set flag\n'})}),"\n",(0,r.jsx)(n.h2,{id:"monitoring",children:"Monitoring"}),"\n",(0,r.jsx)(n.h3,{id:"check-pod-logs",children:"Check Pod Logs"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl logs -l app=llama-4-scout-17b-vllm-nrn -f\n"})}),"\n",(0,r.jsx)(n.h3,{id:"check-neuron-device-utilization",children:"Check Neuron Device Utilization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl exec -it $(kubectl get pods -l app=llama-4-scout-17b-vllm-nrn -o jsonpath='{.items[0].metadata.name}') -- neuron-top\n"})}),"\n",(0,r.jsx)(n.h2,{id:"cleanup",children:"Cleanup"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"helm uninstall llama4-scout\n# Or for Maverick:\nhelm uninstall llama4-maverick\n"})}),"\n",(0,r.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Pre-compilation Required"}),": Unlike GPU deployments, Trainium2 requires model compilation before deployment."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cost-Effective Inference"}),": Trainium2 provides excellent price-performance for large MoE models like Llama 4."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"No Quantization Needed"}),": Trainium2's 1.5 TiB HBM memory supports full BF16 precision for both Scout and Maverick."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Multimodal Support"}),": Llama 4 on Trainium2 supports both text and image inputs."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Helm-based Deployment"}),": Use the AI on EKS inference charts for standardized, reproducible deployments."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/tutorials/llama4-tutorial.html",children:"NxD Inference Llama 4 Tutorial"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/awslabs/ai-on-eks-charts",children:"AI on EKS Inference Charts"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/aws-neuron/vllm-neuron",children:"vLLM-Neuron Plugin"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://awsdocs-neuron.readthedocs-hosted.com/",children:"AWS Neuron SDK Documentation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://huggingface.co/meta-llama",children:"Llama 4 on Hugging Face"})}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},42450:(e,n,i)=>{i.d(n,{A:()=>p});var l=i(96540),r=i(5556),t=i.n(r),s=i(34164);const a="collapsibleContent_q3kw",o="header_QCEw",c="icon_PckA",d="content_qLC1",h="expanded_iGsi";var u=i(74848);function m({children:e,header:n}){const[i,r]=(0,l.useState)(!1);return(0,u.jsxs)("div",{className:a,children:[(0,u.jsxs)("div",{className:(0,s.A)(o,{[h]:i}),onClick:()=>{r(!i)},children:[n,(0,u.jsx)("span",{className:(0,s.A)(c,{[h]:i}),children:i?"\ud83d\udc47":"\ud83d\udc48"})]}),i&&(0,u.jsx)("div",{className:d,children:e})]})}m.propTypes={children:t().node.isRequired,header:t().node.isRequired};const p=m}}]);
"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[693],{28453:(e,n,l)=>{l.d(n,{R:()=>r,x:()=>a});var s=l(96540);const i={},t=s.createContext(i);function r(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(t.Provider,{value:n},e.children)}},42450:(e,n,l)=>{l.d(n,{A:()=>u});var s=l(96540),i=l(5556),t=l.n(i),r=l(34164);const a="collapsibleContent_q3kw",o="header_QCEw",c="icon_PckA",d="content_qLC1",h="expanded_iGsi";var m=l(74848);function p({children:e,header:n}){const[l,i]=(0,s.useState)(!1);return(0,m.jsxs)("div",{className:a,children:[(0,m.jsxs)("div",{className:(0,r.A)(o,{[h]:l}),onClick:()=>{i(!l)},children:[n,(0,m.jsx)("span",{className:(0,r.A)(c,{[h]:l}),children:l?"\ud83d\udc47":"\ud83d\udc48"})]}),l&&(0,m.jsx)("div",{className:d,children:e})]})}p.propTypes={children:t().node.isRequired,header:t().node.isRequired};const u=p},63278:(e,n,l)=>{l.r(n),l.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>a,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"blueprints/inference/GPUs/llama4-vllm","title":"Llama 4 with vLLM on EKS","description":"In this guide, we\'ll explore deploying Llama 4 models using vLLM inference engine on Amazon EKS with EKS Auto Mode for automatic GPU node provisioning.","source":"@site/docs/blueprints/inference/GPUs/llama4-vllm.md","sourceDirName":"blueprints/inference/GPUs","slug":"/blueprints/inference/GPUs/llama4-vllm","permalink":"/ai-on-eks/docs/blueprints/inference/GPUs/llama4-vllm","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/blueprints/inference/GPUs/llama4-vllm.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"Llama 4 with vLLM on EKS","sidebar_position":6},"sidebar":"blueprints","previous":{"title":"DeepSeek-R1 on EKS","permalink":"/ai-on-eks/docs/blueprints/inference/GPUs/ray-vllm-deepseek"},"next":{"title":"NVIDIA Dynamo on Amazon EKS","permalink":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo"}}');var i=l(74848),t=l(28453),r=l(42450);const a={title:"Llama 4 with vLLM on EKS",sidebar_position:6},o="Llama 4 with vLLM on Amazon EKS",c={},d=[{value:"Understanding the GPU Memory Requirements",id:"understanding-the-gpu-memory-requirements",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"EKS Cluster Requirements",id:"eks-cluster-requirements",level:3},{value:"Verify EKS Cluster Environment",id:"verify-eks-cluster-environment",level:3},{value:"Deploying Llama 4 8B with vLLM",id:"deploying-llama-4-8b-with-vllm",level:2},{value:"Testing the Llama 4 Model",id:"testing-the-llama-4-model",level:2},{value:"Deploy Open WebUI",id:"deploy-open-webui",level:2},{value:"Monitoring and Observability",id:"monitoring-and-observability",level:2},{value:"Check vLLM Pod Logs",id:"check-vllm-pod-logs",level:3},{value:"Check GPU Utilization",id:"check-gpu-utilization",level:3},{value:"Performance Metrics",id:"performance-metrics",level:3},{value:"Deploying Llama 4 70B (Multi-GPU)",id:"deploying-llama-4-70b-multi-gpu",level:2},{value:"Cleanup",id:"cleanup",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"llama-4-with-vllm-on-amazon-eks",children:"Llama 4 with vLLM on Amazon EKS"})}),"\n",(0,i.jsxs)(n.p,{children:["In this guide, we'll explore deploying ",(0,i.jsx)(n.a,{href:"https://huggingface.co/meta-llama",children:"Llama 4"})," models using ",(0,i.jsx)(n.a,{href:"https://github.com/vllm-project/vllm",children:"vLLM"})," inference engine on ",(0,i.jsx)(n.a,{href:"https://aws.amazon.com/eks/",children:"Amazon EKS"})," with EKS Auto Mode for automatic GPU node provisioning."]}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["This blueprint uses ",(0,i.jsx)(n.strong,{children:"EKS Auto Mode"})," for automatic GPU node provisioning. When you deploy a GPU workload, EKS automatically provisions the appropriate GPU nodes without requiring Karpenter or manual node group configuration."]})}),"\n",(0,i.jsx)(n.h2,{id:"understanding-the-gpu-memory-requirements",children:"Understanding the GPU Memory Requirements"}),"\n",(0,i.jsxs)(n.p,{children:["Deploying Llama 4 models requires careful memory planning. Each model parameter typically consumes 2 bytes (",(0,i.jsx)(n.code,{children:"BF16"})," precision). Below are the memory requirements for different model variants:"]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{children:"Parameters"}),(0,i.jsx)(n.th,{children:"BF16 Memory"}),(0,i.jsx)(n.th,{children:"Recommended GPU"}),(0,i.jsx)(n.th,{children:"tensor_parallel_size"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Llama 4 8B"}),(0,i.jsx)(n.td,{children:"8B"}),(0,i.jsx)(n.td,{children:"~16 GiB"}),(0,i.jsx)(n.td,{children:"A10G (24GB), L4 (24GB)"}),(0,i.jsx)(n.td,{children:"1"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Llama 4 70B"}),(0,i.jsx)(n.td,{children:"70B"}),(0,i.jsx)(n.td,{children:"~140 GiB"}),(0,i.jsx)(n.td,{children:"8x A10G or 2x A100 (80GB)"}),(0,i.jsx)(n.td,{children:"8 or 2"})]})]})]}),"\n",(0,i.jsxs)(n.p,{children:["Using vLLM with ",(0,i.jsx)(n.code,{children:"gpu-memory-utilization=0.9"}),", we optimize memory usage while preventing out-of-memory (OOM) crashes."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Expected log output during model loading:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-log",children:"INFO model_runner.py:1115] Loading model weights took 14.99 GiB\nINFO worker.py:266] vLLM instance can use total GPU memory (22.30 GiB) x utilization (0.90) = 20.07 GiB\nINFO worker.py:266] Model weights: 14.99 GiB | Activation memory: 0.85 GiB | KV Cache: 4.17 GiB\n"})}),"\n",(0,i.jsxs)(r.A,{header:(0,i.jsx)(n.h2,{children:(0,i.jsx)(n.span,{children:"Prerequisites and EKS Cluster Setup"})}),children:[(0,i.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),(0,i.jsx)(n.p,{children:"Before deploying Llama 4, ensure you have the following tools installed:"}),(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["To simplify the demo process, we assume the use of an IAM role with administrative privileges. For production deployments, create an IAM role with only the necessary permissions using tools like ",(0,i.jsx)(n.a,{href:"https://aws.amazon.com/iam/access-analyzer/",children:"IAM Access Analyzer"}),"."]})}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"aws cli"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://pypi.org/project/envsubst/",children:"envsubst"})}),"\n"]}),(0,i.jsx)(n.h3,{id:"eks-cluster-requirements",children:"EKS Cluster Requirements"}),(0,i.jsx)(n.p,{children:"This blueprint requires an existing EKS cluster with the following configuration:"}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"EKS Version"}),": >= 1.30"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"EKS Auto Mode"}),": Enabled (for automatic GPU node provisioning)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"NVIDIA Device Plugin"}),": Automatically managed by EKS Auto Mode"]}),"\n"]}),(0,i.jsx)(n.h3,{id:"verify-eks-cluster-environment",children:"Verify EKS Cluster Environment"}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 1:"})," Check EKS cluster version"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl version --short\n"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"Client Version: v1.30.0\nServer Version: v1.30.0-eks-xxxxx\n"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 2:"})," Verify EKS Auto Mode is enabled"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"aws eks describe-cluster --name <cluster-name> --query 'cluster.computeConfig.enabled'\n"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"true\n"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 3:"})," Verify NVIDIA Device Plugin (Auto Mode manages this automatically)"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get daemonset -n kube-system | grep nvidia\n"})}),(0,i.jsx)(n.p,{children:"If EKS Auto Mode is enabled, the NVIDIA device plugin is automatically deployed when GPU workloads are scheduled."})]}),"\n",(0,i.jsx)(n.h2,{id:"deploying-llama-4-8b-with-vllm",children:"Deploying Llama 4 8B with vLLM"}),"\n",(0,i.jsx)(n.p,{children:"With the EKS cluster ready, we can now deploy Llama 4 8B using vLLM."}),"\n",(0,i.jsx)(n.admonition,{type:"caution",children:(0,i.jsxs)(n.p,{children:["The use of ",(0,i.jsx)(n.a,{href:"https://huggingface.co/meta-llama",children:"Llama 4"})," models requires access through a Hugging Face account. Make sure you have accepted the model license on HuggingFace."]})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 1:"})," Export the Hugging Face Hub Token"]}),"\n",(0,i.jsx)(n.p,{children:"Create a Hugging Face account and generate an access token:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Navigate to ",(0,i.jsx)(n.a,{href:"https://huggingface.co/settings/tokens",children:"Hugging Face Settings \u2192 Access Tokens"})]}),"\n",(0,i.jsx)(n.li,{children:"Create a new token with read permissions"}),"\n",(0,i.jsx)(n.li,{children:"Export the token as a base64-encoded environment variable:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'export HUGGING_FACE_HUB_TOKEN=$(echo -n "Your-Hugging-Face-Hub-Token-Value" | base64)\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 2:"})," Clone the repository"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/awslabs/ai-on-eks.git\ncd ai-on-eks\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 3:"})," Deploy the vLLM service"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd blueprints/inference/llama4-vllm-gpu/\nenvsubst < llama4-vllm-deployment.yml | kubectl apply -f -\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Output:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"namespace/llama4-vllm created\nsecret/hf-token created\ndeployment.apps/llama4-vllm created\nservice/llama4-vllm-svc created\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 4:"})," Monitor the deployment"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n llama4-vllm -w\n"})}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsx)(n.p,{children:"The first deployment may take 10-15 minutes as EKS Auto Mode provisions a GPU node and the model weights are downloaded from HuggingFace."})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"NAME                           READY   STATUS    RESTARTS   AGE\nllama4-vllm-xxxxxxxxx-xxxxx    1/1     Running   0          10m\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 5:"})," Verify the service"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get svc -n llama4-vllm\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nllama4-vllm-svc   ClusterIP   172.20.xxx.xx   <none>        8000/TCP   10m\n"})}),"\n",(0,i.jsx)(n.h2,{id:"testing-the-llama-4-model",children:"Testing the Llama 4 Model"}),"\n",(0,i.jsx)(n.p,{children:"Now it's time to test the Llama 4 chat model."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 1:"})," Port-forward the vLLM service"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl -n llama4-vllm port-forward svc/llama4-vllm-svc 8000:8000\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 2:"})," Test the /v1/models endpoint"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"curl http://localhost:8000/v1/models\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Response:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "object": "list",\n  "data": [\n    {\n      "id": "meta-llama/Llama-4-8B-Instruct",\n      "object": "model",\n      "created": 1234567890,\n      "owned_by": "vllm"\n    }\n  ]\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 3:"})," Test chat completion"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "meta-llama/Llama-4-8B-Instruct",\n    "messages": [{"role": "user", "content": "Explain what Amazon EKS is in 2 sentences."}],\n    "max_tokens": 100,\n    "stream": false\n  }\'\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Response:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "id": "chatcmpl-xxxxx",\n  "object": "chat.completion",\n  "created": 1234567890,\n  "model": "meta-llama/Llama-4-8B-Instruct",\n  "choices": [\n    {\n      "index": 0,\n      "message": {\n        "role": "assistant",\n        "content": "Amazon Elastic Kubernetes Service (EKS) is a managed container orchestration service that makes it easy to run Kubernetes on AWS without needing to install and operate your own Kubernetes control plane. It automatically manages the availability and scalability of the Kubernetes control plane nodes, handles upgrades, and integrates with other AWS services for security, networking, and monitoring."\n      },\n      "finish_reason": "stop"\n    }\n  ],\n  "usage": {\n    "prompt_tokens": 15,\n    "completion_tokens": 75,\n    "total_tokens": 90\n  }\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 4:"})," Test streaming response"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "meta-llama/Llama-4-8B-Instruct",\n    "messages": [{"role": "user", "content": "Hello!"}],\n    "stream": true\n  }\'\n'})}),"\n",(0,i.jsx)(n.h2,{id:"deploy-open-webui",children:"Deploy Open WebUI"}),"\n",(0,i.jsx)(n.p,{children:"Now, let's deploy Open WebUI, which provides a ChatGPT-style chat interface to interact with the Llama 4 model."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 1:"})," Deploy Open WebUI"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd ai-on-eks/blueprints/inference/llama4-vllm-gpu/\nkubectl apply -f open-webui.yaml\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Output:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"namespace/open-webui created\ndeployment.apps/open-webui created\nservice/open-webui created\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 2:"})," Verify the deployment"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n open-webui\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"NAME                          READY   STATUS    RESTARTS   AGE\nopen-webui-xxxxxxxxx-xxxxx    1/1     Running   0          2m\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 3:"})," Access the Open WebUI"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl -n open-webui port-forward svc/open-webui 8080:80\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Open your browser and navigate to ",(0,i.jsx)(n.a,{href:"http://localhost:8080",children:"http://localhost:8080"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 4:"})," Register and start chatting"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Sign up with your name, email, and password"}),"\n",(0,i.jsx)(n.li,{children:'Click "New Chat"'}),"\n",(0,i.jsx)(n.li,{children:"Select the Llama 4 model from the dropdown"}),"\n",(0,i.jsx)(n.li,{children:"Start chatting!"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"monitoring-and-observability",children:"Monitoring and Observability"}),"\n",(0,i.jsx)(n.h3,{id:"check-vllm-pod-logs",children:"Check vLLM Pod Logs"}),"\n",(0,i.jsx)(n.p,{children:"Monitor the vLLM server logs to check model loading status and inference metrics:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl logs -n llama4-vllm -l app=llama4-vllm -f\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key metrics to watch in logs:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Token throughput"}),": ",(0,i.jsx)(n.code,{children:"Avg prompt throughput: X tokens/s, Avg generation throughput: Y tokens/s"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"GPU KV Cache utilization"}),": ",(0,i.jsx)(n.code,{children:"GPU KV cache usage: X%"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Request processing"}),": ",(0,i.jsx)(n.code,{children:"Received request"})," and ",(0,i.jsx)(n.code,{children:"Finished request"})," entries"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"check-gpu-utilization",children:"Check GPU Utilization"}),"\n",(0,i.jsx)(n.p,{children:"If you have NVIDIA DCGM Exporter or similar monitoring tools deployed:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Check GPU memory usage on the node\nkubectl exec -n llama4-vllm -it $(kubectl get pods -n llama4-vllm -l app=llama4-vllm -o jsonpath='{.items[0].metadata.name}') -- nvidia-smi\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Expected output:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 535.xx.xx    Driver Version: 535.xx.xx    CUDA Version: 12.x     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  NVIDIA A10G         On   | 00000000:00:1E.0 Off |                    0 |\n|  0%   45C    P0    70W / 300W |  18000MiB / 24576MiB |     25%      Default |\n+-------------------------------+----------------------+----------------------+\n"})}),"\n",(0,i.jsx)(n.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,i.jsxs)(n.p,{children:["vLLM provides built-in metrics at the ",(0,i.jsx)(n.code,{children:"/metrics"})," endpoint:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"curl http://localhost:8000/metrics\n"})}),"\n",(0,i.jsx)(n.p,{children:"Key metrics include:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"vllm:num_requests_running"})," - Number of requests currently being processed"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"vllm:num_requests_waiting"})," - Number of requests waiting in queue"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"vllm:gpu_cache_usage_perc"})," - GPU KV cache utilization percentage"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"vllm:avg_generation_throughput_toks_per_s"})," - Average token generation throughput"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"deploying-llama-4-70b-multi-gpu",children:"Deploying Llama 4 70B (Multi-GPU)"}),"\n",(0,i.jsx)(n.p,{children:"For the larger 70B model, you need multiple GPUs with tensor parallelism."}),"\n",(0,i.jsx)(n.admonition,{type:"danger",children:(0,i.jsx)(n.p,{children:"Important: Deploying Llama 4 70B requires 8x A10G GPUs or 2x A100 (80GB) GPUs. This can be expensive. Ensure you monitor your usage carefully."})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 1:"})," Deploy the 70B model"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd ai-on-eks/blueprints/inference/llama4-vllm-gpu/\nenvsubst < llama4-vllm-deployment-70b.yml | kubectl apply -f -\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 2:"})," Monitor the deployment"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n llama4-vllm -l model=llama4-70b -w\n"})}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsx)(n.p,{children:"The 70B model deployment may take 20-30 minutes due to larger model weights download and multi-GPU initialization."})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 3:"})," Test the 70B model"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl -n llama4-vllm port-forward svc/llama4-vllm-70b-svc 8001:8000\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:8001/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "meta-llama/Llama-4-70B-Instruct",\n    "messages": [{"role": "user", "content": "Hello!"}]\n  }\'\n'})}),"\n",(0,i.jsx)(n.h2,{id:"cleanup",children:"Cleanup"}),"\n",(0,i.jsx)(n.p,{children:"To remove all deployed resources:"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 1:"})," Delete Open WebUI"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl delete -f open-webui.yaml\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 2:"})," Delete Llama 4 vLLM deployment"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl delete -f llama4-vllm-deployment.yml\n# Or for 70B model:\nkubectl delete -f llama4-vllm-deployment-70b.yml\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 3:"})," Delete namespaces (optional)"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl delete namespace llama4-vllm\nkubectl delete namespace open-webui\n"})}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsx)(n.p,{children:"After deleting the GPU workloads, EKS Auto Mode will automatically terminate idle GPU nodes to reduce costs."})}),"\n",(0,i.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"EKS Auto Mode Simplifies GPU Provisioning"}),": No need to configure Karpenter or manage node groups manually."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"vLLM Provides High Performance"}),": Optimized memory management with PagedAttention enables efficient inference."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"OpenAI-Compatible API"}),": Easy integration with existing tools and applications."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Scalable Architecture"}),": Support for both single-GPU (8B) and multi-GPU (70B) deployments."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Cost Optimization"}),": EKS Auto Mode automatically terminates idle GPU nodes."]}),"\n"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}}}]);
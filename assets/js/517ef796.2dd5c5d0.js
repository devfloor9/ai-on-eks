"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[693],{28453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>a});var l=s(96540);const i={},t=l.createContext(i);function r(e){const n=l.useContext(t);return l.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),l.createElement(t.Provider,{value:n},e.children)}},42450:(e,n,s)=>{s.d(n,{A:()=>u});var l=s(96540),i=s(5556),t=s.n(i),r=s(34164);const a="collapsibleContent_q3kw",o="header_QCEw",c="icon_PckA",d="content_qLC1",h="expanded_iGsi";var m=s(74848);function p({children:e,header:n}){const[s,i]=(0,l.useState)(!1);return(0,m.jsxs)("div",{className:a,children:[(0,m.jsxs)("div",{className:(0,r.A)(o,{[h]:s}),onClick:()=>{i(!s)},children:[n,(0,m.jsx)("span",{className:(0,r.A)(c,{[h]:s}),children:s?"\ud83d\udc47":"\ud83d\udc48"})]}),s&&(0,m.jsx)("div",{className:d,children:e})]})}p.propTypes={children:t().node.isRequired,header:t().node.isRequired};const u=p},63278:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>a,metadata:()=>l,toc:()=>d});const l=JSON.parse('{"id":"blueprints/inference/GPUs/llama4-vllm","title":"Llama 4 with vLLM on EKS","description":"In this guide, we\'ll explore deploying Llama 4 models using vLLM inference engine on Amazon EKS with EKS Auto Mode for automatic GPU node provisioning.","source":"@site/docs/blueprints/inference/GPUs/llama4-vllm.md","sourceDirName":"blueprints/inference/GPUs","slug":"/blueprints/inference/GPUs/llama4-vllm","permalink":"/ai-on-eks/docs/blueprints/inference/GPUs/llama4-vllm","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/blueprints/inference/GPUs/llama4-vllm.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"Llama 4 with vLLM on EKS","sidebar_position":6},"sidebar":"blueprints","previous":{"title":"DeepSeek-R1 on EKS","permalink":"/ai-on-eks/docs/blueprints/inference/GPUs/ray-vllm-deepseek"},"next":{"title":"NVIDIA Dynamo on Amazon EKS","permalink":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo"}}');var i=s(74848),t=s(28453),r=s(42450);const a={title:"Llama 4 with vLLM on EKS",sidebar_position:6},o="Llama 4 with vLLM on Amazon EKS",c={},d=[{value:"Understanding the GPU Memory Requirements",id:"understanding-the-gpu-memory-requirements",level:2},{value:"Model Memory Requirements",id:"model-memory-requirements",level:3},{value:"EC2 Instance Selection Guide",id:"ec2-instance-selection-guide",level:3},{value:"Why MoE Models Need More Memory",id:"why-moe-models-need-more-memory",level:3},{value:"Memory Optimization Options",id:"memory-optimization-options",level:3},{value:"Prerequisites",id:"prerequisites",level:3},{value:"EKS Cluster Requirements",id:"eks-cluster-requirements",level:3},{value:"Option A: Create a New EKS Auto Mode Cluster",id:"option-a-create-a-new-eks-auto-mode-cluster",level:3},{value:"Option B: Use an Existing EKS Cluster",id:"option-b-use-an-existing-eks-cluster",level:3},{value:"Step 3: Configure GPU NodePool",id:"step-3-configure-gpu-nodepool",level:3},{value:"Deploying Llama 4 Scout with vLLM",id:"deploying-llama-4-scout-with-vllm",level:2},{value:"Testing the Llama 4 Model",id:"testing-the-llama-4-model",level:2},{value:"Deploy Open WebUI",id:"deploy-open-webui",level:2},{value:"Monitoring and Observability",id:"monitoring-and-observability",level:2},{value:"Check vLLM Pod Logs",id:"check-vllm-pod-logs",level:3},{value:"Check GPU Utilization",id:"check-gpu-utilization",level:3},{value:"Performance Metrics",id:"performance-metrics",level:3},{value:"Deploying Llama 4 Maverick (Multi-GPU)",id:"deploying-llama-4-maverick-multi-gpu",level:2},{value:"Cleanup",id:"cleanup",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"llama-4-with-vllm-on-amazon-eks",children:"Llama 4 with vLLM on Amazon EKS"})}),"\n",(0,i.jsxs)(n.p,{children:["In this guide, we'll explore deploying ",(0,i.jsx)(n.a,{href:"https://huggingface.co/meta-llama",children:"Llama 4"})," models using ",(0,i.jsx)(n.a,{href:"https://github.com/vllm-project/vllm",children:"vLLM"})," inference engine on ",(0,i.jsx)(n.a,{href:"https://aws.amazon.com/eks/",children:"Amazon EKS"})," with EKS Auto Mode for automatic GPU node provisioning."]}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["This blueprint uses ",(0,i.jsx)(n.strong,{children:"EKS Auto Mode"})," for automatic GPU node provisioning. When you deploy a GPU workload, EKS automatically provisions the appropriate GPU nodes without requiring Karpenter or manual node group configuration."]})}),"\n",(0,i.jsx)(n.h2,{id:"understanding-the-gpu-memory-requirements",children:"Understanding the GPU Memory Requirements"}),"\n",(0,i.jsx)(n.p,{children:"Deploying Llama 4 models requires careful memory planning. Llama 4 uses a Mixture of Experts (MoE) architecture where all expert weights must be loaded into GPU memory, even though only a subset of experts are activated per token."}),"\n",(0,i.jsx)(n.h3,{id:"model-memory-requirements",children:"Model Memory Requirements"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{children:"Active Params"}),(0,i.jsx)(n.th,{children:"Total Params"}),(0,i.jsx)(n.th,{children:"BF16 Memory"}),(0,i.jsx)(n.th,{children:"Min GPU Config"}),(0,i.jsx)(n.th,{children:"tensor_parallel_size"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Llama 4 Scout (17B-16E)"}),(0,i.jsx)(n.td,{children:"17B"}),(0,i.jsx)(n.td,{children:"~109B"}),(0,i.jsx)(n.td,{children:"~220 GiB"}),(0,i.jsx)(n.td,{children:"8x A100 (40GB)"}),(0,i.jsx)(n.td,{children:"8"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Llama 4 Maverick (17B-128E)"}),(0,i.jsx)(n.td,{children:"17B"}),(0,i.jsx)(n.td,{children:"~400B"}),(0,i.jsx)(n.td,{children:"~800 GiB"}),(0,i.jsx)(n.td,{children:"8x H100 (80GB)"}),(0,i.jsx)(n.td,{children:"8"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"ec2-instance-selection-guide",children:"EC2 Instance Selection Guide"}),"\n",(0,i.jsx)(n.admonition,{type:"danger",children:(0,i.jsxs)(n.p,{children:["Llama 4 Scout requires ",(0,i.jsx)(n.strong,{children:"at least 220GB of GPU memory"}),". Common GPU instances like g5.48xlarge (8x A10G = 192GB) will fail with CUDA Out of Memory errors."]})}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Instance Type"}),(0,i.jsx)(n.th,{children:"GPU"}),(0,i.jsx)(n.th,{children:"GPU Memory"}),(0,i.jsx)(n.th,{children:"Total VRAM"}),(0,i.jsx)(n.th,{children:"Scout (220GB)"}),(0,i.jsx)(n.th,{children:"Maverick (800GB)"}),(0,i.jsx)(n.th,{children:"Cost/hr"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"g5.48xlarge"}),(0,i.jsx)(n.td,{children:"8x A10G"}),(0,i.jsx)(n.td,{children:"24GB each"}),(0,i.jsx)(n.td,{children:"192GB"}),(0,i.jsx)(n.td,{children:"\u274c Insufficient"}),(0,i.jsx)(n.td,{children:"\u274c Insufficient"}),(0,i.jsx)(n.td,{children:"~$16"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"p4d.24xlarge"}),(0,i.jsx)(n.td,{children:"8x A100"}),(0,i.jsx)(n.td,{children:"40GB each"}),(0,i.jsx)(n.td,{children:"320GB"}),(0,i.jsx)(n.td,{children:"\u2705 Supported"}),(0,i.jsx)(n.td,{children:"\u274c Insufficient"}),(0,i.jsx)(n.td,{children:"~$32"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"p4de.24xlarge"}),(0,i.jsx)(n.td,{children:"8x A100"}),(0,i.jsx)(n.td,{children:"80GB each"}),(0,i.jsx)(n.td,{children:"640GB"}),(0,i.jsx)(n.td,{children:"\u2705 Recommended"}),(0,i.jsx)(n.td,{children:"\u274c Insufficient"}),(0,i.jsx)(n.td,{children:"~$40"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"p5.48xlarge"}),(0,i.jsx)(n.td,{children:"8x H100"}),(0,i.jsx)(n.td,{children:"80GB each"}),(0,i.jsx)(n.td,{children:"640GB"}),(0,i.jsx)(n.td,{children:"\u2705 Recommended"}),(0,i.jsx)(n.td,{children:"\u2705 Supported"}),(0,i.jsx)(n.td,{children:"~$98"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"why-moe-models-need-more-memory",children:"Why MoE Models Need More Memory"}),"\n",(0,i.jsxs)(n.p,{children:["Unlike dense models where memory \u2248 2 \xd7 parameters (for BF16), MoE models load ",(0,i.jsx)(n.strong,{children:"all expert weights"})," into memory:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Scout Memory = Base Model + (16 experts \xd7 expert_size) \u2248 220GB\nMaverick Memory = Base Model + (128 experts \xd7 expert_size) \u2248 800GB\n"})}),"\n",(0,i.jsx)(n.p,{children:"Even though only 1-2 experts are activated per token during inference, all experts must reside in GPU memory for fast routing."}),"\n",(0,i.jsx)(n.h3,{id:"memory-optimization-options",children:"Memory Optimization Options"}),"\n",(0,i.jsx)(n.p,{children:"If you need to run on smaller GPUs, consider these alternatives:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWQ Quantization"})," (4-bit): Reduces memory by ~4x, but may have compatibility issues with vLLM for MoE models"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Smaller Models"}),": Use Llama 3.1 8B/70B which have more predictable memory requirements"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Pipeline Parallelism"}),": Split model across multiple nodes (requires LeaderWorkerSet)"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Using vLLM with ",(0,i.jsx)(n.code,{children:"gpu-memory-utilization=0.9"}),", we optimize memory usage while preventing out-of-memory (OOM) crashes."]}),"\n",(0,i.jsxs)(r.A,{header:(0,i.jsx)(n.h2,{children:(0,i.jsx)(n.span,{children:"Prerequisites and EKS Cluster Setup"})}),children:[(0,i.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),(0,i.jsx)(n.p,{children:"Before deploying Llama 4, ensure you have the following tools installed:"}),(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["To simplify the demo process, we assume the use of an IAM role with administrative privileges. For production deployments, create an IAM role with only the necessary permissions using tools like ",(0,i.jsx)(n.a,{href:"https://aws.amazon.com/iam/access-analyzer/",children:"IAM Access Analyzer"}),"."]})}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"aws cli"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://eksctl.io/installation/",children:"eksctl"})," (optional, for cluster creation)"]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://pypi.org/project/envsubst/",children:"envsubst"})}),"\n"]}),(0,i.jsx)(n.h3,{id:"eks-cluster-requirements",children:"EKS Cluster Requirements"}),(0,i.jsx)(n.p,{children:"This blueprint requires an EKS cluster with the following configuration:"}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"EKS Version"}),": >= 1.30"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"EKS Auto Mode"}),": Enabled (for automatic GPU node provisioning)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"NVIDIA Device Plugin"}),": Automatically managed by EKS Auto Mode"]}),"\n"]}),(0,i.jsx)(n.h3,{id:"option-a-create-a-new-eks-auto-mode-cluster",children:"Option A: Create a New EKS Auto Mode Cluster"}),(0,i.jsxs)(n.p,{children:["If you don't have an existing EKS cluster, you can quickly create one using ",(0,i.jsx)(n.code,{children:"eksctl"}),":"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"eksctl create cluster \\\n  --name llama4-cluster \\\n  --region us-west-2 \\\n  --auto-mode\n"})}),(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["Cluster creation takes approximately 10-15 minutes. The ",(0,i.jsx)(n.code,{children:"--auto-mode"})," flag enables EKS Auto Mode, which automatically manages node provisioning."]})}),(0,i.jsxs)(n.p,{children:["After the cluster is created, proceed to ",(0,i.jsx)(n.strong,{children:"Step 3"})," below to create a GPU NodePool."]}),(0,i.jsx)(n.h3,{id:"option-b-use-an-existing-eks-cluster",children:"Option B: Use an Existing EKS Cluster"}),(0,i.jsx)(n.p,{children:"If you already have an EKS cluster, verify that EKS Auto Mode is enabled:"}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 1:"})," Check EKS cluster version"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl version --short\n"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"Client Version: v1.30.0\nServer Version: v1.30.0-eks-xxxxx\n"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 2:"})," Verify EKS Auto Mode is enabled"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"aws eks describe-cluster --name <cluster-name> --query 'cluster.computeConfig.enabled'\n"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"true\n"})}),(0,i.jsx)(n.h3,{id:"step-3-configure-gpu-nodepool",children:"Step 3: Configure GPU NodePool"}),(0,i.jsx)(n.p,{children:"EKS Auto Mode requires a GPU NodePool to provision GPU instances. Check if a GPU NodePool exists:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get nodepools\n"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"NAME              AGE\ngeneral-purpose   10m\ngpu               5m\nsystem            10m\n"})}),(0,i.jsxs)(n.p,{children:["If you don't see a ",(0,i.jsx)(n.code,{children:"gpu"})," NodePool, create one:"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'cat <<EOF | kubectl apply -f -\napiVersion: karpenter.sh/v1\nkind: NodePool\nmetadata:\n  name: gpu\nspec:\n  disruption:\n    budgets:\n      - nodes: 10%\n    consolidateAfter: 30s\n    consolidationPolicy: WhenEmptyOrUnderutilized\n  template:\n    spec:\n      expireAfter: 336h\n      nodeClassRef:\n        group: eks.amazonaws.com\n        kind: NodeClass\n        name: default\n      requirements:\n        - key: karpenter.sh/capacity-type\n          operator: In\n          values: ["on-demand"]\n        - key: eks.amazonaws.com/instance-category\n          operator: In\n          values: ["g", "p"]\n        - key: eks.amazonaws.com/instance-generation\n          operator: Gte\n          values: ["4"]\n        - key: kubernetes.io/arch\n          operator: In\n          values: ["amd64"]\n        - key: kubernetes.io/os\n          operator: In\n          values: ["linux"]\n      taints:\n        - key: nvidia.com/gpu\n          effect: NoSchedule\nEOF\n'})}),(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["The default ",(0,i.jsx)(n.code,{children:"general-purpose"})," NodePool only supports CPU instance categories (c, m, r). GPU instances (g, p categories) require a dedicated GPU NodePool. The ",(0,i.jsx)(n.code,{children:"instance-generation: Gte 4"})," ensures p4d/p4de/p5 instances are available for Llama 4 models."]})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 4:"})," Verify NVIDIA Device Plugin (Auto Mode manages this automatically)"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get daemonset -n kube-system | grep nvidia\n"})}),(0,i.jsx)(n.p,{children:"If EKS Auto Mode is enabled, the NVIDIA device plugin is automatically deployed when GPU workloads are scheduled."})]}),"\n",(0,i.jsx)(n.h2,{id:"deploying-llama-4-scout-with-vllm",children:"Deploying Llama 4 Scout with vLLM"}),"\n",(0,i.jsx)(n.p,{children:"With the EKS cluster ready, we can now deploy Llama 4 Scout using vLLM."}),"\n",(0,i.jsx)(n.admonition,{type:"caution",children:(0,i.jsxs)(n.p,{children:["The use of ",(0,i.jsx)(n.a,{href:"https://huggingface.co/meta-llama",children:"Llama 4"})," models requires access through a Hugging Face account. Make sure you have accepted the model license on HuggingFace."]})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 1:"})," Export the Hugging Face Hub Token"]}),"\n",(0,i.jsx)(n.p,{children:"Create a Hugging Face account and generate an access token:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Navigate to ",(0,i.jsx)(n.a,{href:"https://huggingface.co/settings/tokens",children:"Hugging Face Settings \u2192 Access Tokens"})]}),"\n",(0,i.jsx)(n.li,{children:"Create a new token with read permissions"}),"\n",(0,i.jsx)(n.li,{children:"Export the token as a base64-encoded environment variable:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'export HUGGING_FACE_HUB_TOKEN=$(echo -n "Your-Hugging-Face-Hub-Token-Value" | base64)\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 2:"})," Clone the repository"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/awslabs/ai-on-eks.git\ncd ai-on-eks\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 3:"})," Deploy the vLLM service"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd blueprints/inference/llama4-vllm-gpu/\nenvsubst < llama4-vllm-deployment.yml | kubectl apply -f -\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Output:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"namespace/llama4-vllm created\nsecret/hf-token created\ndeployment.apps/llama4-vllm created\nservice/llama4-vllm-svc created\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 4:"})," Monitor the deployment"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n llama4-vllm -w\n"})}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsx)(n.p,{children:"The first deployment may take 10-15 minutes as EKS Auto Mode provisions a GPU node and the model weights are downloaded from HuggingFace."})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"NAME                           READY   STATUS    RESTARTS   AGE\nllama4-vllm-xxxxxxxxx-xxxxx    1/1     Running   0          10m\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 5:"})," Verify the service"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get svc -n llama4-vllm\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nllama4-vllm-svc   ClusterIP   172.20.xxx.xx   <none>        8000/TCP   10m\n"})}),"\n",(0,i.jsx)(n.h2,{id:"testing-the-llama-4-model",children:"Testing the Llama 4 Model"}),"\n",(0,i.jsx)(n.p,{children:"Now it's time to test the Llama 4 chat model."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 1:"})," Port-forward the vLLM service"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl -n llama4-vllm port-forward svc/llama4-vllm-svc 8000:8000\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 2:"})," Test the /v1/models endpoint"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"curl http://localhost:8000/v1/models\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Response:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "object": "list",\n  "data": [\n    {\n      "id": "meta-llama/Llama-4-Scout-17B-16E-Instruct",\n      "object": "model",\n      "created": 1234567890,\n      "owned_by": "vllm"\n    }\n  ]\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 3:"})," Test chat completion"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",\n    "messages": [{"role": "user", "content": "Explain what Amazon EKS is in 2 sentences."}],\n    "max_tokens": 100,\n    "stream": false\n  }\'\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Response:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "id": "chatcmpl-xxxxx",\n  "object": "chat.completion",\n  "created": 1234567890,\n  "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",\n  "choices": [\n    {\n      "index": 0,\n      "message": {\n        "role": "assistant",\n        "content": "Amazon Elastic Kubernetes Service (EKS) is a managed container orchestration service that makes it easy to run Kubernetes on AWS without needing to install and operate your own Kubernetes control plane. It automatically manages the availability and scalability of the Kubernetes control plane nodes, handles upgrades, and integrates with other AWS services for security, networking, and monitoring."\n      },\n      "finish_reason": "stop"\n    }\n  ],\n  "usage": {\n    "prompt_tokens": 15,\n    "completion_tokens": 75,\n    "total_tokens": 90\n  }\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 4:"})," Test streaming response"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",\n    "messages": [{"role": "user", "content": "Hello!"}],\n    "stream": true\n  }\'\n'})}),"\n",(0,i.jsx)(n.h2,{id:"deploy-open-webui",children:"Deploy Open WebUI"}),"\n",(0,i.jsx)(n.p,{children:"Now, let's deploy Open WebUI, which provides a ChatGPT-style chat interface to interact with the Llama 4 model."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 1:"})," Deploy Open WebUI"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd ai-on-eks/blueprints/inference/llama4-vllm-gpu/\nkubectl apply -f open-webui.yaml\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Output:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"namespace/open-webui created\ndeployment.apps/open-webui created\nservice/open-webui created\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 2:"})," Verify the deployment"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n open-webui\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"NAME                          READY   STATUS    RESTARTS   AGE\nopen-webui-xxxxxxxxx-xxxxx    1/1     Running   0          2m\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 3:"})," Access the Open WebUI"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl -n open-webui port-forward svc/open-webui 8080:80\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Open your browser and navigate to ",(0,i.jsx)(n.a,{href:"http://localhost:8080",children:"http://localhost:8080"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 4:"})," Register and start chatting"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Sign up with your name, email, and password"}),"\n",(0,i.jsx)(n.li,{children:'Click "New Chat"'}),"\n",(0,i.jsx)(n.li,{children:"Select the Llama 4 model from the dropdown"}),"\n",(0,i.jsx)(n.li,{children:"Start chatting!"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"monitoring-and-observability",children:"Monitoring and Observability"}),"\n",(0,i.jsx)(n.h3,{id:"check-vllm-pod-logs",children:"Check vLLM Pod Logs"}),"\n",(0,i.jsx)(n.p,{children:"Monitor the vLLM server logs to check model loading status and inference metrics:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl logs -n llama4-vllm -l app=llama4-vllm -f\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key metrics to watch in logs:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Token throughput"}),": ",(0,i.jsx)(n.code,{children:"Avg prompt throughput: X tokens/s, Avg generation throughput: Y tokens/s"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"GPU KV Cache utilization"}),": ",(0,i.jsx)(n.code,{children:"GPU KV cache usage: X%"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Request processing"}),": ",(0,i.jsx)(n.code,{children:"Received request"})," and ",(0,i.jsx)(n.code,{children:"Finished request"})," entries"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"check-gpu-utilization",children:"Check GPU Utilization"}),"\n",(0,i.jsx)(n.p,{children:"If you have NVIDIA DCGM Exporter or similar monitoring tools deployed:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Check GPU memory usage on the node\nkubectl exec -n llama4-vllm -it $(kubectl get pods -n llama4-vllm -l app=llama4-vllm -o jsonpath='{.items[0].metadata.name}') -- nvidia-smi\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Expected output:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-text",children:"+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 535.xx.xx    Driver Version: 535.xx.xx    CUDA Version: 12.x     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  NVIDIA A10G         On   | 00000000:00:1E.0 Off |                    0 |\n|  0%   45C    P0    70W / 300W |  18000MiB / 24576MiB |     25%      Default |\n+-------------------------------+----------------------+----------------------+\n"})}),"\n",(0,i.jsx)(n.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,i.jsxs)(n.p,{children:["vLLM provides built-in metrics at the ",(0,i.jsx)(n.code,{children:"/metrics"})," endpoint:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"curl http://localhost:8000/metrics\n"})}),"\n",(0,i.jsx)(n.p,{children:"Key metrics include:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"vllm:num_requests_running"})," - Number of requests currently being processed"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"vllm:num_requests_waiting"})," - Number of requests waiting in queue"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"vllm:gpu_cache_usage_perc"})," - GPU KV cache utilization percentage"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"vllm:avg_generation_throughput_toks_per_s"})," - Average token generation throughput"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"deploying-llama-4-maverick-multi-gpu",children:"Deploying Llama 4 Maverick (Multi-GPU)"}),"\n",(0,i.jsx)(n.p,{children:"For the larger Maverick model with 128 experts, you need multiple GPUs with tensor parallelism."}),"\n",(0,i.jsx)(n.admonition,{type:"danger",children:(0,i.jsx)(n.p,{children:"Important: Deploying Llama 4 Maverick requires 8x H100 (80GB) GPUs. This can be very expensive. Ensure you monitor your usage carefully."})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 1:"})," Deploy the 70B model"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd ai-on-eks/blueprints/inference/llama4-vllm-gpu/\nenvsubst < llama4-vllm-deployment-70b.yml | kubectl apply -f -\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 2:"})," Monitor the deployment"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n llama4-vllm -l model=llama4-maverick -w\n"})}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsx)(n.p,{children:"The Maverick model deployment may take 30-45 minutes due to larger model weights download and multi-GPU initialization."})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 3:"})," Test the Maverick model"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl -n llama4-vllm port-forward svc/llama4-vllm-70b-svc 8001:8000\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:8001/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct",\n    "messages": [{"role": "user", "content": "Hello!"}]\n  }\'\n'})}),"\n",(0,i.jsx)(n.h2,{id:"cleanup",children:"Cleanup"}),"\n",(0,i.jsx)(n.p,{children:"To remove all deployed resources:"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 1:"})," Delete Open WebUI"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl delete -f open-webui.yaml\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 2:"})," Delete Llama 4 vLLM deployment"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl delete -f llama4-vllm-deployment.yml\n# Or for 70B model:\nkubectl delete -f llama4-vllm-deployment-70b.yml\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Step 3:"})," Delete namespaces (optional)"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl delete namespace llama4-vllm\nkubectl delete namespace open-webui\n"})}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsx)(n.p,{children:"After deleting the GPU workloads, EKS Auto Mode will automatically terminate idle GPU nodes to reduce costs."})}),"\n",(0,i.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"EKS Auto Mode Simplifies GPU Provisioning"}),": No need to configure Karpenter or manage node groups manually."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"vLLM Provides High Performance"}),": Optimized memory management with PagedAttention enables efficient inference."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"OpenAI-Compatible API"}),": Easy integration with existing tools and applications."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Scalable Architecture"}),": Support for both single-GPU (8B) and multi-GPU (70B) deployments."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Cost Optimization"}),": EKS Auto Mode automatically terminates idle GPU nodes."]}),"\n"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}}}]);
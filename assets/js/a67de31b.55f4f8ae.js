"use strict";(globalThis.webpackChunkdoeks_website=globalThis.webpackChunkdoeks_website||[]).push([[914],{70903(e,n,r){r.r(n),r.d(n,{assets:()=>d,contentTitle:()=>t,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"blueprints/inference/index","title":"Inference on EKS","description":"Deploy and run Large Language Models (LLMs) and other AI models on Amazon EKS.","source":"@site/docs/blueprints/inference/index.md","sourceDirName":"blueprints/inference","slug":"/blueprints/inference/","permalink":"/ai-on-eks/docs/blueprints/inference/","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/blueprints/inference/index.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"blueprints","previous":{"title":"Overview","permalink":"/ai-on-eks/docs/blueprints/"},"next":{"title":"AI on EKS Inference Charts","permalink":"/ai-on-eks/docs/blueprints/inference/inference-charts"}}');var s=r(74848),o=r(28453);const l={sidebar_position:1},t="Inference on EKS",d={},c=[{value:"What&#39;s in This Section",id:"whats-in-this-section",level:2},{value:"Inference Charts",id:"inference-charts",level:2},{value:"Framework-Specific Deployment Guides",id:"framework-specific-deployment-guides",level:2},{value:"GPU Deployments",id:"gpu-deployments",level:3},{value:"Neuron Deployments",id:"neuron-deployments",level:3},{value:"Getting Started",id:"getting-started",level:2},{value:"Need Help?",id:"need-help",level:2}];function a(e){const n={a:"a",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"inference-on-eks",children:"Inference on EKS"})}),"\n",(0,s.jsx)(n.p,{children:"Deploy and run Large Language Models (LLMs) and other AI models on Amazon EKS."}),"\n",(0,s.jsx)(n.h2,{id:"whats-in-this-section",children:"What's in This Section"}),"\n",(0,s.jsx)(n.p,{children:"This section provides practical deployment guides and Helm charts for running inference workloads on EKS. Whether you're deploying open-source LLMs, diffusion models, or custom AI models, you'll find ready-to-use configurations and step-by-step instructions."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"inference-charts",children:(0,s.jsx)(n.a,{href:"/ai-on-eks/docs/blueprints/inference/inference-charts",children:"Inference Charts"})}),"\n",(0,s.jsx)(n.p,{children:"Helm charts for deploying popular AI models on EKS with pre-configured values for optimal performance."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"What You Get:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Ready-to-deploy Helm charts for vLLM, Ray-vLLM, Triton, and Diffusers"}),"\n",(0,s.jsx)(n.li,{children:"Pre-configured values files for popular models (Llama, DeepSeek, Mistral, Stable Diffusion, and more)"}),"\n",(0,s.jsx)(n.li,{children:"Support for both GPU (NVIDIA) and Neuron (AWS Inferentia/Trainium) deployments"}),"\n",(0,s.jsx)(n.li,{children:"Configurations with health checks, autoscaling, and monitoring"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Use Cases:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Quick deployment of open-source LLMs"}),"\n",(0,s.jsx)(n.li,{children:"Standardized deployment patterns across your organization"}),"\n",(0,s.jsx)(n.li,{children:"Reference implementations for custom model deployments"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"/ai-on-eks/docs/blueprints/inference/inference-charts",children:"Explore Inference Charts \u2192"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"framework-specific-deployment-guides",children:"Framework-Specific Deployment Guides"}),"\n",(0,s.jsx)(n.p,{children:"Detailed guides for deploying models with deep dive into specific frameworks on EKS, organized by hardware type."}),"\n",(0,s.jsx)(n.h3,{id:"gpu-deployments",children:"GPU Deployments"}),"\n",(0,s.jsx)(n.p,{children:"Step-by-step guides for deploying models on NVIDIA GPUs:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/docs/blueprints/inference/framework-guides/GPUs/aibrix-deepseek-distill",children:"AIBrix DeepSeek Distill"})})," - Deploy DeepSeek R1 Distill Llama 8B with AIBrix optimization"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/docs/blueprints/inference/framework-guides/GPUs/nvidia-dynamo",children:"NVIDIA Dynamo"})})," - Deploy models with NVIDIA's Dynamo framework"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/docs/blueprints/inference/framework-guides/GPUs/nvidia-nim-llama3",children:"NVIDIA NIM Llama 3"})})," - Deploy Llama 3 using NVIDIA NIM"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/docs/blueprints/inference/framework-guides/GPUs/nvidia-nim-operator",children:"NVIDIA NIM Operator"})})," - Kubernetes operator for NVIDIA NIM deployments"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/docs/blueprints/inference/framework-guides/GPUs/vLLM-NVIDIATritonServer",children:"vLLM with NVIDIA Triton Server"})})," - Inference with Triton and vLLM"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/docs/blueprints/inference/framework-guides/GPUs/vLLM-rayserve",children:"vLLM with Ray Serve"})})," - Scalable inference with Ray Serve and vLLM"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"neuron-deployments",children:"Neuron Deployments"}),"\n",(0,s.jsx)(n.p,{children:"Step-by-step guides for deploying models on AWS Inferentia and Trainium:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/docs/blueprints/inference/framework-guides/Neuron/Mistral-7b-inf2",children:"Mistral 7B on Inf2"})})," - Deploy Mistral 7B on AWS Inferentia 2"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/docs/blueprints/inference/framework-guides/Neuron/llama2-inf2",children:"Llama 2 on Inf2"})})," - Deploy Llama 2 13B on AWS Inferentia 2"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/docs/blueprints/inference/framework-guides/Neuron/llama3-inf2",children:"Llama 3 on Inf2"})})," - Deploy Llama 3 on AWS Inferentia 2"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/docs/blueprints/inference/framework-guides/Neuron/rayserve-ha",children:"Ray Serve High Availability"})})," - Deploy highly available Ray Serve on Neuron"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/docs/blueprints/inference/framework-guides/Neuron/stablediffusion-inf2",children:"Stable Diffusion on Inf2"})})," - Deploy Stable Diffusion on AWS Inferentia 2"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/docs/blueprints/inference/framework-guides/Neuron/vllm-ray-inf2",children:"vLLM Ray on Inf2"})})," - Deploy vLLM with Ray on AWS Inferentia 2"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Set up your infrastructure"})," - Start with the ",(0,s.jsx)(n.a,{href:"/docs/infra/inference/inference-ready-cluster",children:"Inference-Ready Cluster"})," to provision an EKS cluster optimized for AI/ML workloads"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Choose your deployment method"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["For quick deployments with popular models \u2192 Use ",(0,s.jsx)(n.a,{href:"/ai-on-eks/docs/blueprints/inference/inference-charts",children:"Inference Charts"})]}),"\n",(0,s.jsx)(n.li,{children:"For specific frameworks or custom configurations \u2192 See Framework-Specific Guides above"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Optimize your deployment"})," - Apply best practices from the ",(0,s.jsx)(n.a,{href:"/docs/guidance/",children:"Guidance section"})," to improve performance and reduce costs"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"need-help",children:"Need Help?"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Infrastructure Setup"}),": See ",(0,s.jsx)(n.a,{href:"/docs/infra/inference/",children:"Inference Infrastructure"})," for cluster setup and configuration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Optimization"}),": Check the ",(0,s.jsx)(n.a,{href:"/docs/guidance/",children:"Guidance section"})," for performance tuning and best practices"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Issues"}),": Report bugs or request features on ",(0,s.jsx)(n.a,{href:"https://github.com/awslabs/ai-on-eks/issues",children:"GitHub Issues"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Community"}),": Join discussions on ",(0,s.jsx)(n.a,{href:"https://github.com/awslabs/ai-on-eks/discussions",children:"GitHub Discussions"})]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(a,{...e})}):a(e)}},28453(e,n,r){r.d(n,{R:()=>l,x:()=>t});var i=r(96540);const s={},o=i.createContext(s);function l(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);
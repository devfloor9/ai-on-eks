"use strict";(globalThis.webpackChunkdoeks_website=globalThis.webpackChunkdoeks_website||[]).push([[1531],{17305(e,n,i){i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"infra/inference/index","title":"Model Inference Lifecycle on Amazon EKS","description":"This guide walks you through the complete lifecycle of deploying and optimizing Large Language Model (LLM) inference on Amazon EKS, from infrastructure setup to production-ready optimization.","source":"@site/docs/infra/inference/index.md","sourceDirName":"infra/inference","slug":"/infra/inference/","permalink":"/ai-on-eks/docs/infra/inference/","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/infra/inference/index.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"infra","previous":{"title":"JupyterHub on EKS","permalink":"/ai-on-eks/docs/infra/training/jupyterhub"},"next":{"title":"Inference-Ready EKS Cluster","permalink":"/ai-on-eks/docs/infra/inference/inference-ready-cluster"}}');var s=i(74848),t=i(28453);const l={sidebar_position:1},o="Model Inference Lifecycle on Amazon EKS",a={},c=[{value:"Lifecycle Overview",id:"lifecycle-overview",level:2},{value:"Phase 1: Infrastructure Setup",id:"phase-1-infrastructure-setup",level:2},{value:"Why Infrastructure Matters",id:"why-infrastructure-matters",level:3},{value:"Solution: Inference-Ready Cluster",id:"solution-inference-ready-cluster",level:3},{value:"What You Get",id:"what-you-get",level:4},{value:"Key Considerations",id:"key-considerations",level:4},{value:"Get Started",id:"get-started",level:4},{value:"Phase 2: Model Deployment",id:"phase-2-model-deployment",level:2},{value:"Why Deployment Patterns Matter",id:"why-deployment-patterns-matter",level:3},{value:"Solution: Inference Charts",id:"solution-inference-charts",level:3},{value:"Supported Frameworks",id:"supported-frameworks",level:4},{value:"Example: Deploying Llama 3.2 1B",id:"example-deploying-llama-32-1b",level:4},{value:"Available Pre-configured Models",id:"available-pre-configured-models",level:4},{value:"Key Considerations",id:"key-considerations-1",level:4},{value:"Get Started",id:"get-started-1",level:4},{value:"Phase 3: Optimization",id:"phase-3-optimization",level:2},{value:"Why Optimization Matters",id:"why-optimization-matters",level:3},{value:"Optimization Techniques",id:"optimization-techniques",level:3},{value:"Container Startup Time Optimization",id:"container-startup-time-optimization",level:4},{value:"Dynamic Resource Allocation (DRA)",id:"dynamic-resource-allocation-dra",level:4},{value:"Observability and Monitoring",id:"observability-and-monitoring",level:4},{value:"Networking Optimization",id:"networking-optimization",level:4},{value:"EKS Best Practices",id:"eks-best-practices",level:4},{value:"Optimization Workflow",id:"optimization-workflow",level:3},{value:"Get Started",id:"get-started-2",level:4},{value:"Complete Lifecycle Example",id:"complete-lifecycle-example",level:2},{value:"Step 1: Deploy Infrastructure (30-45 minutes)",id:"step-1-deploy-infrastructure-30-45-minutes",level:3},{value:"Step 2: Deploy Model (5-10 minutes)",id:"step-2-deploy-model-5-10-minutes",level:3},{value:"Step 3: Optimize (Ongoing)",id:"step-3-optimize-ongoing",level:3},{value:"Next Steps",id:"next-steps",level:2},{value:"Additional Resources",id:"additional-resources",level:2},{value:"Community and Support",id:"community-and-support",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"model-inference-lifecycle-on-amazon-eks",children:"Model Inference Lifecycle on Amazon EKS"})}),"\n",(0,s.jsx)(n.p,{children:"This guide walks you through the complete lifecycle of deploying and optimizing Large Language Model (LLM) inference on Amazon EKS, from infrastructure setup to production-ready optimization."}),"\n",(0,s.jsx)(n.h2,{id:"lifecycle-overview",children:"Lifecycle Overview"}),"\n",(0,s.jsx)(n.p,{children:"The model inference lifecycle consists of three key phases:"}),"\n",(0,s.jsx)(n.mermaid,{value:'graph LR\n    A["<b>Infrastructure Setup</b>"] --\x3e B["<b>Model Deployment</b>"]\n    B --\x3e C["<b>Optimization</b>"]\n\n    style A fill:#667eea,stroke:#333,stroke-width:2px,color:#ffffff\n    style B fill:#48bb78,stroke:#333,stroke-width:2px,color:#ffffff\n    style C fill:#ed8936,stroke:#333,stroke-width:2px,color:#ffffff\n\n    click A "inference/inference-ready-cluster" "Go to Inference-Ready Cluster"\n    click B "../blueprints/inference/inference-charts" "Go to Inference Charts"\n    click C "../guidance/" "Go to Guidance"'}),"\n",(0,s.jsx)(n.p,{children:"Each phase addresses specific challenges and provides the building blocks needed for successful LLM inference at scale."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"phase-1-infrastructure-setup",children:(0,s.jsx)(n.a,{href:"/docs/infra/inference/inference-ready-cluster",children:"Phase 1: Infrastructure Setup"})}),"\n",(0,s.jsx)(n.h3,{id:"why-infrastructure-matters",children:"Why Infrastructure Matters"}),"\n",(0,s.jsx)(n.p,{children:"Before deploying any LLM, you need a robust, scalable infrastructure that can handle the unique demands of AI/ML workloads:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPU/Neuron Resource Management"}),": LLMs require specialized accelerators (NVIDIA GPUs or AWS Inferentia/Trainium chips) with proper device plugins and drivers"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Autoscaling Capabilities"}),": Dynamic workload scaling to handle varying inference demand while optimizing costs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Observability Foundation"}),": Monitoring and metrics collection for GPU/Neuron utilization, model performance, and system health"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Distributed Computing Support"}),": Infrastructure for multi-node inference when models exceed single-node capacity"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"solution-inference-ready-cluster",children:"Solution: Inference-Ready Cluster"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.a,{href:"/docs/infra/inference/inference-ready-cluster",children:(0,s.jsx)(n.strong,{children:"Inference-Ready EKS Cluster"})})," provides a pre-configured infrastructure specifically designed for AI/ML inference workloads."]}),"\n",(0,s.jsx)(n.h4,{id:"what-you-get",children:"What You Get"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Pre-installed Components"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"KubeRay Operator for distributed Ray workloads"}),"\n",(0,s.jsx)(n.li,{children:"LeaderWorkerSet for multi-node distributed inference"}),"\n",(0,s.jsx)(n.li,{children:"NVIDIA Device Plugin for GPU management"}),"\n",(0,s.jsx)(n.li,{children:"AWS Neuron Device Plugin for Inferentia/Trainium support"}),"\n",(0,s.jsx)(n.li,{children:"Karpenter for intelligent node autoscaling"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Built-in Observability"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Prometheus for metrics collection"}),"\n",(0,s.jsx)(n.li,{children:"Grafana with AI/ML-specific dashboards"}),"\n",(0,s.jsx)(n.li,{children:"DCGM Exporter for GPU metrics"}),"\n",(0,s.jsx)(n.li,{children:"Node Exporter for system-level metrics"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"AIBrix Integration"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Advanced inference optimization capabilities"}),"\n",(0,s.jsx)(n.li,{children:"Gateway and routing for traffic management"}),"\n",(0,s.jsx)(n.li,{children:"Performance monitoring tools"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"key-considerations",children:"Key Considerations"}),"\n",(0,s.jsx)(n.p,{children:"When setting up your infrastructure, consider:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hardware Selection"}),": Choose between NVIDIA GPUs (P4d, P5, G5) or AWS Neuron (Inf2, Trn1) based on your model requirements and cost constraints"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scaling Strategy"}),": Determine if you need single-node or multi-node inference based on model size"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Network Configuration"}),": Ensure proper VPC setup with sufficient bandwidth for model loading and inference traffic"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Storage Requirements"}),": Plan for model artifact storage (S3, EFS, or FSx) based on model size and access patterns"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"get-started",children:"Get Started"}),"\n",(0,s.jsxs)(n.p,{children:["Follow the ",(0,s.jsx)(n.a,{href:"/docs/infra/inference/inference-ready-cluster",children:"Inference-Ready Cluster deployment guide"})," to provision your infrastructure."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"phase-2-model-deployment",children:(0,s.jsx)(n.a,{href:"/ai-on-eks/docs/blueprints/inference/inference-charts",children:"Phase 2: Model Deployment"})}),"\n",(0,s.jsx)(n.h3,{id:"why-deployment-patterns-matter",children:"Why Deployment Patterns Matter"}),"\n",(0,s.jsx)(n.p,{children:"Once your infrastructure is ready, deploying LLMs requires careful consideration of:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Framework Selection"}),": Different frameworks (vLLM, Ray-vLLM, Triton) offer different trade-offs in performance, features, and complexity"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Compatibility"}),": Not all models work with all frameworks; some require specific optimizations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resource Allocation"}),": Proper GPU/Neuron memory allocation and request/limit configuration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scaling Behavior"}),": Single-replica vs. multi-replica deployments with autoscaling"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"solution-inference-charts",children:"Solution: Inference Charts"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.a,{href:"/ai-on-eks/docs/blueprints/inference/inference-charts",children:(0,s.jsx)(n.strong,{children:"AI on EKS Inference Charts"})})," provide Helm-based deployments with pre-configured values for popular models, supporting multiple deployment frameworks."]}),"\n",(0,s.jsx)(n.h4,{id:"supported-frameworks",children:"Supported Frameworks"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"vLLM"}),": Fast single-node inference with optimized CUDA kernels"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ray-vLLM"}),": Distributed inference with autoscaling and load balancing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Triton-vLLM"}),": Production-ready inference server with advanced features"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LeaderWorkerSet-vLLM"}),": Multi-node inference for models that don't fit on a single node"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Diffusers"}),": Hugging Face Diffusers for image generation models"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"example-deploying-llama-32-1b",children:"Example: Deploying Llama 3.2 1B"}),"\n",(0,s.jsx)(n.p,{children:"Let's walk through deploying a Llama model using the inference charts:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Step 1: Create Hugging Face Token Secret"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl create secret generic hf-token \\\n  --from-literal=token=your_huggingface_token\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Step 2: Deploy the Model"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Deploy Llama 3.2 1B on GPU with vLLM\nhelm install llama-inference ./blueprints/inference/inference-charts \\\n  --values ./blueprints/inference/inference-charts/values-llama-32-1b-vllm.yaml\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Step 3: Verify Deployment"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Check pod status\nkubectl get pods -l app=llama-inference\n\n# Check service endpoint\nkubectl get svc llama-inference\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Step 4: Test Inference"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Port forward to the service\nkubectl port-forward svc/llama-inference 8000:8000\n\n# Send a test request\ncurl http://localhost:8000/v1/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "meta-llama/Llama-3.2-1B",\n    "prompt": "Explain quantum computing in simple terms:",\n    "max_tokens": 100\n  }\'\n'})}),"\n",(0,s.jsx)(n.h4,{id:"available-pre-configured-models",children:"Available Pre-configured Models"}),"\n",(0,s.jsx)(n.p,{children:"The inference charts include ready-to-deploy configurations for:"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Language Models"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"DeepSeek R1 Distill Llama 8B"}),"\n",(0,s.jsx)(n.li,{children:"Llama 3.2 1B, Llama 4 Scout 17B"}),"\n",(0,s.jsx)(n.li,{children:"Mistral Small 24B"}),"\n",(0,s.jsx)(n.li,{children:"GPT OSS 20B"}),"\n",(0,s.jsx)(n.li,{children:"Qwen3 1.7B"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Diffusion Models"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"FLUX.1 Schnell"}),"\n",(0,s.jsx)(n.li,{children:"Stable Diffusion XL, Stable Diffusion 3.5"}),"\n",(0,s.jsx)(n.li,{children:"Kolors, OmniGen"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Neuron-Optimized"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Llama 2 13B on AWS Inferentia"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"key-considerations-1",children:"Key Considerations"}),"\n",(0,s.jsx)(n.p,{children:"When deploying your model, consider:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Size vs. Hardware"}),": Ensure your chosen instance type has sufficient GPU/Neuron memory for the model"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Batch Size Configuration"}),": Tune batch size for optimal throughput vs. latency trade-offs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Quantization Options"}),": Consider using quantized models (INT8, INT4) to reduce memory footprint"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Replica Count"}),": Start with a single replica and scale based on observed load"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Health Checks"}),": Configure appropriate liveness and readiness probes"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"get-started-1",children:"Get Started"}),"\n",(0,s.jsxs)(n.p,{children:["Explore the ",(0,s.jsx)(n.a,{href:"/ai-on-eks/docs/blueprints/inference/inference-charts",children:"Inference Charts documentation"})," for detailed configuration options and advanced deployment scenarios."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"phase-3-optimization",children:(0,s.jsx)(n.a,{href:"/docs/guidance/",children:"Phase 3: Optimization"})}),"\n",(0,s.jsx)(n.h3,{id:"why-optimization-matters",children:"Why Optimization Matters"}),"\n",(0,s.jsx)(n.p,{children:"After deploying your model, optimization ensures:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cost Efficiency"}),": Minimize infrastructure costs while meeting performance requirements"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance Tuning"}),": Achieve target latency and throughput SLOs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resource Utilization"}),": Maximize GPU/Neuron utilization to get the most value from expensive hardware"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Operational Excellence"}),": Implement best practices for monitoring, alerting, and troubleshooting"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"optimization-techniques",children:"Optimization Techniques"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.a,{href:"/ai-on-eks/docs/guidance/",children:(0,s.jsx)(n.strong,{children:"Guidance section"})})," provides comprehensive best practices and optimization techniques for production AI/ML workloads. Each technique addresses specific performance or cost challenges:"]}),"\n",(0,s.jsx)(n.h4,{id:"container-startup-time-optimization",children:(0,s.jsx)(n.a,{href:"/docs/guidance/container-startup-time/",children:"Container Startup Time Optimization"})}),"\n",(0,s.jsx)(n.p,{children:"Reduce model loading time from minutes to seconds, improving autoscaling responsiveness and development iteration speed."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Key Techniques"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/docs/guidance/container-startup-time/reduce-container-image-size/",children:"Reduce Image Size"})}),": Optimize container images to minimize pull time"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/docs/guidance/container-startup-time/reduce-container-image-size/decoupling-model-artifacts",children:"Decouple Model Artifacts"})}),": Separate model weights from container images"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/docs/guidance/container-startup-time/accelerate-pull-process/",children:"Accelerate Pull Process"})}),": Use containerd snapshotters and image pre-fetching"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/docs/guidance/container-startup-time/accelerate-pull-process/prefecthing-images-on-br",children:"Pre-fetch on Nodes"})}),": Warm up nodes with model images before workload scheduling"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Impact"}),": Reduces startup time by 60-80%, enabling faster autoscaling and lower costs."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"When to Use"}),": Critical for autoscaling workloads, development environments, and cost-sensitive deployments."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"dynamic-resource-allocation-dra",children:(0,s.jsx)(n.a,{href:"/docs/guidance/dynamic-resource-allocation",children:"Dynamic Resource Allocation (DRA)"})}),"\n",(0,s.jsx)(n.p,{children:"Next-generation GPU scheduling for fine-grained resource control and improved utilization."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Key Capabilities"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fine-grained GPU Control"}),": Request specific GPU memory amounts instead of whole devices"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Per-workload Sharing"}),": Choose MPS, time-slicing, MIG, or exclusive mode per pod"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Topology-aware Scheduling"}),": Optimize for NVLink and GPU interconnects"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Required for P6e"}),": Mandatory for Amazon EC2 P6e-GB200 UltraServers"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Impact"}),": Increases GPU utilization from 30-40% to 70-90%, reducing infrastructure costs by 50%+."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"When to Use"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Running multiple small models on a single GPU"}),"\n",(0,s.jsx)(n.li,{children:"Need precise GPU memory allocation"}),"\n",(0,s.jsx)(n.li,{children:"Using latest GPU instances (P6e)"}),"\n",(0,s.jsx)(n.li,{children:"Optimizing GPU utilization across workloads"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"observability-and-monitoring",children:(0,s.jsx)(n.a,{href:"/docs/guidance/observability",children:"Observability and Monitoring"})}),"\n",(0,s.jsx)(n.p,{children:"Comprehensive visibility into inference workloads for performance optimization and troubleshooting."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"What's Included"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPU/Neuron Metrics"}),": Track utilization, memory usage, and temperature"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Performance"}),": Monitor latency, throughput, and error rates"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"System Health"}),": CPU, memory, network, and storage metrics"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Custom Dashboards"}),": Pre-built Grafana dashboards for inference workloads"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Key Metrics to Monitor"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"GPU utilization (target: >70% for cost efficiency)"}),"\n",(0,s.jsx)(n.li,{children:"Inference latency (P50, P95, P99)"}),"\n",(0,s.jsx)(n.li,{children:"Requests per second (throughput)"}),"\n",(0,s.jsx)(n.li,{children:"Queue depth (for autoscaling decisions)"}),"\n",(0,s.jsx)(n.li,{children:"Model loading time"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Impact"}),": Enables data-driven optimization decisions and proactive issue detection."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"networking-optimization",children:(0,s.jsx)(n.a,{href:"/docs/guidance/networking",children:"Networking Optimization"})}),"\n",(0,s.jsx)(n.p,{children:"Optimize network configuration for high-throughput inference workloads."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Key Areas"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"VPC Design"}),": Proper subnet sizing and availability zone distribution"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Load Balancing"}),": ALB/NLB configuration for inference endpoints"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Service Mesh"}),": Istio/Linkerd for advanced traffic management"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Network Policies"}),": Security and isolation between workloads"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Impact"}),": Reduces network latency by 20-40% and improves reliability."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"eks-best-practices",children:(0,s.jsx)(n.a,{href:"/docs/guidance/eks-best-practices",children:"EKS Best Practices"})}),"\n",(0,s.jsx)(n.p,{children:"Comprehensive guides for security, reliability, performance, and cost optimization."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Topics Covered"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Security and compliance"}),"\n",(0,s.jsx)(n.li,{children:"Reliability and availability"}),"\n",(0,s.jsx)(n.li,{children:"Performance optimization"}),"\n",(0,s.jsx)(n.li,{children:"Cost optimization"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"optimization-workflow",children:"Optimization Workflow"}),"\n",(0,s.jsx)(n.p,{children:"Follow this workflow to optimize your inference deployment:"}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TD\n    A[Deploy Model] --\x3e B[Establish Baseline Metrics]\n    B --\x3e C[Identify Bottlenecks]\n    C --\x3e D{Bottleneck Type?}\n\n    D --\x3e|GPU Utilization Low| E[Increase Batch Size / Add Replicas]\n    D --\x3e|Latency High| F[Reduce Batch Size / Add GPU Memory]\n    D --\x3e|Cost High| G[Enable GPU Sharing / Use Smaller Instance]\n    D --\x3e|Startup Slow| H[Optimize Container Images]\n\n    E --\x3e I[Measure Impact]\n    F --\x3e I\n    G --\x3e I\n    H --\x3e I\n\n    I --\x3e J{Meets SLOs?}\n    J --\x3e|No| C\n    J --\x3e|Yes| K[Production Ready]\n\n    style A fill:#667eea,stroke:#333,stroke-width:2px,color:#fff\n    style K fill:#48bb78,stroke:#333,stroke-width:2px,color:#fff"}),"\n",(0,s.jsx)(n.h4,{id:"get-started-2",children:"Get Started"}),"\n",(0,s.jsxs)(n.p,{children:["Explore the ",(0,s.jsx)(n.a,{href:"/ai-on-eks/docs/guidance/",children:"Guidance documentation"})," to dive deep into each optimization technique."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"complete-lifecycle-example",children:"Complete Lifecycle Example"}),"\n",(0,s.jsx)(n.p,{children:"Here's a complete example of deploying Llama 3.2 1B from infrastructure to optimized production:"}),"\n",(0,s.jsx)(n.h3,{id:"step-1-deploy-infrastructure-30-45-minutes",children:"Step 1: Deploy Infrastructure (30-45 minutes)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Clone the repository\ngit clone https://github.com/awslabs/ai-on-eks.git\ncd ai-on-eks/infra/solutions/inference-ready-cluster\n\n# Configure your deployment\ncp blueprint.tfvars.example blueprint.tfvars\n# Edit blueprint.tfvars with your settings\n\n# Deploy the infrastructure\nterraform init\nterraform apply -var-file=blueprint.tfvars\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-2-deploy-model-5-10-minutes",children:"Step 2: Deploy Model (5-10 minutes)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Update kubeconfig\naws eks update-kubeconfig --name <cluster-name> --region <region>\n\n# Create Hugging Face token secret\nkubectl create secret generic hf-token \\\n  --from-literal=token=<your-token>\n\n# Deploy Llama 3.2 1B\nhelm repo add ai-on-eks https://awslabs.github.io/ai-on-eks-charts/\nhelm repo update\n\nhelm install qwen3-1-7b ai-on-eks/inference-charts \\\n  -f https://raw.githubusercontent.com/awslabs/ai-on-eks-charts/refs/heads/main/charts/inference-charts/values-llama-32-1b-vllm.yaml\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-3-optimize-ongoing",children:"Step 3: Optimize (Ongoing)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Access Grafana for monitoring\nkubectl port-forward -n monitoring svc/kube-prometheus-stack-grafana 3000:80\n\n# Monitor GPU utilization and adjust replicas\nkubectl scale deployment llama-inference --replicas=3\n\n# Enable autoscaling based on queue depth\nkubectl autoscale deployment llama-inference \\\n  --min=2 --max=10 --cpu-percent=70\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"Now that you understand the complete model inference lifecycle, choose your starting point:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"New to EKS?"})," Start with ",(0,s.jsx)(n.a,{href:"/docs/infra/inference/inference-ready-cluster",children:"Infrastructure Setup"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Have Infrastructure?"})," Jump to ",(0,s.jsx)(n.a,{href:"/ai-on-eks/docs/blueprints/inference/inference-charts",children:"Model Deployment"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Running?"})," Optimize with ",(0,s.jsx)(n.a,{href:"/ai-on-eks/docs/guidance/",children:"Guidance"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/awslabs/ai-on-eks",children:"AI on EKS GitHub Repository"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/",children:"AWS EKS Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/",children:"NVIDIA GPU Operator"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://awsdocs-neuron.readthedocs-hosted.com/",children:"AWS Neuron Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.vllm.ai/",children:"vLLM Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.ray.io/",children:"Ray Documentation"})}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"community-and-support",children:"Community and Support"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/awslabs/ai-on-eks/issues",children:"GitHub Issues"})," - Report bugs or request features"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://github.com/awslabs/ai-on-eks/discussions",children:"GitHub Discussions"})," - Ask questions and share experiences"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.a,{href:"https://repost.aws/",children:["AWS re",":Post"]})," - Get help from AWS experts and community"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},28453(e,n,i){i.d(n,{R:()=>l,x:()=>o});var r=i(96540);const s={},t=r.createContext(s);function l(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);
"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[6243],{28453:(e,n,l)=>{l.d(n,{R:()=>t,x:()=>a});var i=l(96540);const s={},r=i.createContext(s);function t(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),i.createElement(r.Provider,{value:n},e.children)}},29651:(e,n,l)=>{l.r(n),l.d(n,{assets:()=>o,contentTitle:()=>c,default:()=>u,frontMatter:()=>a,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"blueprints/inference/Neuron/llama4-trn2","title":"Llama 4 with vLLM on Trainium2","description":"This guide demonstrates deploying Llama 4 models using vLLM with NxD Inference on AWS Trainium2 instances.","source":"@site/docs/blueprints/inference/Neuron/llama4-trn2.md","sourceDirName":"blueprints/inference/Neuron","slug":"/blueprints/inference/Neuron/llama4-trn2","permalink":"/ai-on-eks/docs/blueprints/inference/Neuron/llama4-trn2","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/blueprints/inference/Neuron/llama4-trn2.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"title":"Llama 4 with vLLM on Trainium2","sidebar_position":7},"sidebar":"blueprints","previous":{"title":"Ray Serve High Availability","permalink":"/ai-on-eks/docs/blueprints/inference/Neuron/rayserve-ha"},"next":{"title":"Overview","permalink":"/ai-on-eks/docs/blueprints/inference/"}}');var s=l(74848),r=l(28453),t=l(42450);const a={title:"Llama 4 with vLLM on Trainium2",sidebar_position:7},c="Llama 4 with vLLM on Amazon EKS using Trainium2",o={},d=[{value:"Understanding Trainium2 Requirements",id:"understanding-trainium2-requirements",level:2},{value:"Model Memory Requirements",id:"model-memory-requirements",level:3},{value:"Trainium2 Instance Specifications",id:"trainium2-instance-specifications",level:3},{value:"Prerequisites",id:"prerequisites",level:3},{value:"EKS Cluster Requirements",id:"eks-cluster-requirements",level:3},{value:"Create EKS Cluster with Trainium2 Support",id:"create-eks-cluster-with-trainium2-support",level:3},{value:"Install Neuron Device Plugin",id:"install-neuron-device-plugin",level:3},{value:"Verify Neuron Devices",id:"verify-neuron-devices",level:3},{value:"Deploying Llama 4 Scout with vLLM",id:"deploying-llama-4-scout-with-vllm",level:2},{value:"Deploy Open WebUI",id:"deploy-open-webui",level:2},{value:"Testing with curl",id:"testing-with-curl",level:2},{value:"Text Completion",id:"text-completion",level:3},{value:"Multimodal (Image + Text)",id:"multimodal-image--text",level:3},{value:"Multiple Images",id:"multiple-images",level:3},{value:"Deploying Llama 4 Maverick",id:"deploying-llama-4-maverick",level:2},{value:"Neuron Configuration",id:"neuron-configuration",level:2},{value:"Monitoring",id:"monitoring",level:2},{value:"Check Pod Logs",id:"check-pod-logs",level:3},{value:"Check Neuron Device Utilization",id:"check-neuron-device-utilization",level:3},{value:"Cleanup",id:"cleanup",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"References",id:"references",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"llama-4-with-vllm-on-amazon-eks-using-trainium2",children:"Llama 4 with vLLM on Amazon EKS using Trainium2"})}),"\n",(0,s.jsxs)(n.p,{children:["This guide demonstrates deploying ",(0,s.jsx)(n.a,{href:"https://huggingface.co/meta-llama",children:"Llama 4"})," models using ",(0,s.jsx)(n.a,{href:"https://github.com/vllm-project/vllm",children:"vLLM"})," with ",(0,s.jsx)(n.a,{href:"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/",children:"NxD Inference"})," on ",(0,s.jsx)(n.a,{href:"https://aws.amazon.com/machine-learning/trainium/",children:"AWS Trainium2"})," instances."]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsxs)(n.p,{children:["This blueprint uses ",(0,s.jsx)(n.strong,{children:"AWS Trainium2 (trn2)"})," instances with the Neuron SDK for cost-effective inference. Llama 4 models support both text and image inputs (multimodal)."]})}),"\n",(0,s.jsx)(n.h2,{id:"understanding-trainium2-requirements",children:"Understanding Trainium2 Requirements"}),"\n",(0,s.jsx)(n.p,{children:"Llama 4 models use a Mixture of Experts (MoE) architecture that requires significant compute resources. Trainium2 provides excellent price-performance for these large models."}),"\n",(0,s.jsx)(n.h3,{id:"model-memory-requirements",children:"Model Memory Requirements"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"Active Params"}),(0,s.jsx)(n.th,{children:"Total Params"}),(0,s.jsx)(n.th,{children:"Experts"}),(0,s.jsx)(n.th,{children:"BF16 Memory"}),(0,s.jsx)(n.th,{children:"Instance Required"}),(0,s.jsx)(n.th,{children:"tensor_parallel_size"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Llama 4 Scout"}),(0,s.jsx)(n.td,{children:"17B"}),(0,s.jsx)(n.td,{children:"~109B"}),(0,s.jsx)(n.td,{children:"16"}),(0,s.jsx)(n.td,{children:"~220 GiB"}),(0,s.jsx)(n.td,{children:"trn2.48xlarge"}),(0,s.jsx)(n.td,{children:"64"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Llama 4 Maverick"}),(0,s.jsx)(n.td,{children:"17B"}),(0,s.jsx)(n.td,{children:"~400B"}),(0,s.jsx)(n.td,{children:"128"}),(0,s.jsx)(n.td,{children:"~800 GiB"}),(0,s.jsx)(n.td,{children:"trn2.48xlarge"}),(0,s.jsx)(n.td,{children:"64"})]})]})]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsxs)(n.p,{children:["Unlike GPU deployments, Trainium2 uses the ",(0,s.jsx)(n.strong,{children:"original BF16/FP16 models"})," without quantization. The Neuron SDK efficiently manages memory across all 64 Neuron cores."]})}),"\n",(0,s.jsx)(n.h3,{id:"trainium2-instance-specifications",children:"Trainium2 Instance Specifications"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Instance Type"}),(0,s.jsx)(n.th,{children:"Neuron Devices"}),(0,s.jsx)(n.th,{children:"Neuron Cores"}),(0,s.jsx)(n.th,{children:"HBM Memory"}),(0,s.jsx)(n.th,{children:"Use Case"})]})}),(0,s.jsx)(n.tbody,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"trn2.48xlarge"}),(0,s.jsx)(n.td,{children:"32"}),(0,s.jsx)(n.td,{children:"64"}),(0,s.jsx)(n.td,{children:"1.5 TiB"}),(0,s.jsx)(n.td,{children:"Scout & Maverick (BF16)"})]})})]}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsxs)(n.p,{children:["Llama 4 models require ",(0,s.jsx)(n.code,{children:"tensor_parallel_size=64"})," which means you need a full trn2.48xlarge instance with all 64 Neuron cores. The 1.5 TiB HBM memory is sufficient for both Scout and Maverick models in BF16 precision."]})}),"\n",(0,s.jsxs)(t.A,{header:(0,s.jsx)(n.h2,{children:(0,s.jsx)(n.span,{children:"Prerequisites and EKS Cluster Setup"})}),children:[(0,s.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"aws cli"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://eksctl.io/installation/",children:"eksctl"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://pypi.org/project/envsubst/",children:"envsubst"})}),"\n"]}),(0,s.jsx)(n.h3,{id:"eks-cluster-requirements",children:"EKS Cluster Requirements"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"EKS Version"}),": >= 1.30"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Trainium2 Node Group"}),": trn2.48xlarge instances"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Neuron Device Plugin"}),": Installed and configured"]}),"\n"]}),(0,s.jsx)(n.h3,{id:"create-eks-cluster-with-trainium2-support",children:"Create EKS Cluster with Trainium2 Support"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"eksctl create cluster \\\n  --name llama4-trn2-cluster \\\n  --region us-east-1 \\\n  --node-type trn2.48xlarge \\\n  --nodes 1 \\\n  --nodes-min 0 \\\n  --nodes-max 2\n"})}),(0,s.jsx)(n.h3,{id:"install-neuron-device-plugin",children:"Install Neuron Device Plugin"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f https://raw.githubusercontent.com/aws-neuron/aws-neuron-sdk/master/src/k8/k8s-neuron-device-plugin.yml\nkubectl apply -f https://raw.githubusercontent.com/aws-neuron/aws-neuron-sdk/master/src/k8/k8s-neuron-scheduler-eks.yml\n"})}),(0,s.jsx)(n.h3,{id:"verify-neuron-devices",children:"Verify Neuron Devices"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get nodes -o json | jq '.items[].status.allocatable[\"aws.amazon.com/neuron\"]'\n"})}),(0,s.jsx)(n.p,{children:"Expected output for trn2.48xlarge:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'"32"\n'})})]}),"\n",(0,s.jsx)(n.h2,{id:"deploying-llama-4-scout-with-vllm",children:"Deploying Llama 4 Scout with vLLM"}),"\n",(0,s.jsx)(n.admonition,{type:"caution",children:(0,s.jsxs)(n.p,{children:["The use of ",(0,s.jsx)(n.a,{href:"https://huggingface.co/meta-llama",children:"Llama 4"})," models requires access through a Hugging Face account. Make sure you have accepted the model license on HuggingFace."]})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Step 1:"})," Export the Hugging Face Hub Token"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'export HUGGING_FACE_HUB_TOKEN=$(echo -n "Your-Hugging-Face-Hub-Token-Value" | base64)\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Step 2:"})," Clone the repository"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/awslabs/ai-on-eks.git\ncd ai-on-eks\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Step 3:"})," Deploy the vLLM service"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd blueprints/inference/llama4-vllm-trn2/\nenvsubst < llama4-vllm-trn2-deployment.yaml | kubectl apply -f -\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Output:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"namespace/llama4-vllm created\nsecret/hf-token created\nconfigmap/llama4-neuron-config created\ndeployment.apps/llama4-vllm-trn2 created\nservice/llama4-vllm-trn2-svc created\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Step 4:"})," Monitor the deployment"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n llama4-vllm -w\n"})}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.p,{children:"The first deployment may take 30-60 minutes as the model needs to be compiled (traced) for Neuron. Subsequent deployments with cached artifacts will be faster."})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"NAME                                READY   STATUS    RESTARTS   AGE\nllama4-vllm-trn2-xxxxxxxxx-xxxxx    1/1     Running   0          45m\n"})}),"\n",(0,s.jsx)(n.h2,{id:"deploy-open-webui",children:"Deploy Open WebUI"}),"\n",(0,s.jsx)(n.p,{children:"Deploy Open WebUI for a ChatGPT-style interface:"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Step 1:"})," Deploy Open WebUI"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f open-webui.yaml\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Step 2:"})," Access the UI"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl -n open-webui port-forward svc/open-webui 8080:80\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Open ",(0,s.jsx)(n.a,{href:"http://localhost:8080",children:"http://localhost:8080"})," in your browser."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Step 3:"})," Register and start chatting"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Sign up with your name, email, and password"}),"\n",(0,s.jsx)(n.li,{children:'Click "New Chat"'}),"\n",(0,s.jsx)(n.li,{children:"Select the Llama 4 Scout model"}),"\n",(0,s.jsx)(n.li,{children:"Start chatting with text or upload images!"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"testing-with-curl",children:"Testing with curl"}),"\n",(0,s.jsx)(n.h3,{id:"text-completion",children:"Text Completion"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl -n llama4-vllm port-forward svc/llama4-vllm-trn2-svc 8000:8000\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",\n    "messages": [{"role": "user", "content": "What is Amazon EKS?"}],\n    "max_tokens": 100\n  }\'\n'})}),"\n",(0,s.jsx)(n.h3,{id:"multimodal-image--text",children:"Multimodal (Image + Text)"}),"\n",(0,s.jsx)(n.p,{children:"Llama 4 supports multimodal inputs with up to 5 images per prompt:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",\n    "messages": [{\n      "role": "user",\n      "content": [\n        {"type": "image_url", "image_url": {"url": "https://httpbin.org/image/png"}},\n        {"type": "text", "text": "Describe this image in detail"}\n      ]\n    }]\n  }\'\n'})}),"\n",(0,s.jsx)(n.h3,{id:"multiple-images",children:"Multiple Images"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",\n    "messages": [{\n      "role": "user",\n      "content": [\n        {"type": "image_url", "image_url": {"url": "https://httpbin.org/image/png"}},\n        {"type": "image_url", "image_url": {"url": "https://httpbin.org/image/png"}},\n        {"type": "text", "text": "Compare these two images"}\n      ]\n    }]\n  }\'\n'})}),"\n",(0,s.jsx)(n.h2,{id:"deploying-llama-4-maverick",children:"Deploying Llama 4 Maverick"}),"\n",(0,s.jsx)(n.p,{children:"For the larger Maverick model with 128 experts:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"envsubst < llama4-vllm-trn2-maverick.yaml | kubectl apply -f -\n"})}),"\n",(0,s.jsx)(n.p,{children:"Monitor deployment:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get pods -n llama4-vllm -l model=llama4-maverick -w\n"})}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsx)(n.p,{children:"Maverick model compilation takes significantly longer (60-90 minutes) due to the larger number of experts."})}),"\n",(0,s.jsx)(n.h2,{id:"neuron-configuration",children:"Neuron Configuration"}),"\n",(0,s.jsx)(n.p,{children:"The deployment includes optimized Neuron configuration for Llama 4:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n  "text_config": {\n    "batch_size": 1,\n    "is_continuous_batching": true,\n    "seq_len": 16384,\n    "torch_dtype": "float16",\n    "async_mode": true,\n    "world_size": 64,\n    "tp_degree": 64,\n    "cp_degree": 16\n  },\n  "vision_config": {\n    "batch_size": 1,\n    "seq_len": 8192,\n    "torch_dtype": "float16",\n    "tp_degree": 16,\n    "dp_degree": 4,\n    "world_size": 64\n  }\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:"Key parameters:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"tp_degree=64"}),": Tensor parallelism across all Neuron cores"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"cp_degree=16"}),": Context parallelism for efficient attention"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"async_mode=true"}),": Asynchronous execution for better throughput"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"enable_bucketing=true"}),": Dynamic batching for variable input lengths"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"monitoring",children:"Monitoring"}),"\n",(0,s.jsx)(n.h3,{id:"check-pod-logs",children:"Check Pod Logs"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl logs -n llama4-vllm -l app=llama4-vllm-trn2 -f\n"})}),"\n",(0,s.jsx)(n.h3,{id:"check-neuron-device-utilization",children:"Check Neuron Device Utilization"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl exec -n llama4-vllm -it $(kubectl get pods -n llama4-vllm -l app=llama4-vllm-trn2 -o jsonpath='{.items[0].metadata.name}') -- neuron-top\n"})}),"\n",(0,s.jsx)(n.h2,{id:"cleanup",children:"Cleanup"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl delete -f open-webui.yaml\nkubectl delete -f llama4-vllm-trn2-deployment.yaml\n# Or for Maverick:\nkubectl delete -f llama4-vllm-trn2-maverick.yaml\n"})}),"\n",(0,s.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Cost-Effective Inference"}),": Trainium2 provides excellent price-performance for large MoE models like Llama 4."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Multimodal Support"}),": Llama 4 on Trainium2 supports both text and image inputs."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"NxD Inference"}),": The Neuron SDK's NxD Inference library enables efficient distributed inference."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"OpenAI-Compatible API"}),": vLLM provides an OpenAI-compatible API for easy integration."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Model Compilation"}),": First deployment requires model compilation (tracing), but subsequent deployments are faster with cached artifacts."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/tutorials/llama4-tutorial.html",children:"NxD Inference Llama 4 Tutorial"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/aws-neuron/vllm-neuron",children:"vLLM-Neuron Plugin"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://awsdocs-neuron.readthedocs-hosted.com/",children:"AWS Neuron SDK Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/meta-llama",children:"Llama 4 on Hugging Face"})}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},42450:(e,n,l)=>{l.d(n,{A:()=>p});var i=l(96540),s=l(5556),r=l.n(s),t=l(34164);const a="collapsibleContent_q3kw",c="header_QCEw",o="icon_PckA",d="content_qLC1",h="expanded_iGsi";var u=l(74848);function m({children:e,header:n}){const[l,s]=(0,i.useState)(!1);return(0,u.jsxs)("div",{className:a,children:[(0,u.jsxs)("div",{className:(0,t.A)(c,{[h]:l}),onClick:()=>{s(!l)},children:[n,(0,u.jsx)("span",{className:(0,t.A)(o,{[h]:l}),children:l?"\ud83d\udc47":"\ud83d\udc48"})]}),l&&(0,u.jsx)("div",{className:d,children:e})]})}m.propTypes={children:r().node.isRequired,header:r().node.isRequired};const p=m}}]);
"use strict";(globalThis.webpackChunkdoeks_website=globalThis.webpackChunkdoeks_website||[]).push([[5216],{68426(e,n,s){s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>d});const l=JSON.parse('{"id":"blueprints/inference/framework-guides/GPUs/llama4-vllm","title":"Llama 4 with vLLM on EKS","description":"Deploy Llama 4 Scout and Maverick models using vLLM on Amazon EKS with GPU acceleration.","source":"@site/docs/blueprints/inference/framework-guides/GPUs/llama4-vllm.md","sourceDirName":"blueprints/inference/framework-guides/GPUs","slug":"/blueprints/inference/framework-guides/GPUs/llama4-vllm","permalink":"/ai-on-eks/docs/blueprints/inference/framework-guides/GPUs/llama4-vllm","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/blueprints/inference/framework-guides/GPUs/llama4-vllm.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"title":"Llama 4 with vLLM on EKS","sidebar_position":10,"description":"Deploy Llama 4 Scout and Maverick models using vLLM on Amazon EKS with GPU acceleration."},"sidebar":"blueprints","previous":{"title":"NVIDIA Enterprise RAG and AI-Q Research Assistant on EKS","permalink":"/ai-on-eks/docs/blueprints/inference/framework-guides/GPUs/nvidia-deep-research"},"next":{"title":"AIBrix on EKS","permalink":"/ai-on-eks/docs/blueprints/inference/framework-guides/GPUs/aibrix-deepseek-distill"}}');var i=s(74848),a=s(28453),t=s(42450);const r={title:"Llama 4 with vLLM on EKS",sidebar_position:10,description:"Deploy Llama 4 Scout and Maverick models using vLLM on Amazon EKS with GPU acceleration."},o="Llama 4 Inference with vLLM on Amazon EKS",c={},d=[{value:"Understanding GPU Memory Requirements",id:"understanding-gpu-memory-requirements",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Deploy the Cluster",id:"deploy-the-cluster",level:3},{value:"Configure kubectl",id:"configure-kubectl",level:3},{value:"Verify EKS Auto Mode Resources",id:"verify-eks-auto-mode-resources",level:3},{value:"Deploy Llama 4 Scout (17B-16E)",id:"deploy-llama-4-scout-17b-16e",level:2},{value:"Step 1: Create Hugging Face Token Secret",id:"step-1-create-hugging-face-token-secret",level:3},{value:"Step 2: Deploy with Helm",id:"step-2-deploy-with-helm",level:3},{value:"Step 3: Verify Deployment",id:"step-3-verify-deployment",level:3},{value:"Deploy Llama 4 Maverick (17B-128E) on GPU",id:"deploy-llama-4-maverick-17b-128e-on-gpu",level:2},{value:"Test the Model",id:"test-the-model",level:2},{value:"Port Forward",id:"port-forward",level:3},{value:"Chat Completion Request",id:"chat-completion-request",level:3},{value:"List Available Models",id:"list-available-models",level:3},{value:"Multimodal Request (Text + Image)",id:"multimodal-request-text--image",level:3},{value:"Deploy Open WebUI",id:"deploy-open-webui",level:2},{value:"Monitoring",id:"monitoring",level:2},{value:"Check Inference Logs",id:"check-inference-logs",level:3},{value:"GPU Utilization",id:"gpu-utilization",level:3},{value:"Cleanup",id:"cleanup",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.admonition,{type:"danger",children:(0,i.jsxs)(n.p,{children:["Use of Llama 4 models is governed by the ",(0,i.jsx)(n.a,{href:"https://www.llama.com/llama4/license/",children:"Meta Llama License"}),".\nPlease visit ",(0,i.jsx)(n.a,{href:"https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct",children:"Hugging Face"})," and accept the license before requesting access."]})}),"\n",(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"llama-4-inference-with-vllm-on-amazon-eks",children:"Llama 4 Inference with vLLM on Amazon EKS"})}),"\n",(0,i.jsxs)(n.p,{children:["In this guide, we'll explore deploying ",(0,i.jsx)(n.a,{href:"https://ai.meta.com/blog/llama-4-multimodal-intelligence/",children:"Llama 4"})," models using ",(0,i.jsx)(n.a,{href:"https://github.com/vllm-project/vllm",children:"vLLM"})," inference engine on ",(0,i.jsx)(n.a,{href:"https://aws.amazon.com/eks/",children:"Amazon EKS"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["Llama 4 introduces a ",(0,i.jsx)(n.strong,{children:"Mixture of Experts (MoE)"})," architecture, where only a subset of parameters are active per token, enabling efficient inference for its large total parameter count. Two model variants are available:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Llama 4 Scout"})," (17B active / 109B total, 16 experts) - Mid-size multimodal model"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Llama 4 Maverick"})," (17B active / 400B total, 128 experts) - Large multimodal model"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Both models support multimodal inputs (text and images) and provide OpenAI-compatible API endpoints via vLLM."}),"\n",(0,i.jsx)(n.h2,{id:"understanding-gpu-memory-requirements",children:"Understanding GPU Memory Requirements"}),"\n",(0,i.jsx)(n.p,{children:"Deploying MoE models requires loading all expert weights into GPU memory, even though only a subset is active per token. This means total parameter count determines memory requirements, not active parameters."}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{children:"Total Params"}),(0,i.jsx)(n.th,{children:"Experts"}),(0,i.jsx)(n.th,{children:"BF16 Memory"}),(0,i.jsx)(n.th,{children:"FP8 Memory"}),(0,i.jsx)(n.th,{children:"Recommended GPU Instance"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Scout 17B-16E"}),(0,i.jsx)(n.td,{children:"~109B"}),(0,i.jsx)(n.td,{children:"16"}),(0,i.jsx)(n.td,{children:"~220 GiB"}),(0,i.jsx)(n.td,{children:"~110 GiB"}),(0,i.jsx)(n.td,{children:"p4d.24xlarge (8x A100 40GB = 320 GiB)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Maverick 17B-128E"}),(0,i.jsx)(n.td,{children:"~400B"}),(0,i.jsx)(n.td,{children:"128"}),(0,i.jsx)(n.td,{children:"~800 GiB"}),(0,i.jsx)(n.td,{children:"~400 GiB"}),(0,i.jsxs)(n.td,{children:["p5.48xlarge (8x H100 80GB = 640 GiB, ",(0,i.jsx)(n.strong,{children:"FP8 required"}),")"]})]})]})]}),"\n",(0,i.jsxs)(n.admonition,{type:"warning",children:[(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Maverick on GPU requires FP8 quantization."})," The BF16 model weights (~800 GiB) exceed the p5.48xlarge total GPU memory (640 GiB). Use the FP8-quantized variant (",(0,i.jsx)(n.code,{children:"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8"}),") which fits within 640 GiB."]}),(0,i.jsxs)(n.p,{children:["For running Maverick without quantization, consider ",(0,i.jsx)(n.a,{href:"/docs/blueprints/inference/framework-guides/Neuron/llama4-trn2",children:"Trainium2 deployment"})," which provides 1.5 TiB HBM memory."]})]}),"\n",(0,i.jsxs)(t.A,{header:(0,i.jsx)(n.h2,{children:(0,i.jsx)(n.span,{children:"Deploying the Inference-Ready EKS Cluster"})}),children:[(0,i.jsxs)(n.p,{children:["This guide assumes you have an existing EKS cluster with GPU support. We recommend using the ",(0,i.jsx)(n.a,{href:"/docs/infra/inference/inference-ready-cluster",children:"Inference-Ready EKS Cluster"})," which comes pre-configured with EKS Auto Mode and GPU NodePool."]}),(0,i.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"aws cli"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://helm.sh/docs/intro/install/",children:"Helm 3.0+"})}),"\n"]}),(0,i.jsx)(n.h3,{id:"deploy-the-cluster",children:"Deploy the Cluster"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/awslabs/ai-on-eks.git\ncd ai-on-eks/infra/solutions/inference-ready-cluster\n"})}),(0,i.jsxs)(n.p,{children:["Ensure ",(0,i.jsx)(n.code,{children:"enable_eks_auto_mode = true"})," in ",(0,i.jsx)(n.code,{children:"terraform/blueprint.tfvars"}),", then run:"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"./install.sh\n"})}),(0,i.jsx)(n.h3,{id:"configure-kubectl",children:"Configure kubectl"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"aws eks --region <REGION> update-kubeconfig --name inference-cluster\n"})}),(0,i.jsx)(n.h3,{id:"verify-eks-auto-mode-resources",children:"Verify EKS Auto Mode Resources"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Verify NodePools (gpu, neuron, general-purpose, system)\nkubectl get nodepools\n\n# Verify nodes (GPU nodes are provisioned on-demand when workloads are scheduled)\nkubectl get nodes\n"})})]}),"\n",(0,i.jsx)(n.h2,{id:"deploy-llama-4-scout-17b-16e",children:"Deploy Llama 4 Scout (17B-16E)"}),"\n",(0,i.jsx)(n.h3,{id:"step-1-create-hugging-face-token-secret",children:"Step 1: Create Hugging Face Token Secret"}),"\n",(0,i.jsxs)(n.p,{children:["Create a ",(0,i.jsx)(n.a,{href:"https://huggingface.co/docs/hub/en/security-tokens",children:"Hugging Face access token"})," and store it as a Kubernetes secret:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl create secret generic hf-token --from-literal=token=<your-huggingface-token>\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-2-deploy-with-helm",children:"Step 2: Deploy with Helm"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"helm repo add ai-on-eks https://awslabs.github.io/ai-on-eks-charts/\nhelm repo update\n\nhelm install llama4-scout ai-on-eks/inference-charts \\\n  --values https://raw.githubusercontent.com/awslabs/ai-on-eks-charts/refs/heads/main/charts/inference-charts/values-llama-4-scout-17b-lws-vllm.yaml\n"})}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["The Scout model uses ",(0,i.jsx)(n.strong,{children:"LeaderWorkerSet (LWS)"})," for multi-node tensor parallelism across 8 GPUs. Model download and initialization may take several minutes on first deployment."]})}),"\n",(0,i.jsx)(n.h3,{id:"step-3-verify-deployment",children:"Step 3: Verify Deployment"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Check pod status\nkubectl get pods -l app.kubernetes.io/component=llama-4-scout\n\n# Watch logs for model loading progress\nkubectl logs -l app.kubernetes.io/component=llama-4-scout -f\n"})}),"\n",(0,i.jsx)(n.p,{children:"Wait until you see the vLLM server ready message in the logs:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"INFO:     Uvicorn running on http://0.0.0.0:8000\n"})}),"\n",(0,i.jsx)(n.h2,{id:"deploy-llama-4-maverick-17b-128e-on-gpu",children:"Deploy Llama 4 Maverick (17B-128E) on GPU"}),"\n",(0,i.jsx)(n.p,{children:"Maverick requires FP8 quantization on GPU due to its large model size (~800 GiB BF16)."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"helm install llama4-maverick ai-on-eks/inference-charts \\\n  --values https://raw.githubusercontent.com/awslabs/ai-on-eks-charts/refs/heads/main/charts/inference-charts/values-llama-4-maverick-17b-lws-vllm.yaml\n"})}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.p,{children:["Maverick deployment requires ",(0,i.jsx)(n.strong,{children:"p5.48xlarge"})," instances (8x H100 80GB). Ensure your AWS account has sufficient ",(0,i.jsx)(n.a,{href:"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html",children:"service quota"})," for P5 instances in your region."]})}),"\n",(0,i.jsx)(n.h2,{id:"test-the-model",children:"Test the Model"}),"\n",(0,i.jsx)(n.h3,{id:"port-forward",children:"Port Forward"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward svc/llama-4-scout 8000:8000\n"})}),"\n",(0,i.jsx)(n.h3,{id:"chat-completion-request",children:"Chat Completion Request"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",\n    "messages": [\n      {"role": "user", "content": "What are the key differences between Mixture of Experts and dense transformer models?"}\n    ],\n    "max_tokens": 512,\n    "temperature": 0.7\n  }\'\n'})}),"\n",(0,i.jsx)(n.h3,{id:"list-available-models",children:"List Available Models"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"curl http://localhost:8000/v1/models | python3 -m json.tool\n"})}),"\n",(0,i.jsx)(n.h3,{id:"multimodal-request-text--image",children:"Multimodal Request (Text + Image)"}),"\n",(0,i.jsx)(n.p,{children:"Llama 4 supports vision inputs. You can send image URLs alongside text:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",\n    "messages": [\n      {\n        "role": "user",\n        "content": [\n          {"type": "text", "text": "What do you see in this image?"},\n          {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg"}}\n        ]\n      }\n    ],\n    "max_tokens": 256\n  }\'\n'})}),"\n",(0,i.jsx)(n.h2,{id:"deploy-open-webui",children:"Deploy Open WebUI"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://github.com/open-webui/open-webui",children:"Open WebUI"})," provides a ChatGPT-style interface for interacting with the model."]}),"\n",(0,i.jsx)(n.p,{children:"The inference-ready cluster includes Open WebUI. To access it:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward svc/open-webui 8080:80 -n open-webui\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Open ",(0,i.jsx)(n.a,{href:"http://localhost:8080",children:"http://localhost:8080"})," in your browser and register a new account. The model will appear in the model selector."]}),"\n",(0,i.jsx)(n.h2,{id:"monitoring",children:"Monitoring"}),"\n",(0,i.jsx)(n.h3,{id:"check-inference-logs",children:"Check Inference Logs"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# View vLLM logs for throughput and latency metrics\nkubectl logs -l app.kubernetes.io/component=llama-4-scout --tail=100\n\n# Watch for token generation throughput\nkubectl logs -l app.kubernetes.io/component=llama-4-scout -f | grep "tokens/s"\n'})}),"\n",(0,i.jsx)(n.h3,{id:"gpu-utilization",children:"GPU Utilization"}),"\n",(0,i.jsx)(n.p,{children:"If the observability stack is enabled on your cluster, access Grafana for GPU metrics:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward svc/kube-prometheus-stack-grafana 3000:80 -n monitoring\n"})}),"\n",(0,i.jsx)(n.h2,{id:"cleanup",children:"Cleanup"}),"\n",(0,i.jsx)(n.p,{children:"Remove the model deployment:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Remove Scout\nhelm uninstall llama4-scout\n\n# Remove Maverick (if deployed)\nhelm uninstall llama4-maverick\n"})}),"\n",(0,i.jsx)(n.p,{children:"To destroy the entire cluster infrastructure:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd ai-on-eks/infra/solutions/inference-ready-cluster\n./cleanup.sh\n"})})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},42450(e,n,s){s.d(n,{A:()=>p});var l=s(96540),i=s(5556),a=s.n(i),t=s(34164);const r="collapsibleContent_q3kw",o="header_QCEw",c="icon_PckA",d="content_qLC1",h="expanded_iGsi";var u=s(74848);function m({children:e,header:n}){const[s,i]=(0,l.useState)(!1);return(0,u.jsxs)("div",{className:r,children:[(0,u.jsxs)("div",{className:(0,t.A)(o,{[h]:s}),onClick:()=>{i(!s)},children:[n,(0,u.jsx)("span",{className:(0,t.A)(c,{[h]:s}),children:s?"\ud83d\udc47":"\ud83d\udc48"})]}),s&&(0,u.jsx)("div",{className:d,children:e})]})}m.propTypes={children:a().node.isRequired,header:a().node.isRequired};const p=m},28453(e,n,s){s.d(n,{R:()=>t,x:()=>r});var l=s(96540);const i={},a=l.createContext(i);function t(e){const n=l.useContext(a);return l.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),l.createElement(a.Provider,{value:n},e.children)}}}]);
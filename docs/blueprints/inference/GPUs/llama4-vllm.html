<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-blueprints/inference/GPUs/llama4-vllm" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.0">
<title data-rh="true">Llama 4 with vLLM on EKS | AI on EKS</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://awslabs.github.io/ai-on-eks/docs/blueprints/inference/GPUs/llama4-vllm"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Llama 4 with vLLM on EKS | AI on EKS"><meta data-rh="true" name="description" content="In this guide, we&#x27;ll explore deploying Llama 4 models using vLLM inference engine on Amazon EKS with EKS Auto Mode for automatic GPU node provisioning."><meta data-rh="true" property="og:description" content="In this guide, we&#x27;ll explore deploying Llama 4 models using vLLM inference engine on Amazon EKS with EKS Auto Mode for automatic GPU node provisioning."><link data-rh="true" rel="icon" href="/ai-on-eks/img/header-icon.png"><link data-rh="true" rel="canonical" href="https://awslabs.github.io/ai-on-eks/docs/blueprints/inference/GPUs/llama4-vllm"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/docs/blueprints/inference/GPUs/llama4-vllm" hreflang="en"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/docs/blueprints/inference/GPUs/llama4-vllm" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Inference on EKS","item":"https://awslabs.github.io/ai-on-eks/docs/category/inference-on-eks"},{"@type":"ListItem","position":2,"name":"GPU Inference on EKS","item":"https://awslabs.github.io/ai-on-eks/docs/category/gpu-inference-on-eks"},{"@type":"ListItem","position":3,"name":"Llama 4 with vLLM on EKS","item":"https://awslabs.github.io/ai-on-eks/docs/blueprints/inference/GPUs/llama4-vllm"}]}</script><link rel="stylesheet" href="/ai-on-eks/assets/css/styles.c270b852.css">
<script src="/ai-on-eks/assets/js/runtime~main.89d82209.js" defer="defer"></script>
<script src="/ai-on-eks/assets/js/main.981a63f3.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="theme-announcement-bar announcementBar_mb4j" style="background-color:#667eea;color:#ffffff" role="banner"><div class="announcementBarPlaceholder_vyr4"></div><div class="content_knG7 announcementBarContent_xLdY">GenAI on EKS workshop series! <a target="_blank" rel="noopener noreferrer" href="https://aws-experience.com/emea/smb/events/series/get-hands-on-with-amazon-eks?trk=9be4af2e-2339-40ae-b5e9-57b6a7704c36&sc_channel=el" style="color: #ffffff; text-decoration: underline; font-weight: bold; margin-left: 10px;">Register now ‚Üí</a></div><button type="button" aria-label="Close" class="clean-btn close closeButton_CVFx announcementBarClose_gvF7"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai-on-eks/"><div class="navbar__logo"><img src="/ai-on-eks/img/header-icon.png" alt="AIoEKS Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai-on-eks/img/header-icon.png" alt="AIoEKS Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href="/ai-on-eks/docs/infra">Infrastructure</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/ai-on-eks/docs/blueprints">Blueprints</a><a class="navbar__item navbar__link" href="/ai-on-eks/docs/guidance">Guidance</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/awslabs/ai-on-eks" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai-on-eks/docs/blueprints"><span title="Overview" class="linkLabel_WmDU">Overview</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai-on-eks/docs/category/training-on-eks"><span title="Training on EKS" class="categoryLinkLabel_W154">Training on EKS</span></a><button aria-label="Expand sidebar category &#x27;Training on EKS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai-on-eks/docs/category/inference-on-eks"><span title="Inference on EKS" class="categoryLinkLabel_W154">Inference on EKS</span></a><button aria-label="Collapse sidebar category &#x27;Inference on EKS&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai-on-eks/docs/category/gpu-inference-on-eks"><span title="GPU Inference on EKS" class="categoryLinkLabel_W154">GPU Inference on EKS</span></a><button aria-label="Collapse sidebar category &#x27;GPU Inference on EKS&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-rayserve"><span title="RayServe with vLLM" class="linkLabel_WmDU">RayServe with vLLM</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-NVIDIATritonServer"><span title="NVIDIA Triton Server with vLLM" class="linkLabel_WmDU">NVIDIA Triton Server with vLLM</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/docs/blueprints/inference/GPUs/stablediffusion-gpus"><span title="Stable Diffusion on GPU" class="linkLabel_WmDU">Stable Diffusion on GPU</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-llama3"><span title="NVIDIA NIM LLM on Amazon EKS" class="linkLabel_WmDU">NVIDIA NIM LLM on Amazon EKS</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator"><span title="NVIDIA NIM Operator on EKS" class="linkLabel_WmDU">NVIDIA NIM Operator on EKS</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/docs/blueprints/inference/GPUs/ray-vllm-deepseek"><span title="DeepSeek-R1 on EKS" class="linkLabel_WmDU">DeepSeek-R1 on EKS</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai-on-eks/docs/blueprints/inference/GPUs/llama4-vllm"><span title="Llama 4 with vLLM on EKS" class="linkLabel_WmDU">Llama 4 with vLLM on EKS</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo"><span title="NVIDIA Dynamo on Amazon EKS" class="linkLabel_WmDU">NVIDIA Dynamo on Amazon EKS</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-deep-research"><span title="NVIDIA Enterprise RAG and AI-Q Research Assistant on EKS" class="linkLabel_WmDU">NVIDIA Enterprise RAG and AI-Q Research Assistant on EKS</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/docs/blueprints/inference/GPUs/aibrix-deepseek-distill"><span title="AIBrix on EKS" class="linkLabel_WmDU">AIBrix on EKS</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai-on-eks/docs/category/neuron-inference-on-eks"><span title="Neuron Inference on EKS" class="categoryLinkLabel_W154">Neuron Inference on EKS</span></a><button aria-label="Expand sidebar category &#x27;Neuron Inference on EKS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/docs/blueprints/inference"><span title="Overview" class="linkLabel_WmDU">Overview</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/docs/blueprints/inference/inference-charts"><span title="Inference Charts" class="linkLabel_WmDU">Inference Charts</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai-on-eks/docs/blueprints/gateways/envoy-gateway"><span title="gateways" class="categoryLinkLabel_W154">gateways</span></a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai-on-eks/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai-on-eks/docs/category/inference-on-eks"><span>Inference on EKS</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai-on-eks/docs/category/gpu-inference-on-eks"><span>GPU Inference on EKS</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Llama 4 with vLLM on EKS</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Llama 4 with vLLM on Amazon EKS</h1></header>
<p>In this guide, we&#x27;ll explore deploying <a href="https://huggingface.co/meta-llama" target="_blank" rel="noopener noreferrer">Llama 4</a> models using <a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">vLLM</a> inference engine on <a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener noreferrer">Amazon EKS</a> with EKS Auto Mode for automatic GPU node provisioning.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>This blueprint uses <strong>EKS Auto Mode</strong> for automatic GPU node provisioning. When you deploy a GPU workload, EKS automatically provisions the appropriate GPU nodes without requiring Karpenter or manual node group configuration.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="understanding-the-gpu-memory-requirements">Understanding the GPU Memory Requirements<a href="#understanding-the-gpu-memory-requirements" class="hash-link" aria-label="Direct link to Understanding the GPU Memory Requirements" title="Direct link to Understanding the GPU Memory Requirements" translate="no">‚Äã</a></h2>
<p>Deploying Llama 4 models requires careful memory planning. Llama 4 uses a Mixture of Experts (MoE) architecture where all expert weights must be loaded into GPU memory, even though only a subset of experts are activated per token.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="model-memory-requirements">Model Memory Requirements<a href="#model-memory-requirements" class="hash-link" aria-label="Direct link to Model Memory Requirements" title="Direct link to Model Memory Requirements" translate="no">‚Äã</a></h3>
<table><thead><tr><th>Model</th><th>Active Params</th><th>Total Params</th><th>BF16 Memory</th><th>FP8 Memory</th><th>Min GPU Config</th><th>tensor_parallel_size</th></tr></thead><tbody><tr><td>Llama 4 Scout (17B-16E)</td><td>17B</td><td>~109B</td><td>~220 GiB</td><td>~110 GiB</td><td>8x A100 (40GB)</td><td>8</td></tr><tr><td>Llama 4 Maverick (17B-128E)</td><td>17B</td><td>~400B</td><td>~800 GiB</td><td>~400 GiB</td><td>8x A100 (80GB)</td><td>8</td></tr></tbody></table>
<div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>warning</div><div class="admonitionContent_BuS1"><p>Llama 4 Maverick in BF16 requires ~800GB GPU memory, which exceeds the capacity of any single-node GPU instance (max 640GB on p4de/p5). This blueprint uses the <strong>FP8 quantized version</strong> (<code>RedHatAI/Llama-4-Maverick-17B-128E-Instruct-FP8</code>) which reduces memory requirements to ~400GB.</p></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="ec2-instance-selection-guide">EC2 Instance Selection Guide<a href="#ec2-instance-selection-guide" class="hash-link" aria-label="Direct link to EC2 Instance Selection Guide" title="Direct link to EC2 Instance Selection Guide" translate="no">‚Äã</a></h3>
<div class="theme-admonition theme-admonition-danger admonition_xJq3 alert alert--danger"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"></path></svg></span>danger</div><div class="admonitionContent_BuS1"><p>Llama 4 Scout requires <strong>at least 220GB of GPU memory</strong>. Common GPU instances like g5.48xlarge (8x A10G = 192GB) will fail with CUDA Out of Memory errors.</p></div></div>
<table><thead><tr><th>Instance Type</th><th>GPU</th><th>GPU Memory</th><th>Total VRAM</th><th>Scout (220GB)</th><th>Maverick FP8 (400GB)</th><th>Maverick BF16 (800GB)</th><th>Cost/hr</th></tr></thead><tbody><tr><td>g5.48xlarge</td><td>8x A10G</td><td>24GB each</td><td>192GB</td><td>‚ùå Insufficient</td><td>‚ùå Insufficient</td><td>‚ùå Insufficient</td><td>~$16</td></tr><tr><td>p4d.24xlarge</td><td>8x A100</td><td>40GB each</td><td>320GB</td><td>‚úÖ Supported</td><td>‚ùå Insufficient</td><td>‚ùå Insufficient</td><td>~$32</td></tr><tr><td>p4de.24xlarge</td><td>8x A100</td><td>80GB each</td><td>640GB</td><td>‚úÖ Recommended</td><td>‚úÖ Supported</td><td>‚ùå Insufficient</td><td>~$40</td></tr><tr><td>p5.48xlarge</td><td>8x H100</td><td>80GB each</td><td>640GB</td><td>‚úÖ Recommended</td><td>‚úÖ Supported</td><td>‚ùå Insufficient</td><td>~$98</td></tr><tr><td>p5e.48xlarge</td><td>8x H200</td><td>141GB each</td><td>1,128GB</td><td>‚úÖ Recommended</td><td>‚úÖ Supported</td><td>‚úÖ Supported</td><td>~$120+</td></tr></tbody></table>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>p5e.48xlarge with 8x H200 GPUs (1.1TB total VRAM) is the only single-node instance capable of running Maverick in BF16 without quantization. However, availability is limited to specific regions.</p></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="why-moe-models-need-more-memory">Why MoE Models Need More Memory<a href="#why-moe-models-need-more-memory" class="hash-link" aria-label="Direct link to Why MoE Models Need More Memory" title="Direct link to Why MoE Models Need More Memory" translate="no">‚Äã</a></h3>
<p>Unlike dense models where memory ‚âà 2 √ó parameters (for BF16), MoE models load <strong>all expert weights</strong> into memory:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Scout Memory = Base Model + (16 experts √ó expert_size) ‚âà 220GB</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Maverick Memory = Base Model + (128 experts √ó expert_size) ‚âà 800GB</span><br></span></code></pre></div></div>
<p>Even though only 1-2 experts are activated per token during inference, all experts must reside in GPU memory for fast routing.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="memory-optimization-options">Memory Optimization Options<a href="#memory-optimization-options" class="hash-link" aria-label="Direct link to Memory Optimization Options" title="Direct link to Memory Optimization Options" translate="no">‚Äã</a></h3>
<p>If you need to run on smaller GPUs, consider these alternatives:</p>
<ol>
<li><strong>AWQ Quantization</strong> (4-bit): Reduces memory by ~4x, but may have compatibility issues with vLLM for MoE models</li>
<li><strong>Smaller Models</strong>: Use Llama 3.1 8B/70B which have more predictable memory requirements</li>
<li><strong>Pipeline Parallelism</strong>: Split model across multiple nodes (requires LeaderWorkerSet)</li>
</ol>
<p>Using vLLM with <code>gpu-memory-utilization=0.9</code>, we optimize memory usage while preventing out-of-memory (OOM) crashes.</p>
<div class="collapsibleContent_q3kw"><div class="header_QCEw"><h2><span>Prerequisites and EKS Cluster Setup</span></h2><span class="icon_PckA">üëà</span></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploying-llama-4-scout-with-vllm">Deploying Llama 4 Scout with vLLM<a href="#deploying-llama-4-scout-with-vllm" class="hash-link" aria-label="Direct link to Deploying Llama 4 Scout with vLLM" title="Direct link to Deploying Llama 4 Scout with vLLM" translate="no">‚Äã</a></h2>
<p>With the EKS cluster ready, we can now deploy Llama 4 Scout using vLLM.</p>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>caution</div><div class="admonitionContent_BuS1"><p>The use of <a href="https://huggingface.co/meta-llama" target="_blank" rel="noopener noreferrer">Llama 4</a> models requires access through a Hugging Face account. Make sure you have accepted the model license on HuggingFace.</p></div></div>
<p><strong>Step 1:</strong> Export the Hugging Face Hub Token</p>
<p>Create a Hugging Face account and generate an access token:</p>
<ol>
<li>Navigate to <a href="https://huggingface.co/settings/tokens" target="_blank" rel="noopener noreferrer">Hugging Face Settings ‚Üí Access Tokens</a></li>
<li>Create a new token with read permissions</li>
<li>Export the token as a base64-encoded environment variable:</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">export</span><span class="token plain"> </span><span class="token assign-left variable" style="color:#36acaa">HUGGING_FACE_HUB_TOKEN</span><span class="token operator" style="color:#393A34">=</span><span class="token variable" style="color:#36acaa">$(</span><span class="token variable builtin class-name" style="color:#36acaa">echo</span><span class="token variable" style="color:#36acaa"> </span><span class="token variable parameter variable" style="color:#36acaa">-n</span><span class="token variable" style="color:#36acaa"> </span><span class="token variable string" style="color:#e3116c">&quot;Your-Hugging-Face-Hub-Token-Value&quot;</span><span class="token variable" style="color:#36acaa"> </span><span class="token variable operator" style="color:#393A34">|</span><span class="token variable" style="color:#36acaa"> base64</span><span class="token variable" style="color:#36acaa">)</span><br></span></code></pre></div></div>
<p><strong>Step 2:</strong> Clone the repository</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">git</span><span class="token plain"> clone https://github.com/awslabs/ai-on-eks.git</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token builtin class-name">cd</span><span class="token plain"> ai-on-eks</span><br></span></code></pre></div></div>
<p><strong>Step 3:</strong> Deploy the vLLM service</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> blueprints/inference/llama4-vllm-gpu/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">envsubst </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain"> llama4-vllm-deployment.yml </span><span class="token operator" style="color:#393A34">|</span><span class="token plain"> kubectl apply </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> -</span><br></span></code></pre></div></div>
<p><strong>Output:</strong></p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">namespace/llama4-vllm created</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">secret/hf-token created</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">deployment.apps/llama4-vllm created</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">service/llama4-vllm-svc created</span><br></span></code></pre></div></div>
<p><strong>Step 4:</strong> Monitor the deployment</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get pods </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> llama4-vllm </span><span class="token parameter variable" style="color:#36acaa">-w</span><br></span></code></pre></div></div>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>The first deployment may take 10-15 minutes as EKS Auto Mode provisions a GPU node and the model weights are downloaded from HuggingFace.</p></div></div>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">NAME                           READY   STATUS    RESTARTS   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">llama4-vllm-xxxxxxxxx-xxxxx    1/1     Running   0          10m</span><br></span></code></pre></div></div>
<p><strong>Step 5:</strong> Verify the service</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get svc </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> llama4-vllm</span><br></span></code></pre></div></div>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">llama4-vllm-svc   ClusterIP   172.20.xxx.xx   &lt;none&gt;        8000/TCP   10m</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-open-webui-and-chat-with-llama-4">Deploy Open WebUI and Chat with Llama 4<a href="#deploy-open-webui-and-chat-with-llama-4" class="hash-link" aria-label="Direct link to Deploy Open WebUI and Chat with Llama 4" title="Direct link to Deploy Open WebUI and Chat with Llama 4" translate="no">‚Äã</a></h2>
<p>Now, let&#x27;s deploy Open WebUI, which provides a ChatGPT-style chat interface to interact with the Llama 4 model.</p>
<p><strong>Step 1:</strong> Deploy Open WebUI</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> open-webui.yaml</span><br></span></code></pre></div></div>
<p><strong>Output:</strong></p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">namespace/open-webui created</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">deployment.apps/open-webui created</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">service/open-webui created</span><br></span></code></pre></div></div>
<p><strong>Step 2:</strong> Verify the deployment</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get pods </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> open-webui</span><br></span></code></pre></div></div>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">NAME                          READY   STATUS    RESTARTS   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">open-webui-xxxxxxxxx-xxxxx    1/1     Running   0          2m</span><br></span></code></pre></div></div>
<p><strong>Step 3:</strong> Access the Open WebUI</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> open-webui port-forward svc/open-webui </span><span class="token number" style="color:#36acaa">8080</span><span class="token plain">:80</span><br></span></code></pre></div></div>
<p>Open your browser and navigate to <a href="http://localhost:8080" target="_blank" rel="noopener noreferrer">http://localhost:8080</a></p>
<p><strong>Step 4:</strong> Register and start chatting</p>
<ol>
<li>Sign up with your name, email, and password</li>
<li>Click &quot;New Chat&quot;</li>
<li>Select the Llama 4 Scout model from the dropdown</li>
<li>Start chatting!</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="testing-with-curl-optional">Testing with curl (Optional)<a href="#testing-with-curl-optional" class="hash-link" aria-label="Direct link to Testing with curl (Optional)" title="Direct link to Testing with curl (Optional)" translate="no">‚Äã</a></h2>
<p>You can also test the Llama 4 model directly using curl commands.</p>
<p><strong>Step 1:</strong> Port-forward the vLLM service</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> llama4-vllm port-forward svc/llama4-vllm-svc </span><span class="token number" style="color:#36acaa">8000</span><span class="token plain">:8000</span><br></span></code></pre></div></div>
<p><strong>Step 2:</strong> Test the /v1/models endpoint</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">curl</span><span class="token plain"> http://localhost:8000/v1/models</span><br></span></code></pre></div></div>
<p><strong>Response:</strong></p>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;object&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;list&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;data&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token property" style="color:#36acaa">&quot;id&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;meta-llama/Llama-4-Scout-17B-16E-Instruct&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token property" style="color:#36acaa">&quot;object&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;model&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token property" style="color:#36acaa">&quot;created&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1234567890</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token property" style="color:#36acaa">&quot;owned_by&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;vllm&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre></div></div>
<p><strong>Step 3:</strong> Test chat completion</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">curl</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">-X</span><span class="token plain"> POST http://localhost:8000/v1/chat/completions </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">-H</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Content-Type: application/json&quot;</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">-d</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;{</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;model&quot;: &quot;meta-llama/Llama-4-Scout-17B-16E-Instruct&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Explain what Amazon EKS is in 2 sentences.&quot;}],</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;max_tokens&quot;: 100,</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;stream&quot;: false</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">  }&#x27;</span><br></span></code></pre></div></div>
<p><strong>Response:</strong></p>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;id&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;chatcmpl-xxxxx&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;object&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;chat.completion&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;created&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1234567890</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;model&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;meta-llama/Llama-4-Scout-17B-16E-Instruct&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;choices&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token property" style="color:#36acaa">&quot;index&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token property" style="color:#36acaa">&quot;message&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">&quot;role&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;assistant&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token property" style="color:#36acaa">&quot;content&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Amazon Elastic Kubernetes Service (EKS) is a managed container orchestration service that makes it easy to run Kubernetes on AWS without needing to install and operate your own Kubernetes control plane. It automatically manages the availability and scalability of the Kubernetes control plane nodes, handles upgrades, and integrates with other AWS services for security, networking, and monitoring.&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token property" style="color:#36acaa">&quot;finish_reason&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;stop&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token property" style="color:#36acaa">&quot;usage&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token property" style="color:#36acaa">&quot;prompt_tokens&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">15</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token property" style="color:#36acaa">&quot;completion_tokens&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">75</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token property" style="color:#36acaa">&quot;total_tokens&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">90</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre></div></div>
<p><strong>Step 4:</strong> Test streaming response</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">curl</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">-X</span><span class="token plain"> POST http://localhost:8000/v1/chat/completions </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">-H</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Content-Type: application/json&quot;</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">-d</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;{</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;model&quot;: &quot;meta-llama/Llama-4-Scout-17B-16E-Instruct&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}],</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;stream&quot;: true</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">  }&#x27;</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="monitoring-and-observability">Monitoring and Observability<a href="#monitoring-and-observability" class="hash-link" aria-label="Direct link to Monitoring and Observability" title="Direct link to Monitoring and Observability" translate="no">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="check-vllm-pod-logs">Check vLLM Pod Logs<a href="#check-vllm-pod-logs" class="hash-link" aria-label="Direct link to Check vLLM Pod Logs" title="Direct link to Check vLLM Pod Logs" translate="no">‚Äã</a></h3>
<p>Monitor the vLLM server logs to check model loading status and inference metrics:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl logs </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> llama4-vllm </span><span class="token parameter variable" style="color:#36acaa">-l</span><span class="token plain"> </span><span class="token assign-left variable" style="color:#36acaa">app</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">llama4-vllm </span><span class="token parameter variable" style="color:#36acaa">-f</span><br></span></code></pre></div></div>
<p><strong>Key metrics to watch in logs:</strong></p>
<ul>
<li><strong>Token throughput</strong>: <code>Avg prompt throughput: X tokens/s, Avg generation throughput: Y tokens/s</code></li>
<li><strong>GPU KV Cache utilization</strong>: <code>GPU KV cache usage: X%</code></li>
<li><strong>Request processing</strong>: <code>Received request</code> and <code>Finished request</code> entries</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="check-gpu-utilization">Check GPU Utilization<a href="#check-gpu-utilization" class="hash-link" aria-label="Direct link to Check GPU Utilization" title="Direct link to Check GPU Utilization" translate="no">‚Äã</a></h3>
<p>If you have NVIDIA DCGM Exporter or similar monitoring tools deployed:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Check GPU memory usage on the node</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl </span><span class="token builtin class-name">exec</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> llama4-vllm </span><span class="token parameter variable" style="color:#36acaa">-it</span><span class="token plain"> </span><span class="token variable" style="color:#36acaa">$(</span><span class="token variable" style="color:#36acaa">kubectl get pods </span><span class="token variable parameter variable" style="color:#36acaa">-n</span><span class="token variable" style="color:#36acaa"> llama4-vllm </span><span class="token variable parameter variable" style="color:#36acaa">-l</span><span class="token variable" style="color:#36acaa"> </span><span class="token variable assign-left variable" style="color:#36acaa">app</span><span class="token variable operator" style="color:#393A34">=</span><span class="token variable" style="color:#36acaa">llama4-vllm </span><span class="token variable parameter variable" style="color:#36acaa">-o</span><span class="token variable" style="color:#36acaa"> </span><span class="token variable assign-left variable" style="color:#36acaa">jsonpath</span><span class="token variable operator" style="color:#393A34">=</span><span class="token variable string" style="color:#e3116c">&#x27;{.items[0].metadata.name}&#x27;</span><span class="token variable" style="color:#36acaa">)</span><span class="token plain"> -- nvidia-smi</span><br></span></code></pre></div></div>
<p><strong>Expected output:</strong></p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">+-----------------------------------------------------------------------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| NVIDIA-SMI 535.xx.xx    Driver Version: 535.xx.xx    CUDA Version: 12.x     |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|-------------------------------+----------------------+----------------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|===============================+======================+======================|</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|   0  NVIDIA A10G         On   | 00000000:00:1E.0 Off |                    0 |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  0%   45C    P0    70W / 300W |  18000MiB / 24576MiB |     25%      Default |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-------------------------------+----------------------+----------------------+</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="performance-metrics">Performance Metrics<a href="#performance-metrics" class="hash-link" aria-label="Direct link to Performance Metrics" title="Direct link to Performance Metrics" translate="no">‚Äã</a></h3>
<p>vLLM provides built-in metrics at the <code>/metrics</code> endpoint. If you haven&#x27;t already, set up port-forwarding:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> llama4-vllm port-forward svc/llama4-vllm-svc </span><span class="token number" style="color:#36acaa">8000</span><span class="token plain">:8000</span><br></span></code></pre></div></div>
<p>Then query the metrics:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">curl</span><span class="token plain"> http://localhost:8000/metrics</span><br></span></code></pre></div></div>
<p>Key metrics include:</p>
<ul>
<li><code>vllm:num_requests_running</code> - Number of requests currently being processed</li>
<li><code>vllm:num_requests_waiting</code> - Number of requests waiting in queue</li>
<li><code>vllm:gpu_cache_usage_perc</code> - GPU KV cache utilization percentage</li>
<li><code>vllm:avg_generation_throughput_toks_per_s</code> - Average token generation throughput</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploying-llama-4-maverick-multi-gpu">Deploying Llama 4 Maverick (Multi-GPU)<a href="#deploying-llama-4-maverick-multi-gpu" class="hash-link" aria-label="Direct link to Deploying Llama 4 Maverick (Multi-GPU)" title="Direct link to Deploying Llama 4 Maverick (Multi-GPU)" translate="no">‚Äã</a></h2>
<p>For the larger Maverick model with 128 experts, you need multiple GPUs with tensor parallelism.</p>
<div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>warning</div><div class="admonitionContent_BuS1"><p>Llama 4 Maverick in BF16 requires ~800GB GPU memory, which exceeds the capacity of any single-node instance. This blueprint uses the <strong>FP8 quantized version</strong> from RedHatAI, which reduces memory to ~400GB and fits on p4de.24xlarge or p5.48xlarge instances.</p></div></div>
<div class="theme-admonition theme-admonition-danger admonition_xJq3 alert alert--danger"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"></path></svg></span>danger</div><div class="admonitionContent_BuS1"><p>Important: Deploying Llama 4 Maverick requires 8x A100 (80GB) or 8x H100 (80GB) GPUs. This can be very expensive. Ensure you monitor your usage carefully.</p></div></div>
<p><strong>Step 1:</strong> Deploy the 70B model</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> ai-on-eks/blueprints/inference/llama4-vllm-gpu/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">envsubst </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain"> llama4-vllm-deployment-70b.yml </span><span class="token operator" style="color:#393A34">|</span><span class="token plain"> kubectl apply </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> -</span><br></span></code></pre></div></div>
<p><strong>Step 2:</strong> Monitor the deployment</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get pods </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> llama4-vllm </span><span class="token parameter variable" style="color:#36acaa">-l</span><span class="token plain"> </span><span class="token assign-left variable" style="color:#36acaa">model</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">llama4-maverick </span><span class="token parameter variable" style="color:#36acaa">-w</span><br></span></code></pre></div></div>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>The Maverick model deployment may take 30-45 minutes due to larger model weights download and multi-GPU initialization.</p></div></div>
<p><strong>Step 3:</strong> Test the Maverick model</p>
<p>You can test using curl:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> llama4-vllm port-forward svc/llama4-vllm-70b-svc </span><span class="token number" style="color:#36acaa">8001</span><span class="token plain">:8000</span><br></span></code></pre></div></div>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">curl</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">-X</span><span class="token plain"> POST http://localhost:8001/v1/chat/completions </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">-H</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Content-Type: application/json&quot;</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">-d</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;{</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;model&quot;: &quot;RedHatAI/Llama-4-Maverick-17B-128E-Instruct-FP8&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}]</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">  }&#x27;</span><br></span></code></pre></div></div>
<p><strong>Step 4:</strong> Add Maverick to Open WebUI</p>
<p>If you want to use both Scout and Maverick models in Open WebUI, update the environment variable to include both services:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl </span><span class="token builtin class-name">set</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">env</span><span class="token plain"> deployment/open-webui </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> open-webui </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token assign-left variable" style="color:#36acaa">OPENAI_API_BASE_URLS</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;http://llama4-vllm-svc.llama4-vllm.svc.cluster.local:8000/v1;http://llama4-vllm-70b-svc.llama4-vllm.svc.cluster.local:8000/v1&quot;</span><br></span></code></pre></div></div>
<p>Wait for the Open WebUI pod to restart:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl rollout status deployment/open-webui </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> open-webui</span><br></span></code></pre></div></div>
<p>Now both models will appear in the Open WebUI model dropdown.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="cleanup">Cleanup<a href="#cleanup" class="hash-link" aria-label="Direct link to Cleanup" title="Direct link to Cleanup" translate="no">‚Äã</a></h2>
<p>To remove all deployed resources:</p>
<p><strong>Step 1:</strong> Delete Open WebUI</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl delete </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> open-webui.yaml</span><br></span></code></pre></div></div>
<p><strong>Step 2:</strong> Delete Llama 4 vLLM deployment</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl delete </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> llama4-vllm-deployment.yml</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Or for 70B model:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl delete </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> llama4-vllm-deployment-70b.yml</span><br></span></code></pre></div></div>
<p><strong>Step 3:</strong> Delete namespaces (optional)</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl delete namespace llama4-vllm</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl delete namespace open-webui</span><br></span></code></pre></div></div>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>After deleting the GPU workloads, EKS Auto Mode will automatically terminate idle GPU nodes to reduce costs.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-takeaways">Key Takeaways<a href="#key-takeaways" class="hash-link" aria-label="Direct link to Key Takeaways" title="Direct link to Key Takeaways" translate="no">‚Äã</a></h2>
<ol>
<li>
<p><strong>EKS Auto Mode Simplifies GPU Provisioning</strong>: No need to configure Karpenter or manage node groups manually.</p>
</li>
<li>
<p><strong>vLLM Provides High Performance</strong>: Optimized memory management with PagedAttention enables efficient inference.</p>
</li>
<li>
<p><strong>OpenAI-Compatible API</strong>: Easy integration with existing tools and applications.</p>
</li>
<li>
<p><strong>Scalable Architecture</strong>: Support for both single-GPU (8B) and multi-GPU (70B) deployments.</p>
</li>
<li>
<p><strong>Cost Optimization</strong>: EKS Auto Mode automatically terminates idle GPU nodes.</p>
</li>
</ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/awslabs/ai-on-eks/blob/main/website/docs/blueprints/inference/GPUs/llama4-vllm.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai-on-eks/docs/blueprints/inference/GPUs/ray-vllm-deepseek"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">DeepSeek-R1 on EKS</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">NVIDIA Dynamo on Amazon EKS</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#understanding-the-gpu-memory-requirements" class="table-of-contents__link toc-highlight">Understanding the GPU Memory Requirements</a><ul><li><a href="#model-memory-requirements" class="table-of-contents__link toc-highlight">Model Memory Requirements</a></li><li><a href="#ec2-instance-selection-guide" class="table-of-contents__link toc-highlight">EC2 Instance Selection Guide</a></li><li><a href="#why-moe-models-need-more-memory" class="table-of-contents__link toc-highlight">Why MoE Models Need More Memory</a></li><li><a href="#memory-optimization-options" class="table-of-contents__link toc-highlight">Memory Optimization Options</a></li><li><a href="#prerequisites" class="table-of-contents__link toc-highlight">Prerequisites</a></li><li><a href="#eks-cluster-requirements" class="table-of-contents__link toc-highlight">EKS Cluster Requirements</a></li><li><a href="#option-a-create-a-new-eks-auto-mode-cluster" class="table-of-contents__link toc-highlight">Option A: Create a New EKS Auto Mode Cluster</a></li><li><a href="#option-b-use-an-existing-eks-cluster" class="table-of-contents__link toc-highlight">Option B: Use an Existing EKS Cluster</a></li><li><a href="#step-3-configure-gpu-nodepool" class="table-of-contents__link toc-highlight">Step 3: Configure GPU NodePool</a></li></ul></li><li><a href="#deploying-llama-4-scout-with-vllm" class="table-of-contents__link toc-highlight">Deploying Llama 4 Scout with vLLM</a></li><li><a href="#deploy-open-webui-and-chat-with-llama-4" class="table-of-contents__link toc-highlight">Deploy Open WebUI and Chat with Llama 4</a></li><li><a href="#testing-with-curl-optional" class="table-of-contents__link toc-highlight">Testing with curl (Optional)</a></li><li><a href="#monitoring-and-observability" class="table-of-contents__link toc-highlight">Monitoring and Observability</a><ul><li><a href="#check-vllm-pod-logs" class="table-of-contents__link toc-highlight">Check vLLM Pod Logs</a></li><li><a href="#check-gpu-utilization" class="table-of-contents__link toc-highlight">Check GPU Utilization</a></li><li><a href="#performance-metrics" class="table-of-contents__link toc-highlight">Performance Metrics</a></li></ul></li><li><a href="#deploying-llama-4-maverick-multi-gpu" class="table-of-contents__link toc-highlight">Deploying Llama 4 Maverick (Multi-GPU)</a></li><li><a href="#cleanup" class="table-of-contents__link toc-highlight">Cleanup</a></li><li><a href="#key-takeaways" class="table-of-contents__link toc-highlight">Key Takeaways</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Get Involved</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/awslabs/ai-on-eks" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Built with ‚ù§Ô∏è at AWS  <br> ¬© 2026 Amazon.com, Inc. or its affiliates. All Rights Reserved</div></div></div></footer><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;7fbc7ab02fae4767b1af2588eba0cdf2&quot;}"></script></div>
</body>
</html>
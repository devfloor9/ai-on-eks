"use strict";(globalThis.webpackChunkdoeks_website=globalThis.webpackChunkdoeks_website||[]).push([[8742],{52357(e,n,i){i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>u,frontMatter:()=>o,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"blueprints/inference/framework-guides/Neuron/llama4-trn2","title":"Llama 4 with vLLM on Trainium","description":"Deploy Llama 4 models using vLLM on AWS Trainium instances with EKS and Karpenter.","source":"@site/docs/blueprints/inference/framework-guides/Neuron/llama4-trn2.md","sourceDirName":"blueprints/inference/framework-guides/Neuron","slug":"/blueprints/inference/framework-guides/Neuron/llama4-trn2","permalink":"/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama4-trn2","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/blueprints/inference/framework-guides/Neuron/llama4-trn2.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"title":"Llama 4 with vLLM on Trainium","sidebar_position":7,"description":"Deploy Llama 4 models using vLLM on AWS Trainium instances with EKS and Karpenter."},"sidebar":"blueprints","previous":{"title":"Ray Serve \uace0\uac00\uc6a9\uc131","permalink":"/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/rayserve-ha"},"next":{"title":"\ucd94\ub860 \ucc28\ud2b8","permalink":"/ai-on-eks/ko/docs/blueprints/inference/inference-charts"}}');var s=i(74848),l=i(28453),t=i(42450);const o={title:"Llama 4 with vLLM on Trainium",sidebar_position:7,description:"Deploy Llama 4 models using vLLM on AWS Trainium instances with EKS and Karpenter."},a="Llama 4 Inference with vLLM on AWS Trainium",d={},c=[{value:"Why Trainium for Llama 4?",id:"why-trainium-for-llama-4",level:2},{value:"Memory Comparison: GPU vs Trainium",id:"memory-comparison-gpu-vs-trainium",level:3},{value:"Model Compilation",id:"model-compilation",level:2},{value:"Software Versions",id:"software-versions",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Deploy the Cluster",id:"deploy-the-cluster",level:3},{value:"Configure kubectl",id:"configure-kubectl",level:3},{value:"Verify Karpenter Resources",id:"verify-karpenter-resources",level:3},{value:"Install Neuron Device Plugin",id:"install-neuron-device-plugin",level:3},{value:"Neuron Resource Names",id:"neuron-resource-names",level:3},{value:"Deploy Llama 4 Scout on Trainium",id:"deploy-llama-4-scout-on-trainium",level:2},{value:"Step 1: Create Hugging Face Token Secret",id:"step-1-create-hugging-face-token-secret",level:3},{value:"Step 2: Deploy with Helm",id:"step-2-deploy-with-helm",level:3},{value:"Step 3: Monitor Deployment",id:"step-3-monitor-deployment",level:3},{value:"Deploy Llama 4 Maverick on Trainium2",id:"deploy-llama-4-maverick-on-trainium2",level:2},{value:"Test the Model",id:"test-the-model",level:2},{value:"Port Forward",id:"port-forward",level:3},{value:"Chat Completion Request",id:"chat-completion-request",level:3},{value:"List Available Models",id:"list-available-models",level:3},{value:"Multimodal Request (Text + Image)",id:"multimodal-request-text--image",level:3},{value:"Deploy Open WebUI",id:"deploy-open-webui",level:2},{value:"Monitoring",id:"monitoring",level:2},{value:"Check Inference Logs",id:"check-inference-logs",level:3},{value:"Observability Dashboard",id:"observability-dashboard",level:3},{value:"Cleanup",id:"cleanup",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.admonition,{type:"danger",children:(0,s.jsxs)(n.p,{children:["Use of Llama 4 models is governed by the ",(0,s.jsx)(n.a,{href:"https://www.llama.com/llama4/license/",children:"Meta Llama License"}),".\nPlease visit ",(0,s.jsx)(n.a,{href:"https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct",children:"Hugging Face"})," and accept the license before requesting access."]})}),"\n",(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"llama-4-inference-with-vllm-on-aws-trainium",children:"Llama 4 Inference with vLLM on AWS Trainium"})}),"\n",(0,s.jsxs)(n.p,{children:["This guide covers deploying ",(0,s.jsx)(n.a,{href:"https://ai.meta.com/blog/llama-4-multimodal-intelligence/",children:"Llama 4"})," models using ",(0,s.jsx)(n.a,{href:"https://github.com/vllm-project/vllm",children:"vLLM"})," with ",(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/optimum-neuron/index",children:"optimum-neuron"})," on AWS Trainium instances."]}),"\n",(0,s.jsxs)(n.admonition,{title:"Model Compilation Required",type:"warning",children:[(0,s.jsxs)(n.p,{children:["Llama 4 inference on Neuron is supported via ",(0,s.jsx)(n.strong,{children:"optimum-neuron >= 0.4.0"})," with the ",(0,s.jsx)(n.code,{children:"Llama4NeuronModelForCausalLM"})," class. However, the first deployment requires ",(0,s.jsx)(n.strong,{children:"Neuron model compilation"}),", which happens automatically when ",(0,s.jsx)(n.code,{children:"vllm serve"})," runs but can take ",(0,s.jsx)(n.strong,{children:"30-60+ minutes"}),". Pre-compiled artifacts may not yet be available in the ",(0,s.jsx)(n.a,{href:"https://huggingface.co/aws-neuron/optimum-neuron-cache",children:"optimum-neuron-cache"})," for all configurations."]}),(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"optimum-cli export neuron"})," command does ",(0,s.jsx)(n.strong,{children:"not"})," support Llama 4. Use ",(0,s.jsx)(n.code,{children:"vllm serve"})," directly, which invokes the inference-path compilation internally."]})]}),"\n",(0,s.jsx)(n.h2,{id:"why-trainium-for-llama-4",children:"Why Trainium for Llama 4?"}),"\n",(0,s.jsx)(n.p,{children:"AWS Trainium provides large HBM memory capacity, making it an excellent choice for large MoE models like Llama 4:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Instance"}),(0,s.jsx)(n.th,{children:"Chips"}),(0,s.jsx)(n.th,{children:"NeuronCores"}),(0,s.jsx)(n.th,{children:"HBM Memory"}),(0,s.jsx)(n.th,{children:"Karpenter"}),(0,s.jsx)(n.th,{children:"EKS Auto Mode"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"trn1.32xlarge"}),(0,s.jsx)(n.td,{children:"16 Trainium v1"}),(0,s.jsx)(n.td,{children:"32"}),(0,s.jsx)(n.td,{children:"512 GiB"}),(0,s.jsx)(n.td,{children:"Supported"}),(0,s.jsx)(n.td,{children:"Supported"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"trn2.48xlarge"}),(0,s.jsx)(n.td,{children:"16 Trainium v2"}),(0,s.jsx)(n.td,{children:"64"}),(0,s.jsx)(n.td,{children:"1.5 TiB"}),(0,s.jsx)(n.td,{children:"Supported"}),(0,s.jsx)(n.td,{children:"Not yet supported"})]})]})]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Advantage"}),(0,s.jsx)(n.th,{children:"Detail"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"No quantization needed"})}),(0,s.jsx)(n.td,{children:"Both trn1 (512 GiB) and trn2 (1.5 TiB) support Scout (~220 GiB) in native BF16"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Karpenter auto-provisioning"})}),(0,s.jsx)(n.td,{children:"Neuron NodePool provisions Trainium nodes on-demand when workloads are scheduled"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"trn2 for Maverick"})}),(0,s.jsx)(n.td,{children:"trn2.48xlarge (1.5 TiB) supports Maverick (~800 GiB) in BF16 without quantization"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"memory-comparison-gpu-vs-trainium",children:"Memory Comparison: GPU vs Trainium"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"BF16 Memory"}),(0,s.jsx)(n.th,{children:"GPU (FP8 required?)"}),(0,s.jsx)(n.th,{children:"trn1.32xlarge (512 GiB)"}),(0,s.jsx)(n.th,{children:"trn2.48xlarge (1.5 TiB)"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Scout 17B-16E"}),(0,s.jsx)(n.td,{children:"~220 GiB"}),(0,s.jsx)(n.td,{children:"p4d.24xlarge (320 GiB) - No"}),(0,s.jsx)(n.td,{children:"Fits in BF16"}),(0,s.jsx)(n.td,{children:"Fits in BF16"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Maverick 17B-128E"}),(0,s.jsx)(n.td,{children:"~800 GiB"}),(0,s.jsxs)(n.td,{children:["p5.48xlarge (640 GiB) - ",(0,s.jsx)(n.strong,{children:"Yes, FP8"})]}),(0,s.jsx)(n.td,{children:"Does not fit"}),(0,s.jsx)(n.td,{children:"Fits in BF16"})]})]})]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsxs)(n.p,{children:["For Maverick, only ",(0,s.jsx)(n.code,{children:"trn2.48xlarge"})," has sufficient memory (1.5 TiB) for BF16. GPU deployment requires FP8 quantization, and ",(0,s.jsx)(n.code,{children:"trn1.32xlarge"})," (512 GiB) is insufficient."]})}),"\n",(0,s.jsxs)(n.admonition,{type:"warning",children:[(0,s.jsx)(n.p,{children:"Trainium instance availability varies by region:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"aws ec2 describe-instance-type-offerings --region <REGION> \\\n  --location-type availability-zone \\\n  --filters \"Name=instance-type,Values=trn*\" \\\n  --query 'InstanceTypeOfferings[].{Type:InstanceType,Zone:Location}' --output table\n"})}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"trn2.48xlarge"}),": Limited availability (",(0,s.jsx)(n.code,{children:"us-east-2"}),"). ",(0,s.jsx)(n.strong,{children:"Not supported by EKS Auto Mode"})," \u2014 use Karpenter with the inference-ready cluster."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"trn1.32xlarge"}),": Available in more regions (",(0,s.jsx)(n.code,{children:"us-west-2"}),", ",(0,s.jsx)(n.code,{children:"us-east-1"}),", ",(0,s.jsx)(n.code,{children:"us-east-2"}),", etc.)."]}),"\n"]})]}),"\n",(0,s.jsx)(n.h2,{id:"model-compilation",children:"Model Compilation"}),"\n",(0,s.jsxs)(n.p,{children:["The AWS Neuron DLC uses ",(0,s.jsx)(n.strong,{children:"optimum-neuron"})," to run vLLM on Trainium. Models must be pre-compiled for Neuron before serving. The DLC checks the ",(0,s.jsx)(n.a,{href:"https://huggingface.co/aws-neuron/optimum-neuron-cache",children:"optimum-neuron-cache"})," on Hugging Face for pre-compiled model artifacts matching your configuration (model, batch size, sequence length, tensor parallelism, dtype)."]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"optimum-cli export neuron"})," command does ",(0,s.jsx)(n.strong,{children:"not"})," support ",(0,s.jsx)(n.code,{children:"llama4"})," as a model type. However, ",(0,s.jsx)(n.code,{children:"vllm serve"})," uses a separate inference code path (",(0,s.jsx)(n.code,{children:"optimum.neuron.models.inference.llama4"}),") that includes full MoE support via ",(0,s.jsx)(n.code,{children:"Llama4NeuronModelForCausalLM"}),". Compilation is triggered automatically on first serve."]})}),"\n",(0,s.jsx)(n.h2,{id:"software-versions",children:"Software Versions"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Component"}),(0,s.jsx)(n.th,{children:"Version"}),(0,s.jsx)(n.th,{children:"Notes"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Neuron SDK"}),(0,s.jsx)(n.td,{children:"2.26.1"}),(0,s.jsx)(n.td,{children:"Required"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"optimum-neuron"}),(0,s.jsx)(n.td,{children:">= 0.4.0"}),(0,s.jsx)(n.td,{children:"Llama 4 inference support added in v0.4.0"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"vLLM"}),(0,s.jsx)(n.td,{children:"0.11.0"}),(0,s.jsx)(n.td,{children:"With optimum-neuron Neuron platform plugin"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"neuronx-distributed"}),(0,s.jsx)(n.td,{children:"0.15"}),(0,s.jsx)(n.td,{children:"MoE module used by Llama 4 inference"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"DLC Image"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"763104351884.dkr.ecr.<region>.amazonaws.com/huggingface-vllm-inference-neuronx:0.11.0-optimum0.4.5-neuronx-py310-sdk2.26.1-ubuntu22.04"})}),(0,s.jsx)(n.td,{children:"Latest available"})]})]})]}),"\n",(0,s.jsxs)(t.A,{header:(0,s.jsx)(n.h2,{children:(0,s.jsx)(n.span,{children:"Deploying the Inference-Ready EKS Cluster"})}),children:[(0,s.jsxs)(n.p,{children:["This guide assumes you have an existing EKS cluster with Trainium support. We recommend using the ",(0,s.jsx)(n.a,{href:"/docs/infra/inference/inference-ready-cluster",children:"Inference-Ready EKS Cluster"})," which uses Karpenter for node provisioning and includes pre-configured Neuron NodePools."]}),(0,s.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"aws cli"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://helm.sh/docs/intro/install/",children:"Helm 3.0+"})}),"\n"]}),(0,s.jsx)(n.h3,{id:"deploy-the-cluster",children:"Deploy the Cluster"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/awslabs/ai-on-eks.git\ncd ai-on-eks/infra/solutions/inference-ready-cluster\n"})}),(0,s.jsxs)(n.p,{children:["The default ",(0,s.jsx)(n.code,{children:"terraform/blueprint.tfvars"})," uses Karpenter (not EKS Auto Mode). The cluster creates Karpenter NodePools including ",(0,s.jsx)(n.code,{children:"trn1-neuron"})," for Trainium workloads."]}),(0,s.jsxs)(n.p,{children:["To add trn2 support, add ",(0,s.jsx)(n.code,{children:"trn2-neuron"})," to the additional EC2NodeClass names:"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-hcl",children:'region                              = "us-east-2"  # trn2 is available in us-east-2\nkarpenter_additional_ec2nodeclassnames = ["trn2-neuron"]\n'})}),(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"us-east-2"})," has only 3 availability zones. Set ",(0,s.jsx)(n.code,{children:"availability_zones_count = 3"})," in your tfvars."]})}),(0,s.jsx)(n.p,{children:"Then deploy:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"./install.sh\n"})}),(0,s.jsx)(n.h3,{id:"configure-kubectl",children:"Configure kubectl"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"aws eks --region <REGION> update-kubeconfig --name inference-cluster\n"})}),(0,s.jsx)(n.h3,{id:"verify-karpenter-resources",children:"Verify Karpenter Resources"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Verify NodePools\nkubectl get nodepools\n\n# Verify EC2NodeClasses\nkubectl get ec2nodeclasses\n"})}),(0,s.jsx)(n.p,{children:"Expected output:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"NAME            NODECLASS       NODES   READY   AGE\ntrn1-neuron     trn1-neuron     0       True    3m\ntrn2-neuron     trn2-neuron     0       True    3m\ng5-nvidia       g5-nvidia       0       True    3m\n...\n"})}),(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"trn1-neuron"})," and ",(0,s.jsx)(n.code,{children:"trn2-neuron"})," NodePools include the ",(0,s.jsx)(n.code,{children:"aws.amazon.com/neuron"})," taint. Trainium nodes are provisioned automatically when a workload with the matching toleration is scheduled."]}),(0,s.jsx)(n.h3,{id:"install-neuron-device-plugin",children:"Install Neuron Device Plugin"}),(0,s.jsxs)(n.p,{children:["The Neuron device plugin is ",(0,s.jsx)(n.strong,{children:"required"})," for Trainium workloads. Install it using the AWS Neuron Helm chart:"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Create required namespace\nkubectl create namespace neuron-healthcheck-system\n\n# Install Neuron Helm chart (includes device plugin and node-problem-detector)\nhelm install neuron-helm-chart \\\n  oci://public.ecr.aws/neuron/neuron-helm-chart \\\n  --namespace kube-system \\\n  --version 1.3.0\n"})}),(0,s.jsx)(n.p,{children:"Verify the installation:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Check Neuron device plugin DaemonSet (0 desired is expected until Neuron nodes are provisioned)\nkubectl get daemonset neuron-device-plugin -n kube-system\n"})}),(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.p,{children:["The Neuron device plugin DaemonSet will show ",(0,s.jsx)(n.code,{children:"0/0"})," desired/ready pods until a Trainium node is provisioned. This is expected behavior - the DaemonSet pods will automatically start when a Neuron node joins the cluster."]})}),(0,s.jsx)(n.h3,{id:"neuron-resource-names",children:"Neuron Resource Names"}),(0,s.jsx)(n.p,{children:"When a Trainium node is provisioned, the device plugin exposes the following extended resources:"}),(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Resource"}),(0,s.jsx)(n.th,{children:"Description"}),(0,s.jsx)(n.th,{children:"trn1.32xlarge"}),(0,s.jsx)(n.th,{children:"trn2.48xlarge"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"aws.amazon.com/neuron"})}),(0,s.jsx)(n.td,{children:"Neuron devices (chips)"}),(0,s.jsx)(n.td,{children:"16"}),(0,s.jsx)(n.td,{children:"16"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"aws.amazon.com/neuroncore"})}),(0,s.jsx)(n.td,{children:"NeuronCores (2 per v1 chip, 4 per v2 chip)"}),(0,s.jsx)(n.td,{children:"32"}),(0,s.jsx)(n.td,{children:"64"})]})]})]}),(0,s.jsxs)(n.p,{children:["Use ",(0,s.jsx)(n.code,{children:"aws.amazon.com/neuron"})," in pod resource requests to allocate Neuron devices."]})]}),"\n",(0,s.jsx)(n.h2,{id:"deploy-llama-4-scout-on-trainium",children:"Deploy Llama 4 Scout on Trainium"}),"\n",(0,s.jsx)(n.h3,{id:"step-1-create-hugging-face-token-secret",children:"Step 1: Create Hugging Face Token Secret"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl create secret generic hf-token --from-literal=token=<your-huggingface-token>\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-2-deploy-with-helm",children:"Step 2: Deploy with Helm"}),"\n",(0,s.jsxs)(n.p,{children:["For ",(0,s.jsx)(n.strong,{children:"trn2.48xlarge"})," (Scout):"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"helm repo add ai-on-eks https://awslabs.github.io/ai-on-eks-charts/\nhelm repo update\n\nhelm install llama4-scout-neuron ai-on-eks/inference-charts \\\n  --values https://raw.githubusercontent.com/awslabs/ai-on-eks-charts/refs/heads/main/charts/inference-charts/values-llama-4-scout-17b-vllm-neuron.yaml\n"})}),"\n",(0,s.jsxs)(n.admonition,{type:"info",children:[(0,s.jsx)(n.p,{children:"Key deployment parameters:"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tensor_parallel_size: 16"})," (one per Trainium chip, not per NeuronCore)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Docker image"}),": AWS Neuron DLC from private ECR (",(0,s.jsx)(n.code,{children:"763104351884.dkr.ecr.<region>.amazonaws.com/huggingface-vllm-inference-neuronx"}),")"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Neuron device requests"}),": ",(0,s.jsx)(n.code,{children:"aws.amazon.com/neuron: 16"})," for all 16 chips"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"CPU memory"}),": ",(0,s.jsx)(n.code,{children:"384Gi"})," minimum (weight sharding requires loading the full model into CPU memory)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Instance type"}),": ",(0,s.jsx)(n.code,{children:"trn2.48xlarge"})," (default for both Scout and Maverick)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Environment variable"}),": ",(0,s.jsx)(n.code,{children:"VLLM_NEURON_FRAMEWORK=optimum"})," is required for on-the-fly Neuron compilation"]}),"\n"]})]}),"\n",(0,s.jsx)(n.h3,{id:"step-3-monitor-deployment",children:"Step 3: Monitor Deployment"}),"\n",(0,s.jsx)(n.p,{children:"After deploying, Karpenter will automatically provision a Trainium node:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Watch node provisioning\nkubectl get nodeclaims -w\n\n# Check pod status\nkubectl get pods -w\n"})}),"\n",(0,s.jsx)(n.p,{children:"During deployment, the pod will go through these stages:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pending"})," - waiting for Trainium node provisioning (~5 minutes)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ContainerCreating"})," - pulling the Neuron DLC image (~2.9 GiB)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Running"})," - Neuron model compilation (30-60+ minutes on first run)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ready"})," - vLLM server is serving requests"]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"CPU Memory Requirements",type:"warning",children:(0,s.jsxs)(n.p,{children:["The pod requires ",(0,s.jsx)(n.strong,{children:"at least 384 GiB of CPU memory"})," for model weight sharding across 16 Neuron devices. With insufficient memory (e.g., 64 GiB), the pod will be OOMKilled during weight loading. The trn2.48xlarge instance provides ~2 TiB of system memory, so this is well within capacity."]})}),"\n",(0,s.jsxs)(n.admonition,{type:"warning",children:[(0,s.jsx)(n.p,{children:"The first deployment takes significantly longer due to Neuron model compilation. Subsequent deployments with the same configuration will use cached artifacts. Monitor the compilation progress in the logs:"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl logs -f -l app.kubernetes.io/instance=llama4-scout-neuron\n"})})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Tested deployment timeline on trn2.48xlarge (Scout):"})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Phase"}),(0,s.jsx)(n.th,{children:"Duration"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Node provisioning"}),(0,s.jsx)(n.td,{children:"~5 min"}),(0,s.jsx)(n.td,{children:"Karpenter provisions trn2.48xlarge"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Image pull"}),(0,s.jsx)(n.td,{children:"~30 sec"}),(0,s.jsx)(n.td,{children:"DLC image (~2.9 GiB, cached after first pull)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"HLO generation"}),(0,s.jsx)(n.td,{children:"~60 sec"}),(0,s.jsx)(n.td,{children:"Generates HLOs for context_encoding and token_generation"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Neuron compilation"}),(0,s.jsx)(n.td,{children:"~200 sec"}),(0,s.jsx)(n.td,{children:"neuronx-cc compiles HLOs to NEFFs (target=trn2)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Model build"}),(0,s.jsx)(n.td,{children:"~650 sec"}),(0,s.jsx)(n.td,{children:"Weight layout transformation"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Weight loading"}),(0,s.jsx)(n.td,{children:"~5 min"}),(0,s.jsx)(n.td,{children:"Download, shard, and load weights to 16 Neuron devices"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Total (first deploy)"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"~20 min"})}),(0,s.jsx)(n.td,{children:"Subsequent deploys reuse cached compilation artifacts"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"Once complete, the vLLM server will start:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"INFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000\n"})}),"\n",(0,s.jsx)(n.h2,{id:"deploy-llama-4-maverick-on-trainium2",children:"Deploy Llama 4 Maverick on Trainium2"}),"\n",(0,s.jsxs)(n.p,{children:["Maverick requires ",(0,s.jsx)(n.code,{children:"trn2.48xlarge"})," (1.5 TiB HBM) and runs in native BF16 without quantization. Ensure your cluster has the ",(0,s.jsx)(n.code,{children:"trn2-neuron"})," Karpenter NodePool configured (see cluster setup above)."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"helm install llama4-maverick-neuron ai-on-eks/inference-charts \\\n  --values https://raw.githubusercontent.com/awslabs/ai-on-eks-charts/refs/heads/main/charts/inference-charts/values-llama-4-maverick-17b-vllm-neuron.yaml\n"})}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"trn2.48xlarge"})," is available in limited regions (",(0,s.jsx)(n.code,{children:"us-east-2"}),"). Verify availability before deploying."]}),"\n",(0,s.jsxs)(n.li,{children:["Ensure your AWS account has sufficient ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html",children:"service quota"})," for Trainium instances (Maverick requires 192 vCPUs)."]}),"\n"]})}),"\n",(0,s.jsx)(n.h2,{id:"test-the-model",children:"Test the Model"}),"\n",(0,s.jsx)(n.h3,{id:"port-forward",children:"Port Forward"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward svc/llama4-scout-neuron 8000:8000\n"})}),"\n",(0,s.jsx)(n.h3,{id:"chat-completion-request",children:"Chat Completion Request"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",\n    "messages": [\n      {"role": "user", "content": "Explain the benefits of Mixture of Experts architecture in large language models."}\n    ],\n    "max_tokens": 512,\n    "temperature": 0.7\n  }\'\n'})}),"\n",(0,s.jsx)(n.h3,{id:"list-available-models",children:"List Available Models"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"curl http://localhost:8000/v1/models | python3 -m json.tool\n"})}),"\n",(0,s.jsx)(n.h3,{id:"multimodal-request-text--image",children:"Multimodal Request (Text + Image)"}),"\n",(0,s.jsx)(n.p,{children:"Llama 4 supports multimodal inference. Send image URLs alongside text:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",\n    "messages": [\n      {\n        "role": "user",\n        "content": [\n          {"type": "text", "text": "Describe what you see in this image."},\n          {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg"}}\n        ]\n      }\n    ],\n    "max_tokens": 256\n  }\'\n'})}),"\n",(0,s.jsx)(n.h2,{id:"deploy-open-webui",children:"Deploy Open WebUI"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://github.com/open-webui/open-webui",children:"Open WebUI"})," provides a ChatGPT-style interface for interacting with the model."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"helm repo add open-webui https://helm.openwebui.com/\nhelm repo update\n\nhelm install open-webui open-webui/open-webui \\\n  --namespace open-webui --create-namespace \\\n  --set ollama.enabled=false \\\n  --set env.OPENAI_API_BASE_URL=http://llama4-scout-neuron.default.svc.cluster.local:8000/v1 \\\n  --set env.OPENAI_API_KEY=dummy\n"})}),"\n",(0,s.jsx)(n.p,{children:"Access the UI:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward svc/open-webui 8080:80 -n open-webui\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Open ",(0,s.jsx)(n.a,{href:"http://localhost:8080",children:"http://localhost:8080"})," in your browser and register a new account. The model will appear in the model selector."]}),"\n",(0,s.jsx)(n.h2,{id:"monitoring",children:"Monitoring"}),"\n",(0,s.jsx)(n.h3,{id:"check-inference-logs",children:"Check Inference Logs"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# View vLLM Neuron logs\nkubectl logs -l app.kubernetes.io/instance=llama4-scout-neuron --tail=100\n\n# Monitor token generation throughput\nkubectl logs -l app.kubernetes.io/instance=llama4-scout-neuron -f | grep "tokens/s"\n'})}),"\n",(0,s.jsx)(n.h3,{id:"observability-dashboard",children:"Observability Dashboard"}),"\n",(0,s.jsx)(n.p,{children:"If the observability stack is enabled on your cluster, access Grafana:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward svc/kube-prometheus-stack-grafana 3000:80 -n monitoring\n"})}),"\n",(0,s.jsx)(n.h2,{id:"cleanup",children:"Cleanup"}),"\n",(0,s.jsx)(n.p,{children:"Remove the model deployment:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Remove Scout\nhelm uninstall llama4-scout-neuron\n\n# Remove Maverick (if deployed)\nhelm uninstall llama4-maverick-neuron\n"})}),"\n",(0,s.jsx)(n.p,{children:"To destroy the entire cluster infrastructure:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd ai-on-eks/infra/solutions/inference-ready-cluster\n./cleanup.sh\n"})})]})}function u(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},42450(e,n,i){i.d(n,{A:()=>p});var r=i(96540),s=i(5556),l=i.n(s),t=i(34164);const o="collapsibleContent_q3kw",a="header_QCEw",d="icon_PckA",c="content_qLC1",h="expanded_iGsi";var u=i(74848);function m({children:e,header:n}){const[i,s]=(0,r.useState)(!1);return(0,u.jsxs)("div",{className:o,children:[(0,u.jsxs)("div",{className:(0,t.A)(a,{[h]:i}),onClick:()=>{s(!i)},children:[n,(0,u.jsx)("span",{className:(0,t.A)(d,{[h]:i}),children:i?"\ud83d\udc47":"\ud83d\udc48"})]}),i&&(0,u.jsx)("div",{className:c,children:e})]})}m.propTypes={children:l().node.isRequired,header:l().node.isRequired};const p=m},28453(e,n,i){i.d(n,{R:()=>t,x:()=>o});var r=i(96540);const s={},l=r.createContext(s);function t(e){const n=r.useContext(l);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(l.Provider,{value:n},e.children)}}}]);
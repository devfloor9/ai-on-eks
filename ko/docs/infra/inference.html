<!doctype html>
<html lang="ko-KR" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-infra/inference/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.0">
<title data-rh="true">Amazon EKS에서의 모델 추론 라이프사이클 | AI on EKS</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://awslabs.github.io/ai-on-eks/ko/docs/infra/inference"><meta data-rh="true" property="og:locale" content="ko_KR"><meta data-rh="true" property="og:locale:alternate" content="en_US"><meta data-rh="true" name="docusaurus_locale" content="ko"><meta data-rh="true" name="docsearch:language" content="ko"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Amazon EKS에서의 모델 추론 라이프사이클 | AI on EKS"><meta data-rh="true" name="description" content="이 가이드는 인프라 설정부터 프로덕션 최적화까지, Amazon EKS에서 대규모 언어 모델(LLM) 추론을 배포하고 최적화하는 전체 라이프사이클을 안내합니다."><meta data-rh="true" property="og:description" content="이 가이드는 인프라 설정부터 프로덕션 최적화까지, Amazon EKS에서 대규모 언어 모델(LLM) 추론을 배포하고 최적화하는 전체 라이프사이클을 안내합니다."><link data-rh="true" rel="icon" href="/ai-on-eks/ko/img/header-icon.png"><link data-rh="true" rel="canonical" href="https://awslabs.github.io/ai-on-eks/ko/docs/infra/inference"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/docs/infra/inference" hreflang="en-US"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/ko/docs/infra/inference" hreflang="ko-KR"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/docs/infra/inference" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Inference","item":"https://awslabs.github.io/ai-on-eks/ko/docs/infra/inference/"}]}</script><link rel="stylesheet" href="/ai-on-eks/ko/assets/css/styles.c270b852.css">
<script src="/ai-on-eks/ko/assets/js/runtime~main.84f685ef.js" defer="defer"></script>
<script src="/ai-on-eks/ko/assets/js/main.d98ac8f0.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><div role="region" aria-label="본문으로 건너뛰기"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">본문으로 건너뛰기</a></div><div class="theme-announcement-bar announcementBar_mb4j" style="background-color:#667eea;color:#ffffff" role="banner"><div class="announcementBarPlaceholder_vyr4"></div><div class="content_knG7 announcementBarContent_xLdY">GenAI on EKS workshop series! <a target="_blank" rel="noopener noreferrer" href="https://aws-experience.com/emea/smb/events/series/get-hands-on-with-amazon-eks?trk=9be4af2e-2339-40ae-b5e9-57b6a7704c36&sc_channel=el" style="color: #ffffff; text-decoration: underline; font-weight: bold; margin-left: 10px;">Register now →</a></div><button type="button" aria-label="닫기" class="clean-btn close closeButton_CVFx announcementBarClose_gvF7"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav aria-label="메인" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="사이드바 펼치거나 접기" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai-on-eks/ko/"><div class="navbar__logo"><img src="/ai-on-eks/ko/img/header-icon.png" alt="AIoEKS 로고" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai-on-eks/ko/img/header-icon.png" alt="AIoEKS 로고" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/ai-on-eks/ko/docs/infra">인프라</a><a class="navbar__item navbar__link" href="/ai-on-eks/ko/docs/blueprints">블루프린트</a><a class="navbar__item navbar__link" href="/ai-on-eks/ko/docs/guidance">가이드</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/awslabs/ai-on-eks" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>한국어</a><ul class="dropdown__menu"><li><a href="/ai-on-eks/docs/infra/inference" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en-US">English</a></li><li><a href="/ai-on-eks/ko/docs/infra/inference" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="ko-KR">한국어</a></li></ul></div><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="어두운 모드와 밝은 모드 전환하기 (현재 system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="맨 위로 스크롤하기" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="문서 사이드바" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai-on-eks/ko/docs/infra"><span title="소개" class="linkLabel_WmDU"> 소개</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai-on-eks/ko/docs/category/training"><span title="Training" class="categoryLinkLabel_W154">Training</span></a><button aria-label="사이드바 분류 &#x27;Training&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible menu__list-item-collapsible--active"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" aria-current="page" href="/ai-on-eks/ko/docs/infra/inference"><span title="Inference" class="categoryLinkLabel_W154">Inference</span></a><button aria-label="사이드바 분류 &#x27;Inference&#x27; 접기" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/infra/inference/inference-ready-cluster"><span title="추론 준비 클러스터" class="linkLabel_WmDU">추론 준비 클러스터</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/infra/inference/aibrix"><span title="EKS 기반 AIBrix" class="linkLabel_WmDU">EKS 기반 AIBrix</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai-on-eks/ko/docs/category/miscellaneous"><span title="Miscellaneous" class="categoryLinkLabel_W154">Miscellaneous</span></a><button aria-label="사 이드바 분류 &#x27;Miscellaneous&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai-on-eks/ko/docs/infra/agents-on-eks"><span title="EKS 기반 에이전트" class="linkLabel_WmDU">EKS 기반 에이전트</span></a></li></ul></nav><button type="button" title="사이드바 숨기기" aria-label="사이드바 숨기기" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="탐색 경로"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="홈" class="breadcrumbs__link" href="/ai-on-eks/ko/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Inference</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">이 페이지에서</button></div><div class="theme-doc-markdown markdown"><header><h1>Amazon EKS에서의 모델 추론 라이프사이클</h1></header>
<p>이 가이드는 인프라 설정부터 프로덕션 최적화까지, Amazon EKS에서 대규모 언어 모델(LLM) 추론을 배포하고 최적화하는 전체 라이프사이클을 안내합니다.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="라이프사이클-개요">라이프사이클 개요<a href="#라이프사이클-개요" class="hash-link" aria-label="라이프사이클 개요에 대한 직접 링크" title="라이프사이클 개요에 대한 직접 링크" translate="no">​</a></h2>
<p>모델 추론 라이프사이클은 세 가지 핵심 단계로 구성됩니다:</p>
<!-- -->
<p>각 단계는 특정 과제를 해결하고 대규모 LLM 추론 성공에 필요한 구성 요소를 제공합니다.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1단계-인프라-설정"><a href="/ai-on-eks/ko/docs/infra/inference/inference-ready-cluster">1단계: 인프라 설정</a><a href="#1단계-인프라-설정" class="hash-link" aria-label="1단계-인프라-설정에 대한 직접 링크" title="1단계-인프라-설정에 대한 직접 링크" translate="no">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="인프라가-중요한-이유">인프라가 중요한 이유<a href="#인프라가-중요한-이유" class="hash-link" aria-label="인프라가 중요한 이유에 대한 직접 링크" title="인프라가 중요한 이유에 대한 직접 링크" translate="no">​</a></h3>
<p>LLM을 배포하기 전에 AI/ML 워크로드의 고유한 요구사항을 처리할 수 있는 견고하고 확장 가능한 인프라가 필요합니다:</p>
<ul>
<li><strong>GPU/Neuron 리소스 관리</strong>: LLM에는 적절한 디바이스 플러그인과 드라이버가 있는 특수 가속기(NVIDIA GPU 또는 AWS Inferentia/Trainium 칩)가 필요합니다</li>
<li><strong>오토스케일링 기능</strong>: 비용을 최적화하면서 다양한 추론 수요를 처리하기 위한 동적 워크로드 스케일링</li>
<li><strong>관측성 기반</strong>: GPU/Neuron 활용률, 모델 성능 및 시스템 상태에 대한 모니터링 및 메트릭 수집</li>
<li><strong>분산 컴퓨팅 지원</strong>: 모델이 단일 노드 용량을 초과하는 경우를 위한 멀티 노드 추론 인프라</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="솔루션-추론-준비-클러스터">솔루션: 추론 준비 클러스터<a href="#솔루션-추론-준비-클러스터" class="hash-link" aria-label="솔루션: 추론 준비 클러스터에 대한 직접 링크" title="솔루션: 추론 준비 클러스터에 대한 직접 링크" translate="no">​</a></h3>
<p><a href="/ai-on-eks/ko/docs/infra/inference/inference-ready-cluster"><strong>추론 준비 EKS 클러스터</strong></a>는 AI/ML 추론 워크로드를 위해 특별히 설계된 사전 구성된 인프라를 제공합니다.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="제공-내용">제공 내용<a href="#제공-내용" class="hash-link" aria-label="제공 내용에 대한 직접 링크" title="제공 내용에 대한 직접 링크" translate="no">​</a></h4>
<ul>
<li>
<p><strong>사전 설치된 구성 요소</strong>:</p>
<ul>
<li>분산 Ray 워크로드를 위한 KubeRay Operator</li>
<li>멀티 노드 분산 추론을 위한 LeaderWorkerSet</li>
<li>GPU 관리를 위한 NVIDIA Device Plugin</li>
<li>Inferentia/Trainium 지원을 위한 AWS Neuron Device Plugin</li>
<li>지능형 노드 오토스케일링을 위한 Karpenter</li>
</ul>
</li>
<li>
<p><strong>내장 관측성</strong>:</p>
<ul>
<li>메트릭 수집을 위한 Prometheus</li>
<li>AI/ML 전용 대시보드가 있는 Grafana</li>
<li>GPU 메트릭을 위한 DCGM Exporter</li>
<li>시스템 수준 메트릭을 위한 Node Exporter</li>
</ul>
</li>
<li>
<p><strong>AIBrix 통합</strong>:</p>
<ul>
<li>고급 추론 최적화 기능</li>
<li>트래픽 관리를 위한 게이트웨이 및 라우팅</li>
<li>성능 모니터링 도구</li>
</ul>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="주요-고려사항">주요 고려사항<a href="#주요-고려사항" class="hash-link" aria-label="주요 고려사항에 대한 직접 링크" title="주요 고려사항에 대한 직접 링크" translate="no">​</a></h4>
<p>인프라를 설정할 때 다음을 고려하세요:</p>
<ol>
<li><strong>하드웨어 선택</strong>: 모델 요구사항과 비용 제약에 따라 NVIDIA GPU(P4d, P5, G5)와 AWS Neuron(Inf2, Trn1) 중 선택</li>
<li><strong>스케일링 전략</strong>: 모델 크기에 따라 단일 노드 또는 멀티 노드 추론 필요 여부 결정</li>
<li><strong>네트워크 구성</strong>: 모델 로딩 및 추론 트래픽을 위한 충분한 대역폭으로 적절한 VPC 설정 보장</li>
<li><strong>스토리지 요구사항</strong>: 모델 크기 및 액세스 패턴에 따라 모델 아티팩트 스토리지(S3, EFS 또는 FSx) 계획</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="시작하기">시작하기<a href="#시작하기" class="hash-link" aria-label="시작하기에 대한 직접 링크" title="시작하기에 대한 직접 링크" translate="no">​</a></h4>
<p>인프라를 프로비저닝하려면 <a href="/ai-on-eks/ko/docs/infra/inference/inference-ready-cluster">추론 준비 클러스터 배포 가이드</a>를 따르세요.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2단계-모델-배포"><a href="/ai-on-eks/ko/docs/blueprints/inference/inference-charts">2단계: 모델 배포</a><a href="#2단계-모델-배포" class="hash-link" aria-label="2단계-모델-배포에 대한 직접 링크" title="2단계-모델-배포에 대한 직접 링크" translate="no">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="배포-패턴이-중요한-이유">배포 패턴이 중요한 이유<a href="#배포-패턴이-중요한-이유" class="hash-link" aria-label="배포 패턴이 중요한 이유에 대한 직접 링크" title="배포 패턴이 중요한 이유에 대한 직접 링크" translate="no">​</a></h3>
<p>인프라가 준비되면 LLM 배포에는 다음에 대한 신중한 고려가 필요합니다:</p>
<ul>
<li><strong>프레임워크 선택</strong>: 다양한 프레임워크(vLLM, Ray-vLLM, Triton)는 성능, 기능 및 복잡성에서 서로 다른 트레이드오프를 제공</li>
<li><strong>모델 호환성</strong>: 모든 모델이 모든 프레임워크에서 작동하는 것은 아님; 일부는 특정 최적화 필요</li>
<li><strong>리소스 할당</strong>: 적절한 GPU/Neuron 메모리 할당 및 요청/제한 구성</li>
<li><strong>스케일링 동작</strong>: 오토스케일링이 있는 단일 레플리카 vs 다중 레플리카 배포</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="솔루션-추론-차트">솔루션: 추론 차트<a href="#솔루션-추론-차트" class="hash-link" aria-label="솔루션: 추론 차트에 대한 직접 링크" title="솔루션: 추론 차트에 대한 직접 링크" translate="no">​</a></h3>
<p><a href="/ai-on-eks/ko/docs/blueprints/inference/inference-charts"><strong>AI on EKS 추론 차트</strong></a>는 인기 모델을 위한 사전 구성된 값이 포함된 Helm 기반 배포를 제공하며, 여러 배포 프레임워크를 지원합니다.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="지원-프레임워크">지원 프레임워크<a href="#지원-프레임워크" class="hash-link" aria-label="지원 프레임워크에 대한 직접 링크" title="지원 프레임워크에 대한 직접 링크" translate="no">​</a></h4>
<ul>
<li><strong>vLLM</strong>: 최적화된 CUDA 커널로 빠른 단일 노드 추론</li>
<li><strong>Ray-vLLM</strong>: 오토스케일링 및 로드 밸런싱이 있는 분산 추론</li>
<li><strong>Triton-vLLM</strong>: 고급 기능이 있는 프로덕션 레디 추론 서버</li>
<li><strong>LeaderWorkerSet-vLLM</strong>: 단일 노드에 맞지 않는 모델을 위한 멀티 노드 추론</li>
<li><strong>Diffusers</strong>: 이미지 생성 모델을 위한 Hugging Face Diffusers</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="예제-llama-32-1b-배포">예제: Llama 3.2 1B 배포<a href="#예제-llama-32-1b-배포" class="hash-link" aria-label="예제: Llama 3.2 1B 배포에 대한 직접 링크" title="예제: Llama 3.2 1B 배포에 대한 직접 링크" translate="no">​</a></h4>
<p>추론 차트를 사용하여 Llama 모델을 배포하는 과정을 살펴보겠습니다:</p>
<p><strong>1단계: Hugging Face 토큰 시크릿 생성</strong></p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl create secret generic hf-token </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --from-literal</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">token</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">your_huggingface_token</span><br></span></code></pre></div></div>
<p><strong>2단계: 모델 배포</strong></p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># GPU에서 vLLM으로 Llama 3.2 1B 배포</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">helm </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> llama-inference ./blueprints/inference/inference-charts </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">--values</span><span class="token plain"> ./blueprints/inference/inference-charts/values-llama-32-1b-vllm.yaml</span><br></span></code></pre></div></div>
<p><strong>3단계: 배포 확인</strong></p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># 파드 상태 확인</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get pods </span><span class="token parameter variable" style="color:#36acaa">-l</span><span class="token plain"> </span><span class="token assign-left variable" style="color:#36acaa">app</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">llama-inference</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 서비스 엔드포인트 확인</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get svc llama-inference</span><br></span></code></pre></div></div>
<p><strong>4단계: 추론 테스트</strong></p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># 서비스에 포트 포워딩</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward svc/llama-inference </span><span class="token number" style="color:#36acaa">8000</span><span class="token plain">:8000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 테스트 요청 전송</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token function" style="color:#d73a49">curl</span><span class="token plain"> http://localhost:8000/v1/completions </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">-H</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Content-Type: application/json&quot;</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">-d</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;{</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;model&quot;: &quot;meta-llama/Llama-3.2-1B&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;prompt&quot;: &quot;양자 컴퓨팅을 간단하게 설명해주세요:&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;max_tokens&quot;: 100</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">  }&#x27;</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="사용-가능한-사전-구성-모델">사용 가능한 사전 구성 모델<a href="#사용-가능한-사전-구성-모델" class="hash-link" aria-label="사용 가능한 사전 구성 모델에 대한 직접 링크" title="사용 가능한 사전 구성 모델에 대한 직접 링크" translate="no">​</a></h4>
<p>추론 차트에는 다음 모델을 위한 즉시 배포 가능한 구성이 포함되어 있습니다:</p>
<p><strong>언어 모델</strong>:</p>
<ul>
<li>DeepSeek R1 Distill Llama 8B</li>
<li>Llama 3.2 1B, Llama 4 Scout 17B</li>
<li>Mistral Small 24B</li>
<li>GPT OSS 20B</li>
<li>Qwen3 1.7B</li>
</ul>
<p><strong>Diffusion 모델</strong>:</p>
<ul>
<li>FLUX.1 Schnell</li>
<li>Stable Diffusion XL, Stable Diffusion 3.5</li>
<li>Kolors, OmniGen</li>
</ul>
<p><strong>Neuron 최적화</strong>:</p>
<ul>
<li>AWS Inferentia에서의 Llama 2 13B</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="주요-고려사항-1">주요 고려사항<a href="#주요-고려사항-1" class="hash-link" aria-label="주요 고려사항에 대한 직접 링크" title="주요 고려사항에 대한 직접 링크" translate="no">​</a></h4>
<p>모델을 배포할 때 다음을 고려하세요:</p>
<ol>
<li><strong>모델 크기 vs 하드웨어</strong>: 선택한 인스턴스 유형이 모델에 충분한 GPU/Neuron 메모리를 가지고 있는지 확인</li>
<li><strong>배치 크기 구성</strong>: 최적의 처리량 vs 지연 시간 트레이드오프를 위해 배치 크기 튜닝</li>
<li><strong>양자화 옵션</strong>: 메모리 사용량을 줄이기 위해 양자화된 모델(INT8, INT4) 사용 고려</li>
<li><strong>레플리카 수</strong>: 단일 레플리카로 시작하고 관찰된 부하에 따라 확장</li>
<li><strong>헬스 체크</strong>: 적절한 liveness 및 readiness 프로브 구성</li>
</ol>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="시작하기-1">시작하기<a href="#시작하기-1" class="hash-link" aria-label="시작하기에 대한 직접 링크" title="시작하기에 대한 직접 링크" translate="no">​</a></h4>
<p>상세한 구성 옵션과 고급 배포 시나리오는 <a href="/ai-on-eks/ko/docs/blueprints/inference/inference-charts">추론 차트 문서</a>를 참조하세요.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3단계-최적화"><a href="/ai-on-eks/ko/docs/guidance">3단계: 최적화</a><a href="#3단계-최적화" class="hash-link" aria-label="3단계-최적화에 대한 직접 링크" title="3단계-최적화에 대한 직접 링크" translate="no">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="최적화가-중요한-이유">최적화가 중요한 이유<a href="#최적화가-중요한-이유" class="hash-link" aria-label="최적화가 중요한 이유에 대한 직접 링크" title="최적화가 중요한 이유에 대한 직접 링크" translate="no">​</a></h3>
<p>모델을 배포한 후 최적화는 다음을 보장합니다:</p>
<ul>
<li><strong>비용 효율성</strong>: 성능 요구  사항을 충족하면서 인프라 비용 최소화</li>
<li><strong>성능 튜닝</strong>: 목표 지연 시간 및 처리량 SLO 달성</li>
<li><strong>리소스 활용</strong>: GPU/Neuron 활용률을 극대화하여 고가 하드웨어에서 최대 가치 확보</li>
<li><strong>운영 우수성</strong>: 모니터링, 알림 및 문제 해결을 위한 모범 사례 구현</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="최적화-기법">최적화 기법<a href="#최적화-기법" class="hash-link" aria-label="최적화 기법에 대한 직접 링크" title="최적화 기법에 대한 직접 링크" translate="no">​</a></h3>
<p><a href="/ai-on-eks/ko/docs/guidance"><strong>가이던스 섹션</strong></a>은 프로덕션 AI/ML 워크로드를 위한 포괄적인 모범 사례와 최적화 기법을 제공합니다. 각 기법은 특정 성능 또는 비용 과제를 해결합니다:</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="컨테이너-시작-시간-최적화"><a href="/ai-on-eks/ko/docs/guidance/container-startup-time">컨테이너 시작 시간 최적화</a><a href="#컨테이너-시작-시간-최적화" class="hash-link" aria-label="컨테이너-시작-시간-최적화에 대한 직접 링크" title="컨테이너-시작-시간-최적화에 대한 직접 링크" translate="no">​</a></h4>
<p>모델 로딩 시간을 수 분에서 수 초로 줄여 오토스케일링 응답성과 개발 반복 속도를 개선합니다.</p>
<p><strong>주요 기법</strong>:</p>
<ul>
<li><strong><a href="/ai-on-eks/ko/docs/guidance/container-startup-time/reduce-container-image-size">이미지 크기 줄이기</a></strong>: 풀 시간을 최소화하기 위해 컨테이너 이미지 최적화</li>
<li><strong><a href="/ai-on-eks/ko/docs/guidance/container-startup-time/reduce-container-image-size/decoupling-model-artifacts">모델 아티팩트 분리</a></strong>: 컨테이너 이미지에서 모델 가중치 분리</li>
<li><strong><a href="/ai-on-eks/ko/docs/guidance/container-startup-time/accelerate-pull-process">풀 프로세스 가속화</a></strong>: containerd 스냅샷터 및 이미지 프리패칭 사용</li>
<li><strong><a href="/ai-on-eks/ko/docs/guidance/container-startup-time/accelerate-pull-process/prefecthing-images-on-br">노드에 프리패치</a></strong>: 워크로드 스케줄링 전에 모델 이미지로 노드 예열</li>
</ul>
<p><strong>효과</strong>: 시작 시간을 60-80% 단축하여 더 빠른 오토스케일링과 낮은 비용을 가능하게 합니다.</p>
<p><strong>사용 시기</strong>: 오토스케일링 워크로드, 개발 환경 및 비용에 민감한 배포에 중요합니다.</p>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="동적-리소스-할당-dra"><a href="/ai-on-eks/ko/docs/guidance/dynamic-resource-allocation">동적 리소스 할당 (DRA)</a><a href="#동적-리소스-할당-dra" class="hash-link" aria-label="동적-리소스-할당-dra에 대한 직접 링크" title="동적-리소스-할당-dra에 대한 직접 링크" translate="no">​</a></h4>
<p>세밀한 리소스 제어와 향상된 활용률을 위한 차세대 GPU 스케줄링입니다.</p>
<p><strong>주요 기능</strong>:</p>
<ul>
<li><strong>세밀한 GPU 제어</strong>: 전체 디바이스 대신 특정 GPU 메모리 양 요청</li>
<li><strong>워크로드별 공유</strong>: 파드별로 MPS, 타임 슬라이싱, MIG 또는 전용 모드 선택</li>
<li><strong>토폴로지 인식 스케줄링</strong>: NVLink 및 GPU 인터커넥트 최적화</li>
<li><strong>P6e 필수</strong>: Amazon EC2 P6e-GB200 UltraServer에 필수</li>
</ul>
<p><strong>효과</strong>: GPU 활용률을 30-40%에서 70-90%로 증가시켜 인프라 비용을 50% 이상 절감합니다.</p>
<p><strong>사용 시기</strong>:</p>
<ul>
<li>단일 GPU에서 여러 소규모 모델 실행</li>
<li>정밀한 GPU 메모리 할당 필요</li>
<li>최신 GPU 인스턴스(P6e) 사용</li>
<li>워크로드 전체에서 GPU 활용률 최적화</li>
</ul>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="관측성-및-모니터링"><a href="/ai-on-eks/ko/docs/guidance/observability">관측성 및 모니터링</a><a href="#관측성-및-모니터링" class="hash-link" aria-label="관측성-및-모니터링에 대한 직접 링크" title="관측성-및-모니터링에 대한 직접 링크" translate="no">​</a></h4>
<p>성능 최적화 및 문제 해결을 위한 추론 워크로드의 종합적인 가시성입니다.</p>
<p><strong>포함 내용</strong>:</p>
<ul>
<li><strong>GPU/Neuron 메트릭</strong>: 활용률, 메모리 사용량 및 온도 추적</li>
<li><strong>모델 성능</strong>: 지연 시간, 처리량 및 오류율 모니터링</li>
<li><strong>시스템 상태</strong>: CPU, 메모리, 네트워크 및 스토리지 메트릭</li>
<li><strong>커스텀 대시보드</strong>: 추론 워크로드를 위한 사전 구축된 Grafana 대시보드</li>
</ul>
<p><strong>주요 모니터링 메트릭</strong>:</p>
<ul>
<li>GPU 활용률 (비용 효율성을 위한 목표: &gt;70%)</li>
<li>추론 지연 시간 (P50, P95, P99)</li>
<li>초당 요청 수 (처리량)</li>
<li>큐 깊이 (오토스케일링 결정용)</li>
<li>모델 로딩 시간</li>
</ul>
<p><strong>효과</strong>: 데이터 기반 최적화 결정과 사전 문제 감지를 가능하게 합니다.</p>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="네트워킹-최적화"><a href="/ai-on-eks/ko/docs/guidance/networking">네트워킹 최적화</a><a href="#네트워킹-최적화" class="hash-link" aria-label="네트워킹-최적화에 대한 직접 링크" title="네트워킹-최적화에 대한 직접 링크" translate="no">​</a></h4>
<p>고처리량 추론 워크로드를 위한 네트워크 구성을 최적화합니다.</p>
<p><strong>주요 영역</strong>:</p>
<ul>
<li><strong>VPC 설계</strong>: 적절한 서브넷 크기 조정 및 가용 영역 분배</li>
<li><strong>로드 밸런싱</strong>: 추론 엔드포인트를 위한 ALB/NLB 구성</li>
<li><strong>서비스 메시</strong>: 고급 트래픽 관리를 위한 Istio/Linkerd</li>
<li><strong>네트워크 정책</strong>: 워크로드 간 보안 및 격리</li>
</ul>
<p><strong>효과</strong>: 네트워크 지연 시간을 20-40% 줄이고 안정성을 개선합니다.</p>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="eks-모범-사례"><a href="/ai-on-eks/ko/docs/guidance/eks-best-practices">EKS 모범 사례</a><a href="#eks-모범-사례" class="hash-link" aria-label="eks-모범-사례에 대한 직접 링크" title="eks-모범-사례에 대한 직접 링크" translate="no">​</a></h4>
<p>보안, 안정성, 성능 및 비용 최적화를 위한 종합 가이드입니다.</p>
<p><strong>다루는 주제</strong>:</p>
<ul>
<li>보안 및 규정 준수</li>
<li>안정성 및 가용성</li>
<li>성능 최적화</li>
<li>비용 최적화</li>
</ul>
<hr>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="최적화-워크플로우">최적화 워크플로우<a href="#최적화-워크플로우" class="hash-link" aria-label="최적화 워크플로우에 대한 직접 링크" title="최적화 워크플로우에 대한 직접 링크" translate="no">​</a></h3>
<p>추론 배포를 최적화하기 위해 다음 워크플로우를 따르세요:</p>
<!-- -->
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="시작하기-2">시작하기<a href="#시작하기-2" class="hash-link" aria-label="시작하기에 대한 직접 링크" title="시작하기에 대한 직접 링크" translate="no">​</a></h4>
<p>각 최적화 기법에 대해 자세히 알아보려면 <a href="/ai-on-eks/ko/docs/guidance">가이던스 문서</a>를 참조하세요.</p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="전체-라이프사이클-예제">전체 라이프사이클 예제<a href="#전체-라이프사이클-예제" class="hash-link" aria-label="전체 라이프사이클 예제에 대한 직접 링크" title="전체 라이프사이클 예제에 대한 직접 링크" translate="no">​</a></h2>
<p>인프라부터 최적화된 프로덕션까지 Llama 3.2 1B를 배포하는 전체 예제입니다:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1단계-인프라-배포">1단계: 인프라 배포<a href="#1단계-인프라-배포" class="hash-link" aria-label="1단계: 인프라 배포에 대한 직접 링크" title="1단계: 인프라 배포에 대한 직접 링크" translate="no">​</a></h3>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># 리포지토리 클론</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token function" style="color:#d73a49">git</span><span class="token plain"> clone https://github.com/awslabs/ai-on-eks.git</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token builtin class-name">cd</span><span class="token plain"> ai-on-eks/infra/solutions/inference-ready-cluster</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 배포 구성</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token function" style="color:#d73a49">cp</span><span class="token plain"> blueprint.tfvars.example blueprint.tfvars</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># blueprint.tfvars를 설정에 맞게 편집</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 인프라 배포</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">terraform init</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">terraform apply -var-file</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">blueprint.tfvars</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2단계-모델-배포-1">2단계: 모델 배포<a href="#2단계-모델-배포-1" class="hash-link" aria-label="2단계: 모델 배포에 대한 직접 링크" title="2단계: 모델 배포에 대한 직접 링크" translate="no">​</a></h3>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># kubeconfig 업데이트</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">aws eks update-kubeconfig </span><span class="token parameter variable" style="color:#36acaa">--name</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">cluster-name</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">--region</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">region</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Hugging Face 토큰 시크릿 생성</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl create secret generic hf-token </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --from-literal</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">token</span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">your-token</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Llama 3.2 1B 배포</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">helm repo </span><span class="token function" style="color:#d73a49">add</span><span class="token plain"> ai-on-eks https://awslabs.github.io/ai-on-eks-charts/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">helm repo update</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">helm </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> qwen3-1-7b ai-on-eks/inference-charts </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> https://raw.githubusercontent.com/awslabs/ai-on-eks-charts/refs/heads/main/charts/inference-charts/values-llama-32-1b-vllm.yaml</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="3단계-최적화-지속적">3단계: 최적화 (지속적)<a href="#3단계-최적화-지속적" class="hash-link" aria-label="3단계: 최적화 (지속적)에 대한 직접 링크" title="3단계: 최적화 (지속적)에 대한 직접 링크" translate="no">​</a></h3>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># 모니터링을 위해 Grafana 접근</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> monitoring svc/kube-prometheus-stack-grafana </span><span class="token number" style="color:#36acaa">3000</span><span class="token plain">:80</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># GPU 활용률 모니터링 및 레플리카 조정</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl scale deployment llama-inference </span><span class="token parameter variable" style="color:#36acaa">--replicas</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">3</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 큐 깊이 기반 오토스케일링 활성화</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl autoscale deployment llama-inference </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">--min</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">2</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">--max</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">10</span><span class="token plain"> --cpu-percent</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">70</span><br></span></code></pre></div></div>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="다음-단계">다음 단계<a href="#다음-단계" class="hash-link" aria-label="다음 단계에 대한 직접 링크" title="다음 단계에 대한 직접 링크" translate="no">​</a></h2>
<p>모델 추론 라이프사이클 전체를 이해했으니, 출발점을 선택하세요:</p>
<ul>
<li><strong>EKS가 처음이신가요?</strong> <a href="/ai-on-eks/ko/docs/infra/inference/inference-ready-cluster">인프라 설정</a>부터 시작하세요</li>
<li><strong>인프라가 있으신가요?</strong> <a href="/ai-on-eks/ko/docs/blueprints/inference/inference-charts">모델 배포</a>로 바로 이동하세요</li>
<li><strong>모델이 실행 중인가요?</strong> <a href="/ai-on-eks/ko/docs/guidance">가이던스</a>로 최적화하세요</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="추가-리소스">추가 리소스<a href="#추가-리소스" class="hash-link" aria-label="추가 리소스에 대한 직접 링크" title="추가 리소스에 대한 직접 링크" translate="no">​</a></h2>
<ul>
<li><a href="https://github.com/awslabs/ai-on-eks" target="_blank" rel="noopener noreferrer">AI on EKS GitHub 리포지토리</a></li>
<li><a href="https://docs.aws.amazon.com/eks/" target="_blank" rel="noopener noreferrer">AWS EKS 문서</a></li>
<li><a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/" target="_blank" rel="noopener noreferrer">NVIDIA GPU Operator</a></li>
<li><a href="https://awsdocs-neuron.readthedocs-hosted.com/" target="_blank" rel="noopener noreferrer">AWS Neuron 문서</a></li>
<li><a href="https://docs.vllm.ai/" target="_blank" rel="noopener noreferrer">vLLM 문서</a></li>
<li><a href="https://docs.ray.io/" target="_blank" rel="noopener noreferrer">Ray 문서</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="커뮤니티-및-지원">커뮤니티 및 지원<a href="#커뮤니티-및-지원" class="hash-link" aria-label="커뮤니티 및 지원에 대한 직접 링크" title="커뮤니티 및 지원에 대한 직접 링크" translate="no">​</a></h2>
<ul>
<li><a href="https://github.com/awslabs/ai-on-eks/issues" target="_blank" rel="noopener noreferrer">GitHub Issues</a> - 버그 리포트 또는 기능 요청</li>
<li><a href="https://github.com/awslabs/ai-on-eks/discussions" target="_blank" rel="noopener noreferrer">GitHub Discussions</a> - 질문 및 경험 공유</li>
<li><a href="https://repost.aws/" target="_blank" rel="noopener noreferrer">AWS re<!-- -->:Post</a> - AWS 전문가 및 커뮤니티의 도움 받기</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/awslabs/ai-on-eks/blob/main/website/docs/infra/inference/index.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>페이지 편집</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="문서 페이지"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai-on-eks/ko/docs/infra/training/jupyterhub"><div class="pagination-nav__sublabel">이전</div><div class="pagination-nav__label">EKS 기반 JupyterHub</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai-on-eks/ko/docs/infra/inference/inference-ready-cluster"><div class="pagination-nav__sublabel">다음</div><div class="pagination-nav__label">추론 준비 클러스터</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#라이프사이클-개요" class="table-of-contents__link toc-highlight">라이프사이클 개요</a></li><li><a href="#1단계-인프라-설정" class="table-of-contents__link toc-highlight">1단계: 인프라 설정</a><ul><li><a href="#인프라가-중요한-이유" class="table-of-contents__link toc-highlight">인프라가 중요한 이유</a></li><li><a href="#솔루션-추론-준비-클러스터" class="table-of-contents__link toc-highlight">솔루션: 추론 준비 클러스터</a></li></ul></li><li><a href="#2단계-모델-배포" class="table-of-contents__link toc-highlight">2단계: 모델 배포</a><ul><li><a href="#배포-패턴이-중요한-이유" class="table-of-contents__link toc-highlight">배포 패턴이 중요한 이유</a></li><li><a href="#솔루션-추론-차트" class="table-of-contents__link toc-highlight">솔루션: 추론 차트</a></li></ul></li><li><a href="#3단계-최적화" class="table-of-contents__link toc-highlight">3단계: 최적화</a><ul><li><a href="#최적화가-중요한-이유" class="table-of-contents__link toc-highlight">최적화가 중요한 이유</a></li><li><a href="#최적화-기법" class="table-of-contents__link toc-highlight">최적화 기법</a></li><li><a href="#최적화-워크플로우" class="table-of-contents__link toc-highlight">최적화 워크플로우</a></li></ul></li><li><a href="#전체-라이프사이클-예제" class="table-of-contents__link toc-highlight">전체 라이프사이클 예제</a><ul><li><a href="#1단계-인프라-배포" class="table-of-contents__link toc-highlight">1단계: 인프라 배포</a></li><li><a href="#2단계-모델-배포-1" class="table-of-contents__link toc-highlight">2단계: 모델 배포</a></li><li><a href="#3단계-최적화-지속적" class="table-of-contents__link toc-highlight">3단계: 최적화 (지속적)</a></li></ul></li><li><a href="#다음-단계" class="table-of-contents__link toc-highlight">다음 단계</a></li><li><a href="#추가-리소스" class="table-of-contents__link toc-highlight">추가 리소스</a></li><li><a href="#커뮤니티-및-지원" class="table-of-contents__link toc-highlight">커뮤니티 및 지원</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">참여하기</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/awslabs/ai-on-eks" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Built with ❤️ at AWS  <br> © ${new Date().getFullYear()} Amazon.com, Inc. or its affiliates. All Rights Reserved</div></div></div></footer><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;7fbc7ab02fae4767b1af2588eba0cdf2&quot;}"></script></div>
</body>
</html>
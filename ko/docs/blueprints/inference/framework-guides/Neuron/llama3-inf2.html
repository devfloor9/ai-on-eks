<!doctype html>
<html lang="ko-KR" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-blueprints/inference/framework-guides/Neuron/llama3-inf2" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.0">
<title data-rh="true">Inferentia2의 Llama-3-8B | AI on EKS</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama3-inf2"><meta data-rh="true" property="og:locale" content="ko_KR"><meta data-rh="true" property="og:locale:alternate" content="en_US"><meta data-rh="true" name="docusaurus_locale" content="ko"><meta data-rh="true" name="docsearch:language" content="ko"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Inferentia2의 Llama-3-8B | AI on EKS"><meta data-rh="true" name="description" content="AWS Inferentia 가속기에서 효율적인 추론을 위해 Llama-3 모델 서빙."><meta data-rh="true" property="og:description" content="AWS Inferentia 가속기에서 효율적인 추론을 위해 Llama-3 모델 서빙."><link data-rh="true" rel="icon" href="/ai-on-eks/ko/img/header-icon.png"><link data-rh="true" rel="canonical" href="https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama3-inf2"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/docs/blueprints/inference/framework-guides/Neuron/llama3-inf2" hreflang="en-US"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama3-inf2" hreflang="ko-KR"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/docs/blueprints/inference/framework-guides/Neuron/llama3-inf2" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"EKS에서의 추론","item":"https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/inference/"},{"@type":"ListItem","position":2,"name":"Framework-Specific Deployment Guides","item":"https://awslabs.github.io/ai-on-eks/ko/docs/category/framework-specific-deployment-guides"},{"@type":"ListItem","position":3,"name":"EKS에서의 Neuron 추론","item":"https://awslabs.github.io/ai-on-eks/ko/docs/category/neuron-inference-on-eks"},{"@type":"ListItem","position":4,"name":"Inferentia2의 Llama-3-8B","item":"https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama3-inf2"}]}</script><link rel="stylesheet" href="/ai-on-eks/ko/assets/css/styles.c270b852.css">
<script src="/ai-on-eks/ko/assets/js/runtime~main.84f685ef.js" defer="defer"></script>
<script src="/ai-on-eks/ko/assets/js/main.62626cce.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><div role="region" aria-label="본문으로 건너뛰기"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">본문으로 건너뛰기</a></div><div class="theme-announcement-bar announcementBar_mb4j" style="background-color:#667eea;color:#ffffff" role="banner"><div class="announcementBarPlaceholder_vyr4"></div><div class="content_knG7 announcementBarContent_xLdY">GenAI on EKS workshop series! <a target="_blank" rel="noopener noreferrer" href="https://aws-experience.com/emea/smb/events/series/get-hands-on-with-amazon-eks?trk=9be4af2e-2339-40ae-b5e9-57b6a7704c36&sc_channel=el" style="color: #ffffff; text-decoration: underline; font-weight: bold; margin-left: 10px;">Register now →</a></div><button type="button" aria-label="닫기" class="clean-btn close closeButton_CVFx announcementBarClose_gvF7"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav aria-label="메인" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="사이드바 펼치거나 접기" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai-on-eks/ko/"><div class="navbar__logo"><img src="/ai-on-eks/ko/img/header-icon.png" alt="AIoEKS 로고" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai-on-eks/ko/img/header-icon.png" alt="AIoEKS 로고" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href="/ai-on-eks/ko/docs/infra">인프라</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/ai-on-eks/ko/docs/blueprints">블루프린트</a><a class="navbar__item navbar__link" href="/ai-on-eks/ko/docs/guidance">가이드</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/awslabs/ai-on-eks" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>한국어</a><ul class="dropdown__menu"><li><a href="/ai-on-eks/docs/blueprints/inference/framework-guides/Neuron/llama3-inf2" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en-US">English</a></li><li><a href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama3-inf2" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="ko-KR">한국어</a></li></ul></div><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="어두운 모드와 밝은 모드 전환하기 (현재 system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="맨 위로 스크롤하기" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="문서 사이드바" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai-on-eks/ko/docs/blueprints"><span title="개요" class="linkLabel_WmDU">개요</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai-on-eks/ko/docs/blueprints/inference"><span title="EKS에서의 추론" class="categoryLinkLabel_W154">EKS에서의 추론</span></a><button aria-label="사이드바 분류 &#x27;EKS에서의 추론&#x27; 접기" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai-on-eks/ko/docs/category/framework-specific-deployment-guides"><span title="Framework-Specific Deployment Guides" class="categoryLinkLabel_W154">Framework-Specific Deployment Guides</span></a><button aria-label="사이드바 분류 &#x27;Framework-Specific Deployment Guides&#x27; 접기" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai-on-eks/ko/docs/category/gpu-inference-on-eks"><span title="EKS에서의 GPU 추론" class="categoryLinkLabel_W154">EKS에서의 GPU 추론</span></a><button aria-label="사이드바 분류 &#x27;EKS에서의 GPU 추론&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai-on-eks/ko/docs/category/neuron-inference-on-eks"><span title="EKS에서의 Neuron 추론" class="categoryLinkLabel_W154">EKS에서의 Neuron 추론</span></a><button aria-label="사이드바 분류 &#x27;EKS에서의 Neuron 추론&#x27; 접기" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/vllm-ray-inf2"><span title="Inferentia2에서 vLLM을 사용한 Llama-3-8B" class="linkLabel_WmDU">Inferentia2에서 vLLM을 사용한 Llama-3-8B</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/Mistral-7b-inf2"><span title="Inferentia2의 Mistral-7B" class="linkLabel_WmDU">Inferentia2의 Mistral-7B</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama3-inf2"><span title="Inferentia2의 Llama-3-8B" class="linkLabel_WmDU">Inferentia2의 Llama-3-8B</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama2-inf2"><span title="Inferentia2의 Llama-2" class="linkLabel_WmDU">Inferentia2의 Llama-2</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/stablediffusion-inf2"><span title="Inferentia2의 Stable Diffusion" class="linkLabel_WmDU">Inferentia2의 Stable Diffusion</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/rayserve-ha"><span title="Ray Serve 고가용성" class="linkLabel_WmDU">Ray Serve 고가용성</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama4-trn2"><span title="Llama 4 with vLLM on Trainium" class="linkLabel_WmDU">Llama 4 with vLLM on Trainium</span></a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/inference-charts"><span title="추론 차트" class="linkLabel_WmDU">추론 차트</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai-on-eks/ko/docs/category/training-on-eks"><span title="EKS에서의 학습" class="categoryLinkLabel_W154">EKS에서의 학습</span></a><button aria-label="사이드바 분류 &#x27;EKS에서의 학습&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai-on-eks/ko/docs/blueprints/gateways/envoy-gateway"><span title="게이트웨이" class="categoryLinkLabel_W154">게이트웨이</span></a></div></li></ul></nav><button type="button" title="사이드바 숨기기" aria-label="사이드바 숨기기" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="탐색 경로"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="홈" class="breadcrumbs__link" href="/ai-on-eks/ko/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai-on-eks/ko/docs/blueprints/inference"><span>EKS에서의 추론</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai-on-eks/ko/docs/category/framework-specific-deployment-guides"><span>Framework-Specific Deployment Guides</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai-on-eks/ko/docs/category/neuron-inference-on-eks"><span>EKS에서의 Neuron 추론</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Inferentia2의 Llama-3-8B</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">이 페이지에서</button></div><div class="theme-doc-markdown markdown"><div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>경고</div><div class="admonitionContent_BuS1"><p>EKS에 ML 모델을 배포하려면 GPU 또는 Neuron 인스턴스에 대한 액세스가 필요합니다. 배포가 작동하지 않는 경우 이러한 리소스에 대한 액세스가 누락되어 있기 때문인 경우가 많습니다. 또한 일부 배포 패턴은 Karpenter 오토스케일링 및 정적 노드 그룹에 의존합니다. 노드가 초기화되지 않으면 Karpenter 또는 노드 그룹의 로그를 확인하여 문제를 해결하십시오.</p></div></div>
<div class="theme-admonition theme-admonition-danger admonition_xJq3 alert alert--danger"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"></path></svg></span>위험</div><div class="admonitionContent_BuS1"><p>참고: 이 Llama-3 Instruct 모델의 사용은 Meta 라이선스의 적용을 받습니다.
모델 가중치와 토크나이저를 다운로드하려면 <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B" target="_blank" rel="noopener noreferrer">웹사이트</a>를 방문하여 액세스를 요청하기 전에 라이선스에 동의해 주십시오.</p></div></div>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>정보</div><div class="admonitionContent_BuS1"><p>관측성, 로깅 및 확장성 측면의 개선 사항을 포함하기 위해 이 블루프린트를 적극적으로 개선하고 있습니다.</p></div></div>
<header><h1>Inferentia, Ray Serve 및 Gradio를 사용한 Llama-3-8B Instruct 모델 서빙</h1></header>
<p><a href="https://docs.ray.io/en/latest/serve/index.html" target="_blank" rel="noopener noreferrer">Ray Serve</a>를 사용하여 Amazon Elastic Kubernetes Service (EKS)에 <a href="https://ai.meta.com/llama/#inside-the-model" target="_blank" rel="noopener noreferrer">Meta Llama-3-8B Instruct</a> 모델을 배포하는 포괄적인 가이드에 오신 것을 환영합니다.</p>
<p>이 튜토리얼에서는 Llama-3의 강력한 기능을 활용하는 방법뿐만 아니라 대규모 언어 모델(LLM)을 효율적으로 배포하는 복잡한 과정에 대한 통찰력을 얻을 수 있습니다. 특히 대규모 언어 모델 배포 및 확장에 최적화된 <code>inf2.24xlarge</code> 및 <code>inf2.48xlarge</code>와 같은 <a href="https://aws.amazon.com/machine-learning/neuron/" target="_blank" rel="noopener noreferrer">trn1/inf2</a> (AWS Trainium 및 Inferentia 기반) 인스턴스에서의 배포를 다룹니다.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="llama-3-8b-instruct란">Llama-3-8B Instruct란?<a href="#llama-3-8b-instruct란" class="hash-link" aria-label="Llama-3-8B Instruct란?에 대한 직접 링  크" title="Llama-3-8B Instruct란?에 대한 직접 링크" translate="no">​</a></h3>
<p>Meta는 8B 및 70B 크기의 사전 훈련 및 명령어 조정 생성 텍스트 모델 컬렉션인 Meta Llama 3 대규모 언어 모델(LLM) 제품군을 개발하고 출시했습니다. Llama 3 명령어 조정 모델은 대화 사용 사례에 최적화되어 있으며 일반적인 업계 벤치마크에서 사용 가능한 많은 오픈 소스 채팅 모델을 능가합니다. 또한 이러한 모델을 개발할 때 유용성과 안전성을 최적화하는 데 세심한 주의를 기울였습니다.</p>
<p>Llama3 크기 및 모델 아키텍처에 대한 자세한 정보는 <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct" target="_blank" rel="noopener noreferrer">여기</a>에서 확인할 수 있습니다.</p>
<p><strong>확장성 및 가용성</strong></p>
<p>Llama-3와 같은 대규모 언어 모델(<code>LLM</code>)을 배포할 때 주요 과제 중 하나는 적절한 하드웨어의 확장성과 가용성입니다. 기존 <code>GPU</code> 인스턴스는 높은 수요로 인해 부족한 경우가 많아 리소스를 효과적으로 프로비저닝하고 확장하기가 어렵습니다.</p>
<p>반면 <code>trn1.32xlarge</code>, <code>trn1n.32xlarge</code>, <code>inf2.24xlarge</code> 및 <code>inf2.48xlarge</code>와 같은 <code>Trn1/Inf2</code> 인스턴스는 LLM을 포함한 생성형 AI 모델의 고성능 딥러닝(DL) 훈련 및 추론을 위해 특별히 구축되었습니다. 확장성과 가용성을 모두 제공하여 리소스 병목 현상이나 지연 없이 필요에 따라 <code>Llama-3</code> 모델을 배포하고 확장할 수 있습니다.</p>
<p><strong>비용 최적화</strong></p>
<p>기존 GPU 인스턴스에서 LLM을 실행하면 GPU의 부족과 경쟁적인 가격으로 인해 비용이 많이 들 수 있습니다. <strong>Trn1/Inf2</strong> 인스턴스는 비용 효율적인 대안을 제공합니다. AI 및 기계 학습 작업에 최적화된 전용 하드웨어를 제공함으로써 Trn1/Inf2 인스턴스를 통해 비용의 일부로 최고 수준의 성능을 달성할 수 있습니다. 이러한 비용 최적화를 통해 예산을 효율적으로 할당하여 LLM 배포를 접근 가능하고 지속 가능하게 만들 수 있습니다.</p>
<p><strong>성능 향상</strong></p>
<p>Llama-3는 GPU에서 고성능 추론을 달성할 수 있지만, Neuron 가속기는 성능을 한 단계 더 끌어올립니다. Neuron 가속기는 기계 학습 워크로드를 위해 특별히 구축되어 Llama-3의 추론 속도를 크게 향상시키는 하드웨어 가속을 제공합니다. 이는 Trn1/Inf2 인스턴스에 Llama-3를 배포할 때 더 빠른 응답 시간과 개선된 사용자 경험으로 이어집니다.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="예제-사용-사례">예제 사용 사례<a href="#예제-사용-사례" class="hash-link" aria-label="예제 사용 사례에 대한 직접 링크" title="예제 사용 사례에 대한 직접 링크" translate="no">​</a></h3>
<p>회사가 고객 지원을 제공하기 위해 Llama-3 챗봇을 배포하려고 합니다. 회사는 대규모 고객 기반을 보유하고 있으며 피크 시간에 많은 양의 채팅 요청을 받을 것으로 예상합니다. 회사는 높은 요청량을 처리하고 빠른 응답 시간을 제공할 수 있는 인프라를 설계해야 합니다.</p>
<p>회사는 Inferentia2 인스턴스를 사용하여 Llama-3 챗봇을 효율적으로 확장할 수 있습니다. Inferentia2 인스턴스는 기계 학습 작업을 위한 특수 하드웨어 가속기입니다. 기계 학습 워크로드에 대해 GPU보다 최대 20배 더 나은 성능과 최대 7배 더 낮은 비용을 제공할 수 있습니다.</p>
<p>회사는 또한 Ray Serve를 사용하여 Llama-3 챗봇을 수평으로 확장할 수 있습니다. Ray Serve는 기계 학습 모델을 서빙하기 위한 분산 프레임워크입니다. 수요에 따라 모델을 자동으로 확장하거나 축소할   수 있습니다.</p>
<p>Llama-3 챗봇을 확장하기 위해 회사는 여러 Inferentia2 인스턴스를 배포하고 Ray Serve를 사용하여 인스턴스 간에 트래픽을 분산할 수 있습니다. 이를 통해 회사는 높은 요청량을 처리하고 빠른 응답 시간을 제공할 수 있습니다.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="솔루션-아키텍처">솔루션 아키텍처<a href="#솔루션-아키텍처" class="hash-link" aria-label="솔루션 아키텍처에 대한 직접 링크" title="솔루션 아키텍처에 대한 직접 링크" translate="no">​</a></h2>
<p>이 섹션에서는 Amazon EKS에서 Llama-3 모델, <a href="https://docs.ray.io/en/latest/serve/index.html" target="_blank" rel="noopener noreferrer">Ray Serve</a> 및 <a href="https://aws.amazon.com/ec2/instance-types/inf2/" target="_blank" rel="noopener noreferrer">Inferentia2</a>를 결합한 솔루션의 아키텍처를 자세히 살펴봅니다.</p>
<p><img decoding="async" loading="lazy" alt="Llama-3-inf2" src="/ai-on-eks/ko/assets/images/llama3-d03bea05cda1353f5e86b8c4b060635a.png" width="1818" height="1019" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="솔루션-배포">솔루션 배포<a href="#솔루션-배포" class="hash-link" aria-label="솔루션 배포에 대한 직접 링크" title="솔루션 배포에 대한 직접 링크" translate="no">​</a></h2>
<p><a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener noreferrer">Amazon EKS</a>에 <code>Llama-3-8b-instruct</code>를 배포하려면 필요한 사전 요구 사항을 다루고 배포 프로세스를 단계별로 안내합니다.</p>
<p>여기에는 인프라 설정, <strong>Ray 클러스터</strong> 배포 및 <a href="https://www.gradio.app/" target="_blank" rel="noopener noreferrer">Gradio</a> WebUI 앱 생성이 포함됩니다.</p>
<div class="collapsibleContent_q3kw"><div class="header_QCEw"><h2><span>사전 요구 사항</span></h2><span class="icon_PckA">👈</span></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llama3-모델이-있는-ray-클러스터-배포">Llama3 모델이 있는 Ray 클러스터 배포<a href="#llama3-모델이-있는-ray-클러스터-배포" class="hash-link" aria-label="Llama3 모델이 있는 Ray 클러스터 배포에 대한 직접 링크" title="Llama3 모델이 있는 Ray 클러스터 배포에 대한 직접 링크" translate="no">​</a></h2>
<p><code>Trainium on EKS</code> 클러스터가 배포되면 <code>kubectl</code>을 사용하여 <code>ray-service-Llama-3.yaml</code>을 배포할 수 있습니다.</p>
<p>이 단계에서는 Karpenter 오토스케일링을 사용하는 <code>x86 CPU</code> 인스턴스의 <code>Head Pod</code> 하나와 <a href="https://karpenter.sh/" target="_blank" rel="noopener noreferrer">Karpenter</a>에 의해 오토스케일링되는 <code>Inf2.48xlarge</code> 인스턴스의 <code>Ray 워커</code>로 구성된 Ray Serve 클러스터를 배포합니다.</p>
<p>배포를 진행하기 전에 이 배포에서 사용되는 주요 파일을 자세히 살펴보고 기능을 이해해 봅시다:</p>
<ul>
<li><strong>ray_serve_Llama-3.py:</strong></li>
</ul>
<p>이 스크립트는 FastAPI, Ray Serve 및 PyTorch 기반 Hugging Face Transformers를 사용하여 <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct" target="_blank" rel="noopener noreferrer">meta-llama/Meta-Llama-3-8B-Instruct</a> 언어 모델을 사용한 효율적인 텍스트 생성 API를 생성합니다.</p>
<p>스크립트는 입력 문장을 수락하고 향상된 성능을 위한 Neuron 가속의 이점을 활용하여 텍스트 출력을 효율적으로 생성하는 엔드포인트를 설정합니다. 높은 구성 가능성으로 사용자는 챗봇 및 텍스트 생성 작업을 포함한 다양한 자연어 처리 애플리케이션에 맞게 모델 파라미터를 미세 조정할 수 있습니다.</p>
<ul>
<li><strong>ray-service-Llama-3.yaml:</strong></li>
</ul>
<p>이 Ray Serve YAML 파일은 <code>llama-3-8B-Instruct</code> 모델을 사용한 효율적인 텍스트 생성을 용이하게 하는 Ray Serve 서비스를 배포하기 위한 Kubernetes 구성 역할을 합니다.</p>
<p>리소스를 분리하기 위해 <code>llama3</code>라는 Kubernetes 네임스페이스를 정의합니다. 구성 내에서 <code>llama-3</code>라는 <code>RayService</code> 사양이 생성되고 <code>llama3</code> 네임스페이스 내에 호스팅됩니다. <code>RayService</code> 사양은 Ray Serve 서비스를 생성하기 위해 Python 스크립트 <code>ray_serve_llama3.py</code> (같은 폴더 내의 Dockerfile에 복사됨)를 활용합니다.</p>
<p>이 예제에서 사용된 Docker 이미지는 배포 용이성을 위해 Amazon Elastic Container Registry (ECR)에 공개적으로 제공됩니다.
사용자는 특정 요구 사항에 맞게 Dockerfile을 수정하고 자체 ECR 리포지토리에 푸시하여 YAML 파일에서 참조할 수도 있습니다.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="llama-3-instruct-모델-배포">Llama-3-Instruct 모델 배포<a href="#llama-3-instruct-모델-배포" class="hash-link" aria-label="Llama-3-Instruct 모델 배포에 대한 직접 링크" title="Llama-3-Instruct 모델 배포에 대한 직접 링크" translate="no">​</a></h3>
<p><strong>클러스터가 로컬에서 구성되었는지 확인</strong></p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">aws eks </span><span class="token parameter variable" style="color:#36acaa">--region</span><span class="token plain"> us-west-2 update-kubeconfig </span><span class="token parameter variable" style="color:#36acaa">--name</span><span class="token plain"> trainium-inferentia</span><br></span></code></pre></div></div>
<p><strong>RayServe 클러스터 배포</strong></p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>정보</div><div class="admonitionContent_BuS1"><p>llama3-8B-Instruct 모델을 배포하려면 Hugging Face Hub 토큰을 환경 변수로 구성하는 것이 필수적입니다. 이 토큰은 인증 및 모델 액세스에 필요합니다. Hugging Face 토큰 생성 및 관리 방법에 대한 지침은 <a href="https://huggingface.co/docs/hub/security-tokens" target="_blank" rel="noopener noreferrer">Hugging Face Token Management</a>를 참조하십시오.</p></div></div>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Hugging Face Hub 토큰을 환경 변수로 설정합니다. 이 변수는 ray-service-llama3.yaml 파일을 적용할 때 대체됩니다</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token builtin class-name">export</span><span class="token plain">  </span><span class="token assign-left variable" style="color:#36acaa">HUGGING_FACE_HUB_TOKEN</span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">Your-Hugging-Face-Hub-Token-Value</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token builtin class-name">cd</span><span class="token plain"> ai-on-eks/blueprints/inference/llama3-8b-rayserve-inf2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">envsubst </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain"> ray-service-llama3.yaml</span><span class="token operator" style="color:#393A34">|</span><span class="token plain"> kubectl apply </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> -</span><br></span></code></pre></div></div>
<p>다음 명령을 실행하여 배포 확인</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>정보</div><div class="admonitionContent_BuS1"><p>배포 프로세스는 최대 10분이 소요될 수 있습니다. Head Pod는 2~3분 내에 준비되고, Ray Serve 워커 Pod는 Huggingface에서 이미지 검색 및 모델 배포에 최대 10분이 소요될 수 있습니다.</p></div></div>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$ kubectl get all -n llama3</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME                                                          READY   STATUS              RESTARTS   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pod/llama3-raycluster-smqrl-head-4wlbb                        0/1     Running             0          77s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pod/service-raycluster-smqrl-worker-inf2-wjxqq                0/1     Running             0          77s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME                     TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                                       AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">service/llama3           ClusterIP   172.20.246.48   &lt;none&gt;       8000:32138/TCP,52365:32653/TCP,8080:32604/TCP,6379:32739/TCP,8265:32288/TCP,10001:32419/TCP   78s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">$ kubectl get ingress -n llama3</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME             CLASS   HOSTS   ADDRESS                                                                         PORTS   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">llama3           nginx   *       k8s-ingressn-ingressn-randomid-randomid.elb.us-west-2.amazonaws.com             80      2m4s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre></div></div>
<p>이제 아래의 로드 밸런서 URL을 사용하여 Ray 대시보드에 액세스할 수 있습니다.</p>
<p>http://&lt;NLB_DNS_NAME&gt;/dashboard/#/serve</p>
<p>공개 로드 밸런서에 액세스할 수 없는 경우 포트 포워딩을 사용하고 다음 명령으로 localhost를 사용하여 Ray 대시보드를 탐색할 수 있습니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward svc/llama3 </span><span class="token number" style="color:#36acaa">8265</span><span class="token plain">:8265 </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> llama3</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 브라우저에서 링크 열기</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">http://localhost:8265/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre></div></div>
<p>이 웹페이지에서 아래 이미지와 같이 모델 배포 진행 상황을 모니터링할 수 있습니다:</p>
<p><img decoding="async" loading="lazy" alt="Ray Dashboard" src="/ai-on-eks/ko/assets/images/ray-dashboard-f6cb3c6c4a023ceb750a02ccc787b6d0.png" width="1345" height="886" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="llama3-모델-테스트">Llama3 모델 테스트<a href="#llama3-모델-테스트" class="hash-link" aria-label="Llama3 모델 테스트에 대한 직접 링크" title="Llama3 모델 테스트에 대한 직접 링크" translate="no">​</a></h3>
<p>모델 배포 상태가 <code>running</code> 상태가 되면 Llama-3-instruct 사용을 시작할 수 있습니다.</p>
<p>URL 끝에 쿼리를 추가하여 다음 URL을 사용할 수 있습니다.</p>
<p>http://&lt;NLB_DNS_NAME&gt;/serve/infer?sentence=what is data parallelism and tensor parallelisma and the differences</p>
<p>브라우저에서 다음과 같은 출력을 볼 수 있습니다:</p>
<p><img decoding="async" loading="lazy" alt="Chat Output" src="/ai-on-eks/ko/assets/images/llama-2-chat-ouput-e291e1c9481febc387222da8c06c1170.png" width="2432" height="1308" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="gradio-webui-앱-배포">Gradio WebUI 앱 배포<a href="#gradio-webui-앱-배포" class="hash-link" aria-label="Gradio WebUI 앱 배포에 대한 직접 링크" title="Gradio WebUI 앱 배포에 대한 직접 링크" translate="no">​</a></h2>
<p>배포된 모델과 원활하게 통합되는 사용자 친화적인 채팅 인터페이스를 <a href="https://www.gradio.app/" target="_blank" rel="noopener noreferrer">Gradio</a>를 사용하여 만드는 방법을 알아봅니다.</p>
<p>RayServe를 사용하여 배포된 LLama-3-Instruct 모델과 상호 작용하기 위해 로컬 머신에 Gradio 앱을 배포해 봅시다.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>정보</div><div class="admonitionContent_BuS1"><p>Gradio 앱은 데모 목적으로만 생성된 로컬로 노출된 서비스와 상호 작용합니다. 또는 더 넓은 접근성을 위해 Ingress 및 Load Balancer가 있는 Pod로 EKS에 Gradio 앱을 배포할 수 있습니다.</p></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="llama3-ray-서비스로-포트-포워딩-실행">llama3 Ray 서비스로 포트 포워딩 실행<a href="#llama3-ray-서비스로-포트-포워딩-실행" class="hash-link" aria-label="llama3 Ray 서비스로 포트 포워딩 실행에 대한 직접 링크" title="llama3 Ray 서비스로 포트 포워딩 실행에 대한 직접 링크" translate="no">​</a></h3>
<p>먼저 kubectl을 사용하여 Llama-3 Ray 서비스로 포트 포워딩을 실행합니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward svc/llama3-service </span><span class="token number" style="color:#36acaa">8000</span><span class="token plain">:8000 </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> llama3</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="gradio-webui-앱-배포-1">Gradio WebUI 앱 배포<a href="#gradio-webui-앱-배포-1" class="hash-link" aria-label="Gradio WebUI 앱 배포에 대한 직접 링크" title="Gradio WebUI 앱 배포  에 대한 직접 링크" translate="no">​</a></h2>
<p>배포된 모델과 원활하게 통합되는 사용자 친화적인 채팅 인터페이스를 <a href="https://www.gradio.app/" target="_blank" rel="noopener noreferrer">Gradio</a>를 사용하여 만드는 방법을 알아봅니다.</p>
<p>localhost에서 Docker 컨테이너로 실행되는 Gradio 앱을 설정하는 것으로 진행합니다. 이 설정을 통해 RayServe를 사용하여 배포된 Llama-3-Instruct 모델과 상호 작용할 수 있습니다.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="gradio-앱-docker-컨테이너-빌드">Gradio 앱 Docker 컨테이너 빌드<a href="#gradio-앱-docker-컨테이너-빌드" class="hash-link" aria-label="Gradio 앱 Docker 컨테이너 빌드에 대한 직접 링크" title="Gradio 앱 Docker 컨테이너 빌드에 대한 직접 링크" translate="no">​</a></h3>
<p>먼저 클라이언트 앱용 Docker 컨테이너를 빌드합니다.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> ai-on-eks/blueprints/inference/gradio-ui</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token function" style="color:#d73a49">docker</span><span class="token plain"> build </span><span class="token parameter variable" style="color:#36acaa">--platform</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">linux/amd64 </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token parameter variable" style="color:#36acaa">-t</span><span class="token plain"> gradio-app:llama </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --build-arg </span><span class="token assign-left variable" style="color:#36acaa">GRADIO_APP</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;gradio-app-llama.py&quot;</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token builtin class-name">.</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="gradio-컨테이너-배포">Gradio 컨테이너 배포<a href="#gradio-컨테이너-배포" class="hash-link" aria-label="Gradio 컨테이너 배포에 대한 직접 링크" title="Gradio 컨테이너 배포에 대한 직접 링크" translate="no">​</a></h3>
<p>docker를 사용하여 localhost에서 컨테이너로 Gradio 앱을 배포합니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">docker</span><span class="token plain"> run </span><span class="token parameter variable" style="color:#36acaa">--rm</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">-it</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">-p</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">7860</span><span class="token plain">:7860 </span><span class="token parameter variable" style="color:#36acaa">-p</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">8000</span><span class="token plain">:8000 gradio-app:llama</span><br></span></code></pre></div></div>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>정보</div><div class="admonitionContent_BuS1"><p>머신에서 Docker Desktop을 실행하지 않고 <a href="https://runfinch.com/" target="_blank" rel="noopener noreferrer">finch</a>와 같은 것을 대신 사용하는 경우 컨테이너 내부의 사용자 정의 호스트-IP 매핑을 위한 추가 플래그가 필요합니다.</p><div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">docker run --rm -it \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --add-host ray-service:&lt;workstation-ip&gt; \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -e &quot;SERVICE_NAME=http://ray-service:8000&quot; \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    -p 7860:7860 gradio-app:llama</span><br></span></code></pre></div></div></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="webui-호출">WebUI 호출<a href="#webui-호출" class="hash-link" aria-label="WebUI 호출에 대한 직접 링크" title="WebUI 호출에 대한 직접 링크" translate="no">​</a></h4>
<p>웹 브라우저를 열고 다음 URL로 이동하여 Gradio WebUI에 액세스합니다:</p>
<p>로컬 URL에서 실행 중:  <a href="http://localhost:7860" target="_blank" rel="noopener noreferrer">http://localhost:7860</a></p>
<p>이제 로컬 머신에서 Gradio 애플리케이션과 상호 작용할 수 있습니다.</p>
<p><img decoding="async" loading="lazy" alt="Gradio Llama-3 AI Chat" src="/ai-on-eks/ko/assets/images/llama3-d03bea05cda1353f5e86b8c4b060635a.png" width="1818" height="1019" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="결론">결론<a href="#결론" class="hash-link" aria-label="결론에 대한 직접 링크" title="결론에 대한 직접 링크" translate="no">​</a></h2>
<p>요약하면, Llama-3를 배포하고 확장할 때 AWS Trn1/Inf2 인스턴스는 매력적인 이점을 제공합니다.
GPU 부족과 관련된 문제를 극복하면서 대규모 언어 모델을 효율적이고 접근 가능하게 실행하는 데 필요한 확장성, 비용 최적화 및 성능 향상을 제공합니다. 챗봇, 자연어 처리 애플리케이션 또는 기타 LLM 기반 솔루션을 구축하든 Trn1/Inf2 인스턴스를 통해 AWS 클라우드에서 Llama-3의 잠재력을 최대한 활용할 수 있습니다.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="정리">정리<a href="#정리" class="hash-link" aria-label="정리에 대한 직접 링크" title="정리에 대한 직접 링크" translate="no">​</a></h2>
<p>마지막으로 더 이상 필요하지 않은 리소스를 정리하고 프로비저닝 해제하는 방법을 안내합니다.</p>
<p><strong>1단계:</strong> Gradio 컨테이너 삭제</p>
<p>Gradio 앱을 실행하는 컨테이너를 종료하려면 <code>docker run</code>이 실행 중인 localhost 터미널 창에서 <code>Ctrl-c</code>를  누릅니다. 선택적으로 Docker 이미지를 정리합니다</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">docker</span><span class="token plain"> rmi gradio-app:llama</span><br></span></code></pre></div></div>
<p><strong>2단계:</strong> Ray 클러스터 삭제</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> ai-on-eks/blueprints/inference/llama3-8b-instruct-rayserve-inf2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl delete </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> ray-service-llama3.yaml</span><br></span></code></pre></div></div>
<p><strong>3단계:</strong> EKS 클러스터 정리
이 스크립트는 <code>-target</code> 옵션을 사용하여 모든 리소스가 올바른 순서로 삭제되도록 환경을 정리합니다.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> ai-on-eks/infra/trainium-inferentia/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">./cleanup.sh</span><br></span></code></pre></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/awslabs/ai-on-eks/blob/main/website/docs/blueprints/inference/framework-guides/Neuron/llama3-inf2.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>페이지 편집</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="문서 페이지"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/Mistral-7b-inf2"><div class="pagination-nav__sublabel">이전</div><div class="pagination-nav__label">Inferentia2의 Mistral-7B</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama2-inf2"><div class="pagination-nav__sublabel">다음</div><div class="pagination-nav__label">Inferentia2의 Llama-2</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#llama-3-8b-instruct란" class="table-of-contents__link toc-highlight">Llama-3-8B Instruct란?</a></li><li><a href="#예제-사용-사례" class="table-of-contents__link toc-highlight">예제 사용 사례</a></li><li><a href="#솔루션-아키텍처" class="table-of-contents__link toc-highlight">솔루션 아  키텍처</a></li><li><a href="#솔루션-배포" class="table-of-contents__link toc-highlight">솔루션 배포</a><ul><li><a href="#배포" class="table-of-contents__link toc-highlight">배포</a></li><li><a href="#리소스-확인" class="table-of-contents__link toc-highlight">리소스 확인</a></li></ul></li><li><a href="#llama3-모델이-있는-ray-클러스터-배포" class="table-of-contents__link toc-highlight">Llama3 모델이 있는 Ray 클러스터 배포</a><ul><li><a href="#llama-3-instruct-모델-배포" class="table-of-contents__link toc-highlight">Llama-3-Instruct 모델 배포</a></li><li><a href="#llama3-모델-테스트" class="table-of-contents__link toc-highlight">Llama3 모델 테스트</a></li></ul></li><li><a href="#gradio-webui-앱-배포" class="table-of-contents__link toc-highlight">Gradio WebUI 앱 배포</a><ul><li><a href="#llama3-ray-서비스로-포트-포워딩-실행" class="table-of-contents__link toc-highlight">llama3 Ray 서비스로 포트 포워딩 실행</a></li></ul></li><li><a href="#gradio-webui-앱-배포-1" class="table-of-contents__link toc-highlight">Gradio WebUI 앱 배포</a><ul><li><a href="#gradio-앱-docker-컨테이너-빌드" class="table-of-contents__link toc-highlight">Gradio 앱 Docker 컨테이너 빌드</a></li><li><a href="#gradio-컨테이너-배포" class="table-of-contents__link toc-highlight">Gradio 컨테이너 배포</a></li></ul></li><li><a href="#결론" class="table-of-contents__link toc-highlight">결론</a></li><li><a href="#정리" class="table-of-contents__link toc-highlight">정리</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">참여하기</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/awslabs/ai-on-eks" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Built with ❤️ at AWS  <br> © ${new Date().getFullYear()} Amazon.com, Inc. or its affiliates. All Rights Reserved</div></div></div></footer><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;7fbc7ab02fae4767b1af2588eba0cdf2&quot;}"></script></div>
</body>
</html>
<!doctype html>
<html lang="ko-KR" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-blueprints/inference/framework-guides/Neuron/llama2-inf2" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.0">
<title data-rh="true">Inferentia2의 Llama-2 | AI on EKS</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama2-inf2"><meta data-rh="true" property="og:locale" content="ko_KR"><meta data-rh="true" property="og:locale:alternate" content="en_US"><meta data-rh="true" name="docusaurus_locale" content="ko"><meta data-rh="true" name="docsearch:language" content="ko"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Inferentia2의 Llama-2 | AI on EKS"><meta data-rh="true" name="description" content="AWS Inferentia 가속기에서 효율적인 추론을 위해 Llama-2 모델 서빙."><meta data-rh="true" property="og:description" content="AWS Inferentia 가속기에서 효율적인 추론을 위해 Llama-2 모델 서빙."><link data-rh="true" rel="icon" href="/ai-on-eks/ko/img/header-icon.png"><link data-rh="true" rel="canonical" href="https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama2-inf2"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/docs/blueprints/inference/framework-guides/Neuron/llama2-inf2" hreflang="en-US"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama2-inf2" hreflang="ko-KR"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/docs/blueprints/inference/framework-guides/Neuron/llama2-inf2" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"EKS에서의 추론","item":"https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/inference/"},{"@type":"ListItem","position":2,"name":"Framework-Specific Deployment Guides","item":"https://awslabs.github.io/ai-on-eks/ko/docs/category/framework-specific-deployment-guides"},{"@type":"ListItem","position":3,"name":"EKS에서의 Neuron 추론","item":"https://awslabs.github.io/ai-on-eks/ko/docs/category/neuron-inference-on-eks"},{"@type":"ListItem","position":4,"name":"Inferentia2의 Llama-2","item":"https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama2-inf2"}]}</script><link rel="stylesheet" href="/ai-on-eks/ko/assets/css/styles.c270b852.css">
<script src="/ai-on-eks/ko/assets/js/runtime~main.84f685ef.js" defer="defer"></script>
<script src="/ai-on-eks/ko/assets/js/main.d98ac8f0.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><div role="region" aria-label="본문으로 건너뛰기"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">본문으로 건너뛰기</a></div><div class="theme-announcement-bar announcementBar_mb4j" style="background-color:#667eea;color:#ffffff" role="banner"><div class="announcementBarPlaceholder_vyr4"></div><div class="content_knG7 announcementBarContent_xLdY">GenAI on EKS workshop series! <a target="_blank" rel="noopener noreferrer" href="https://aws-experience.com/emea/smb/events/series/get-hands-on-with-amazon-eks?trk=9be4af2e-2339-40ae-b5e9-57b6a7704c36&sc_channel=el" style="color: #ffffff; text-decoration: underline; font-weight: bold; margin-left: 10px;">Register now →</a></div><button type="button" aria-label="닫기" class="clean-btn close closeButton_CVFx announcementBarClose_gvF7"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav aria-label="메인" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="사이드바 펼치거나 접기" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai-on-eks/ko/"><div class="navbar__logo"><img src="/ai-on-eks/ko/img/header-icon.png" alt="AIoEKS 로고" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai-on-eks/ko/img/header-icon.png" alt="AIoEKS 로고" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href="/ai-on-eks/ko/docs/infra">인프라</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/ai-on-eks/ko/docs/blueprints">블루프린트</a><a class="navbar__item navbar__link" href="/ai-on-eks/ko/docs/guidance">가이드</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/awslabs/ai-on-eks" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>한국어</a><ul class="dropdown__menu"><li><a href="/ai-on-eks/docs/blueprints/inference/framework-guides/Neuron/llama2-inf2" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en-US">English</a></li><li><a href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama2-inf2" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="ko-KR">한국어</a></li></ul></div><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="어두운 모드와 밝은 모드 전환하기 (현재 system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="맨 위로 스크롤하기" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="문서 사이드바" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai-on-eks/ko/docs/blueprints"><span title="개요" class="linkLabel_WmDU">개요</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai-on-eks/ko/docs/blueprints/inference"><span title="EKS에서의 추론" class="categoryLinkLabel_W154">EKS에서의 추론</span></a><button aria-label="사이드바 분류 &#x27;EKS에서의 추론&#x27; 접기" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai-on-eks/ko/docs/category/framework-specific-deployment-guides"><span title="Framework-Specific Deployment Guides" class="categoryLinkLabel_W154">Framework-Specific Deployment Guides</span></a><button aria-label="사이드바 분류 &#x27;Framework-Specific Deployment Guides&#x27; 접기" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai-on-eks/ko/docs/category/gpu-inference-on-eks"><span title="EKS에서의 GPU 추론" class="categoryLinkLabel_W154">EKS에서의 GPU 추론</span></a><button aria-label="사이드바 분류 &#x27;EKS에서의 GPU 추론&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai-on-eks/ko/docs/category/neuron-inference-on-eks"><span title="EKS에서의 Neuron 추론" class="categoryLinkLabel_W154">EKS에서의 Neuron 추론</span></a><button aria-label="사이드바 분류 &#x27;EKS에서의 Neuron 추론&#x27; 접기" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/vllm-ray-inf2"><span title="Inferentia2에서 vLLM을 사용한 Llama-3-8B" class="linkLabel_WmDU">Inferentia2에서 vLLM을 사용한 Llama-3-8B</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/Mistral-7b-inf2"><span title="Inferentia2의 Mistral-7B" class="linkLabel_WmDU">Inferentia2의 Mistral-7B</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama3-inf2"><span title="Inferentia2의 Llama-3-8B" class="linkLabel_WmDU">Inferentia2의 Llama-3-8B</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama2-inf2"><span title="Inferentia2의 Llama-2" class="linkLabel_WmDU">Inferentia2의 Llama-2</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/stablediffusion-inf2"><span title="Inferentia2의 Stable Diffusion" class="linkLabel_WmDU">Inferentia2의 Stable Diffusion</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/rayserve-ha"><span title="Ray Serve 고가용성" class="linkLabel_WmDU">Ray Serve 고가용성</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama4-trn2"><span title="Llama 4 with vLLM on Trainium" class="linkLabel_WmDU">Llama 4 with vLLM on Trainium</span></a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/inference-charts"><span title="추론 차트" class="linkLabel_WmDU">추론 차트</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai-on-eks/ko/docs/category/training-on-eks"><span title="EKS에서의 학습" class="categoryLinkLabel_W154">EKS에서의 학습</span></a><button aria-label="사이드바 분류 &#x27;EKS에서의 학습&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai-on-eks/ko/docs/blueprints/gateways/envoy-gateway"><span title="게이트웨이" class="categoryLinkLabel_W154">게이트웨이</span></a></div></li></ul></nav><button type="button" title="사이드바 숨기기" aria-label="사이드바 숨기기" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="탐색 경로"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="홈" class="breadcrumbs__link" href="/ai-on-eks/ko/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai-on-eks/ko/docs/blueprints/inference"><span>EKS에서의 추론</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai-on-eks/ko/docs/category/framework-specific-deployment-guides"><span>Framework-Specific Deployment Guides</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai-on-eks/ko/docs/category/neuron-inference-on-eks"><span>EKS에서의 Neuron 추론</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Inferentia2의 Llama-2</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">이 페이지에서</button></div><div class="theme-doc-markdown markdown"><div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>경고</div><div class="admonitionContent_BuS1"><p>EKS에 ML 모델을 배포하려면 GPU 또는 Neuron 인스턴스에 대한 액세스가 필요합니다. 배포가 작동하지 않는 경우 이러한 리소스에 대한 액세스가 누락되어 있기 때문인 경우가 많습니다. 또한 일부 배포 패턴은 Karpenter 오토스케일링 및 정적 노드 그룹에 의존합니다. 노드가 초기화되지 않으면 Karpenter 또는 노드 그룹의 로그를 확인하여 문제를 해결하십시오.</p></div></div>
<div class="theme-admonition theme-admonition-danger admonition_xJq3 alert alert--danger"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"></path></svg></span>위험</div><div class="admonitionContent_BuS1"><p>참고: 이 Llama-2 모델의 사용은 Meta 라이선스의 적용을 받습니다.
모델 가중치와 토크나이저를 다운로드하려면 <a href="https://ai.meta.com/" target="_blank" rel="noopener noreferrer">웹사이트</a>를 방문하여 액세스를 요청하기 전에 라이선스에 동의해 주십시오.</p></div></div>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>정보</div><div class="admonitionContent_BuS1"><p>관측성, 로깅 및 확장성 측면의 개선 사항을 포함하기 위해 이 블루프린트를 적극적으로 개선하고 있습니다.</p></div></div>
<header><h1>Inferentia, Ray Serve 및 Gradio를 사용한 Llama-2-13b Chat 모델 서빙</h1></header>
<p><a href="https://docs.ray.io/en/latest/serve/index.html" target="_blank" rel="noopener noreferrer">Ray Serve</a>를 사용하여 Amazon Elastic Kubernetes Service (EKS)에 <a href="https://ai.meta.com/llama/#inside-the-model" target="_blank" rel="noopener noreferrer">Meta Llama-2-13b chat</a> 모델을 배포하는 포괄적인 가이드에 오신 것을 환영합니다.
이 튜토리얼에서는 Llama-2의 강력한 기능을 활용하는 방법뿐만 아니라 대규모 언어 모델(LLM)을 효율적으로 배포하는 복잡한 과정에 대한 통찰력을 얻을 수 있습니다. 특히 대규모 언어 모델 배포 및 확장에 최적화된 <code>inf2.24xlarge</code> 및 <code>inf2.48xlarge</code>와 같은 <a href="https://aws.amazon.com/machine-learning/neuron/" target="_blank" rel="noopener noreferrer">trn1/inf2</a> (AWS Trainium 및 Inferentia 기반) 인스턴스에서의 배포를 다룹니다.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="llama-2란">Llama-2란?<a href="#llama-2란" class="hash-link" aria-label="Llama-2란?에 대한 직접 링크" title="Llama-2란?에 대한 직접 링크" translate="no">​</a></h3>
<p>Llama-2는 2조 개의  텍스트 및 코드 토큰으로 훈련된 사전 훈련된 대규모 언어 모델(LLM)입니다. 현재 사용 가능한 가장 크고 강력한 LLM 중 하나입니다. Llama-2는 자연어 처리, 텍스트 생성 및 번역을 포함한 다양한 작업에 사용할 수 있습니다.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="llama-2-chat">Llama-2-chat<a href="#llama-2-chat" class="hash-link" aria-label="Llama-2-chat에 대한 직접 링크" title="Llama-2-chat에 대한 직접 링크" translate="no">​</a></h4>
<p>Llama-2는 엄격한 훈련 과정을 거친 뛰어난 언어 모델입니다. 공개적으로 사용 가능한 온라인 데이터를 사용한 사전 훈련으로 시작합니다. 그런 다음 지도 미세 조정을 통해 초기 버전의 Llama-2-chat이 생성됩니다.
이후 <code>Llama-2-chat</code>은 거부 샘플링 및 근접 정책 최적화(<code>PPO</code>)와 같은 기술을 포함하는 인간 피드백을 통한 강화 학습(<code>RLHF</code>)을 사용하여 반복적으로 정제됩니다.
이 프로세스를 통해 <strong>Amazon EKS</strong>와 <strong>Ray Serve</strong>에서 효과적으로 배포하고 활용할 수 있도록 안내하는 고도로 유능하고 미세 조정된 언어 모델이 생성됩니다.</p>
<p>Llama-2는 세 가지 모델 크기로 제공됩니다:</p>
<ul>
<li><strong>Llama-2-70b:</strong> 700억 개의 파라미터를 가진 가장 큰 Llama-2 모델입니다. 가장 강력한 Llama-2 모델이며 가장 까다로운 작업에 사용할 수 있습니다.</li>
<li><strong>Llama-2-13b:</strong> 130억 개의 파라미터를 가진 중간 크기의 Llama-2 모델입니다. 성능과 효율성 사이의 좋은 균형을 제공하며 다양한 작업에 사용할 수 있습니다.</li>
<li><strong>Llama-2-7b:</strong> 70억 개의 파라미터를 가진 가장 작은 Llama-2 모델입니다. 가장 효율적인 Llama-2 모델이며 최고 수준의 성능이 필요하지 않은 작업에 사용할 수 있습니다.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="어떤-llama-2-모델-크기를-사용해야-하나요"><strong>어떤 Llama-2 모델 크기를 사용해야 하나요?</strong><a href="#어떤-llama-2-모델-크기를-사용해야-하나요" class="hash-link" aria-label="어떤-llama-2-모델-크기를-사용해야-하나요에 대한 직접 링크" title="어떤-llama-2-모델-크기를-사용해야-하나요에 대한 직접 링크" translate="no">​</a></h3>
<p>가장 적합한 Llama-2 모델 크기는 특정 요구 사항에 따라 달라지며, 최고의 성능을 달성하기 위해 항상 가장 큰 모델이 필요한 것은 아닙니다. 적절한 Llama-2 모델 크기를 선택할 때 컴퓨팅 리소스, 응답 시간 및 비용 효율성과 같은 요소를 고려하여 요구 사항을 평가하는 것이 좋습니다. 결정은 애플리케이션의 목표와 제약 조건에 대한 포괄적인 평가를 기반으로 해야 합니다.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="trn1inf2-인스턴스에서의-추론-llama-2의-잠재력-극대화">Trn1/Inf2 인스턴스에서의 추론: Llama-2의 잠재력 극대화<a href="#trn1inf2-인스턴스에서의-추론-llama-2의-잠재력-극대화" class="hash-link" aria-label="Trn1/Inf2 인스턴스에서의 추론: Llama-2의 잠재력 극대화에 대한 직접 링크" title="Trn1/Inf2 인스턴스에서의 추론: Llama-2의 잠재력 극대화에 대한 직접 링크" translate="no">​</a></h2>
<p><strong>Llama-2</strong>는 다양한 하드웨어 플랫폼에 배포할 수 있으며, 각각 고유한 장점이 있습니다. 그러나 Llama-2의 효율성, 확장성 및 비용 효율성을 최대화하는 데 있어 <a href="https://aws.amazon.com/ec2/instance-types/inf2/" target="_blank" rel="noopener noreferrer">AWS Trn1/Inf2 인스턴스</a>가 최적의 선택입니다.</p>
<p><strong>확장성 및 가용성</strong>
Llama-2와 같은 대규모 언어 모델(<code>LLM</code>)을 배포할 때 주요 과제 중 하나는 적절한 하드웨어의 확장성과 가용성입니다. 기존 <code>GPU</code> 인스턴스는 높은 수요로 인해 부족한 경우가 많아 리소스를 효과적으로 프로비저닝하고 확장하기가 어렵습니다.
반면 <code>trn1.32xlarge</code>, <code>trn1n.32xlarge</code>, <code>inf2.24xlarge</code> 및 <code>inf2.48xlarge</code>와 같은 <code>Trn1/Inf2</code> 인스턴스는 LLM을 포함한 생성형 AI 모델의 고성능 딥러닝(DL) 훈련 및 추론을 위해 특별히 구축되었습니다. 확장성과 가용성을 모두 제공하여 리소스 병목 현상이나 지연 없이 필요에 따라 <code>Llama-2</code> 모델을 배포하고 확장할 수 있습니다.</p>
<p><strong>비용 최적화:</strong>
기존 GPU 인스턴스에서 LLM을 실행하면 GPU의 부족과 경쟁적인 가격으로 인해 비용이 많이 들 수 있습니다.
<strong>Trn1/Inf2</strong> 인스턴스는 비용 효율적인 대안을 제공합니다. AI 및 기계 학습 작업에 최적화된 전용 하드웨어를 제공함으로써 Trn1/Inf2 인스턴스를 통해 비용의 일부로 최고 수준의 성능을 달성할 수 있습니다.
이러한 비용 최적화를 통해 예산을 효율적으로 할당하여 LLM 배포를 접근 가능하고 지속 가능하게 만들 수 있습니다.</p>
<p><strong>성능 향상</strong>
Llama-2는 GPU에서 고성능 추론을 달성할 수 있지만, Neuron 가속기는 성능을 한 단계 더 끌어올립니다. Neuron 가속기는 기계 학습 워크로드를 위해 특별히 구축되어 Llama-2의 추론 속도를 크게 향상시키는 하드웨어 가속을 제공합니다. 이는 Trn1/Inf2 인스턴스에 Llama-2를 배포할 때 더 빠른 응답 시간과 개선된 사용자 경험으로 이어집니다.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="모델-사양">모델 사양<a href="#모델-사양" class="hash-link" aria-label="모델 사양에 대한 직접 링크" title="모델 사양에 대한 직접 링크" translate="no">​</a></h3>
<p>아  래 표는 다양한 크기의 Llama-2 모델, 가중치 및 배포를 위한 하드웨어 요구 사항에 대한 정보를 제공합니다. 이 정보를 사용하여 모든 크기의 Llama-2 모델을 배포하는 데 필요한 인프라를 설계할 수 있습니다. 예를 들어 <code>Llama-2-13b-chat</code> 모델을 배포하려면 총 가속기 메모리가 최소 <code>26 GB</code>인 인스턴스 유형을 사용해야 합니다.</p>
<table><thead><tr><th>모델</th><th>가중치</th><th>바이트</th><th>파라미터 크기 (10억)</th><th>총 가속기 메모리 (GB)</th><th>NeuronCore당 가속기 메모리 크기 (GB)</th><th>필요한 Neuron 코어</th><th>필요한 Neuron 가속기</th><th>인스턴스 유형</th><th>tp_degree</th></tr></thead><tbody><tr><td>Meta/Llama-2-70b</td><td>float16</td><td>2</td><td>70</td><td>140</td><td>16</td><td>9</td><td>5</td><td>inf2.48x</td><td>24</td></tr><tr><td>Meta/Llama-2-13b</td><td>float16</td><td>2</td><td>13</td><td>26</td><td>16</td><td>2</td><td>1</td><td>inf2.24x</td><td>12</td></tr><tr><td>Meta/Llama-2-7b</td><td>float16</td><td>2</td><td>7</td><td>14</td><td>16</td><td>1</td><td>1</td><td>inf2.24x</td><td>12</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="예제-사용-사례">예제 사용 사례<a href="#예제-사용-사례" class="hash-link" aria-label="예제 사용 사례에 대한 직접 링크" title="예제 사용 사례에 대한 직접 링크" translate="no">​</a></h3>
<p>회사가 고객 지원을 제공하기 위해 Llama-2 챗봇을 배포하려고 합니다. 회사는 대규모 고객 기반을 보유하고 있으며 피크 시간에 많은 양의 채팅 요청을 받을 것으로 예상합니다. 회사는 높은 요청량을 처리하고 빠른 응답 시간을 제공할 수 있는 인프라를 설계해야 합니다.</p>
<p>회사는 Inferentia2 인스턴스를 사용하여 Llama-2 챗봇을 효율적으로 확장할 수 있습니다. Inferentia2 인스턴스는 기계 학습 작업을 위한 특수  하드웨어 가속기입니다. 기계 학습 워크로드에 대해 GPU보다 최대 20배 더 나은 성능과 최대 7배 더 낮은 비용을 제공할 수 있습니다.</p>
<p>회사는 또한 Ray Serve를 사용하여 Llama-2 챗봇을 수평으로 확장할 수 있습니다. Ray Serve는 기계 학습 모델을 서빙하기 위한 분산 프레임워크입니다. 수요에 따라 모델을 자동으로 확장하거나 축소할 수 있습니다.</p>
<p>Llama-2 챗봇을 확장하기 위해 회사는 여러 Inferentia2 인스턴스를 배포하고 Ray Serve를 사용하여 인스턴스 간에 트래픽을 분산할 수 있습니다. 이를 통해 회사는 높은 요청량을 처리하고 빠른 응답 시간을 제공할 수 있습니다.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="솔루션-아키텍처">솔루션 아키텍처<a href="#솔루션-아키텍처" class="hash-link" aria-label="솔루션 아키텍처에 대한 직접 링크" title="솔루션 아키텍처에 대한 직접 링크" translate="no">​</a></h2>
<p>이 섹션에서는 Amazon EKS에서 Llama-2 모델, <a href="https://docs.ray.io/en/latest/serve/index.html" target="_blank" rel="noopener noreferrer">Ray Serve</a> 및 <a href="https://aws.amazon.com/ec2/instance-types/inf2/" target="_blank" rel="noopener noreferrer">Inferentia2</a>를 결합한 솔루션의 아키텍처를 자세히 살펴봅니다.</p>
<p><img decoding="async" loading="lazy" alt="Llama-2-inf2" src="/ai-on-eks/ko/assets/images/llama2-inf2-6145720e940706806709b8e483dd9a8f.png" width="11358" height="7442" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="솔루션-배포">솔루션 배포<a href="#솔루션-배포" class="hash-link" aria-label="솔루션 배포에 대한 직접 링크" title="솔루션 배포에 대한 직접 링크" translate="no">​</a></h2>
<p><a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener noreferrer">Amazon EKS</a>에 <code>Llama-2-13b chat</code>을 배포하려면 필요한 사전 요구 사 항을 다루고 배포 프로세스를 단계별로 안내합니다.
여기에는 인프라 설정, <strong>Ray 클러스터</strong> 배포 및 <a href="https://www.gradio.app/" target="_blank" rel="noopener noreferrer">Gradio</a> WebUI 앱 생성이 포함됩니다.</p>
<div class="collapsibleContent_q3kw"><div class="header_QCEw"><h2><span>사전 요구 사항</span></h2><span class="icon_PckA">👈</span></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llama-2-chat-모델이-있는-ray-클러스터-배포">Llama-2-Chat 모델이 있는 Ray 클러스터 배포<a href="#llama-2-chat-모델이-있는-ray-클러스터-배포" class="hash-link" aria-label="Llama-2-Chat 모델이 있는 Ray 클러스터 배포에 대한 직접 링크" title="Llama-2-Chat 모델이 있는 Ray 클러스터 배포에 대한 직접 링크" translate="no">​</a></h2>
<p><code>Trainium on EKS</code> 클러스터가 배포되면 <code>kubectl</code>을 사용하여 <code>ray-service-Llama-2.yaml</code>을 배포할 수 있습니다.</p>
<p>이 단계에서는 Karpenter 오토스케일링을 사용하는 <code>x86 CPU</code> 인스턴스의 <code>Head Pod</code> 하나와 <a href="https://karpenter.sh/" target="_blank" rel="noopener noreferrer">Karpenter</a>에 의해 오토스케일링되는 <code>Inf2.48xlarge</code> 인스턴스의 <code>Ray 워커</code>로 구성된 Ray Serve 클러스터를 배포합니다.</p>
<p>배포를 진행하기 전에 이 배포에서 사용되는 주요 파일을 자세히 살펴보고 기능을 이해해 봅시다:</p>
<ul>
<li>
<p><strong>ray_serve_Llama-2.py:</strong>
이 스크립트는 FastAPI, Ray Serve 및 PyTorch 기반 Hugging Face Transformers를 사용하여 <a href="https://huggingface.co/NousResearch/Llama-2-13b-chat-hf" target="_blank" rel="noopener noreferrer">NousResearch/Llama-2-13b-chat-hf</a> 언어 모델을 사용한 효율적인 텍스트 생성 API를 생성합니다.
또는 사용자는 <a href="https://huggingface.co/meta-llama/Llama-2-13b-chat-hf" target="_blank" rel="noopener noreferrer">meta-llama/Llama-2-13b-chat-hf</a> 모델로 유연하게 전환할 수 있습니다. 스크립트는 입력 문장을 수락하고 향상된 성능을 위한 Neuron 가속의 이점을 활용하여 텍스트 출력을 효율적으로 생성하는 엔드포인트를 설정합니다. 높은 구성 가능성으로 사용자는 챗봇 및 텍스트 생성 작업을 포함한 다양한 자연어 처리 애플리케이션에 맞게 모델 파라미터를 미세 조정할 수 있습니다.</p>
</li>
<li>
<p><strong>ray-service-Llama-2.yaml:</strong>
이 Ray Serve YAML 파일은 <code>Llama-2-13b-chat</code> 모델을 사용한 효율적인 텍스트 생성을 용이하게 하는 Ray Serve 서비스를 배포하기 위한 Kubernetes 구성 역할을 합니다.
리소스를 분리하기 위해 <code>Llama-2</code>라는 Kubernetes 네임스페이스를 정의합니다. 구성 내에서 <code>Llama-2-service</code>라는 <code>RayService</code> 사양이 생성되고 <code>Llama-2</code> 네임스페이스 내에 호스팅됩니다. <code>RayService</code> 사양은 Ray Serve 서비스를 생성하기 위해 Python 스크립트 <code>ray_serve_Llama-2.py</code> (같은 폴더 내의 Dockerfile에 복사됨)를 활용합니다.
이 예제에서 사용된 Docker 이미지는 배포 용이성을 위해 Amazon Elastic Container Registry (ECR)에 공개적으로 제공됩니다.
사용자는 특정 요구 사항에 맞게 Dockerfile을 수정하고 자체 ECR 리포지토리에 푸시하여 YAML 파일에서 참조할 수도 있습니다.</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1단계-llama-2-chat-모델-배포">1단계: Llama-2-Chat 모델 배포<a href="#1단계-llama-2-chat-모델-배포" class="hash-link" aria-label="1단계: Llama-2-Chat 모델 배포에 대한 직접 링크" title="1단계: Llama-2-Chat 모델 배포에 대한 직접 링크" translate="no">​</a></h3>
<p><strong>클러스터가 로컬에서 구성되었는지 확인</strong></p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">aws eks </span><span class="token parameter variable" style="color:#36acaa">--region</span><span class="token plain"> us-west-2 update-kubeconfig </span><span class="token parameter variable" style="color:#36acaa">--name</span><span class="token plain"> trainium-inferentia</span><br></span></code></pre></div></div>
<p><strong>RayServe 클러스터 배포</strong></p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> ai-on-eks/blueprints/inference/llama2-13b-chat-rayserve-inf2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> ray-service-llama2.yaml</span><br></span></code></pre></div></div>
<p>다음 명령을 실행하여 배포 확인</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>정보</div><div class="admonitionContent_BuS1"><p>배포 프로세스는 최대 10분이 소요될 수 있습니다. Head Pod는 2~3분 내에 준비되고, Ray Serve 워커 Pod는 Huggingface에서 이미지 검색 및 모델 배포에 최대 10분이 소요될 수 있습니다.</p></div></div>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get all </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> llama2</span><br></span></code></pre></div></div>
<p><strong>출력:</strong></p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">NAME                                            READY   STATUS    RESTARTS   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pod/llama2-raycluster-fcmtr-head-bf58d          1/1     Running   0          67m</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pod/llama2-raycluster-fcmtr-worker-inf2-lgnb2   1/1     Running   0          5m30s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                         AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">service/llama2             ClusterIP   172.20.118.243   &lt;none&gt;        10001/TCP,8000/TCP,8080/TCP,6379/TCP,8265/TCP   67m</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">service/llama2-head-svc    ClusterIP   172.20.168.94    &lt;none&gt;        8080/TCP,6379/TCP,8265/TCP,10001/TCP,8000/TCP   57m</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">service/llama2-serve-svc   ClusterIP   172.20.61.167    &lt;none&gt;        8000/TCP                                        57m</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME                                        DESIRED WORKERS   AVAILABLE WORKERS   CPUS   MEMORY        GPUS   STATUS   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">raycluster.ray.io/llama2-raycluster-fcmtr   1                 1                   184    704565270Ki   0      ready    67m</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME                       SERVICE STATUS   NUM SERVE ENDPOINTS</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">rayservice.ray.io/llama2   Running          2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre></div></div>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get ingress </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> llama2</span><br></span></code></pre></div></div>
<p><strong>출력:</strong></p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">NAME     CLASS   HOSTS   ADDRESS                                                                         PORTS   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">llama2   nginx   *       k8s-ingressn-ingressn-aca7f16a80-1223456666.elb.us-west-2.amazonaws.com   80      69m</span><br></span></code></pre></div></div>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>주의</div><div class="admonitionContent_BuS1"><p>이 블루프린트는 보안상의 이유로 내부 로드 밸런서를 배포하므로 동일한 VPC에 있지 않으면 브라우저에서 액세스할 수 없을 수 있습니다.
<a href="https://github.com/awslabs/data-on-eks/blob/5a2d1dfb39c89f3fd961beb350d6f1df07c2b31c/infra/trainium-inferentia/helm-values/ingress-nginx-values.yaml#L8" target="_blank" rel="noopener noreferrer">여기</a>의 지침에 따라 NLB를 공개로 설정하도록 블루프린트를 수정할 수 있습니다.</p><p>또는 로드 밸런서를 사용하지 않고 서비스를 테스트하기 위해 포트 포워딩을 사용할 수 있습니다.</p></div></div>
<p>이제 아래의 로드 밸런서 URL을 사용하여 Ray 대시보드에 액세스할 수 있습니다. <code>&lt;NLB_DNS_NAME&gt;</code>을 NLB 엔드포인트로 바꾸십시오:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">http://\&lt;NLB_DNS_NAME\&gt;/dashboard/#/serve</span><br></span></code></pre></div></div>
<p>공개 로드 밸런서에 액세스할 수 없는 경우 포트 포워딩을 사용하고 다음 명령으로 localhost를 사용하여 Ray 대시보드를 탐색할 수 있습니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward service/llama2 </span><span class="token number" style="color:#36acaa">8265</span><span class="token plain">:8265 </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> llama2</span><br></span></code></pre></div></div>
<p><strong>브라우저에서 링크 열기</strong>: <a href="http://localhost:8265/" target="_blank" rel="noopener noreferrer">http://localhost:8265/</a></p>
<p>이 웹페이지에서 아래 이미지와 같이 모델 배포 진행 상황을 모니터링할 수 있습니다:</p>
<p><img decoding="async" loading="lazy" alt="RayDashboard" src="/ai-on-eks/ko/assets/images/rayserve-llama2-13b-dashboard-b6e21e6ad927bc22ca05ea7dcdb7c90c.png" width="1668" height="1009" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2단계-llama-2-chat-모델-테스트">2단계: Llama-2-Chat 모델 테스트<a href="#2단계-llama-2-chat-모델-테스트" class="hash-link" aria-label="2단계: Llama-2-Chat 모델 테스트에 대한 직접 링크" title="2단계: Llama-2-Chat 모델 테스트에 대한 직접 링크" translate="no">​</a></h3>
<p>모델 배포 상태가 <code>running</code> 상태가 되면 Llama-2-chat 사용을 시작할 수 있습니다.</p>
<p><strong>포트 포워딩 사용</strong></p>
<p>먼저 포트 포워딩을 사용하여 서비스에 로컬로 액세스합니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward service/llama2-serve-svc </span><span class="token number" style="color:#36acaa">8000</span><span class="token plain">:8000 </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> llama2</span><br></span></code></pre></div></div>
<p>그런 다음 URL 끝에 쿼리를 추가하여 다음 URL로 모델을 테스트할 수 있습니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">http://localhost:8000/infer?sentence</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">what is data parallelism and tensor parallelism and the differences</span><br></span></code></pre></div></div>
<p>브라우저에서 다음과 같은 출력을 볼 수 있습니다.</p>
<p><img decoding="async" loading="lazy" alt="llama2-13b-response" src="/ai-on-eks/ko/assets/images/llama2-13b-response-9c3626eb0cbb1d683a7c1b3cfe1d0356.png" width="1511" height="426" class="img_ev3q"></p>
<p><strong>NLB 사용</strong>:</p>
<p>Network Load Balancer (NLB)를 사용하려는 경우 <a href="https://github.com/awslabs/ai-on-eks/blob/5a2d1dfb39c89f3fd961beb350d6f1df07c2b31c/infra/trainium-inferentia/helm-values/ingress-nginx-values.yaml#L8" target="_blank" rel="noopener noreferrer">여기</a>의 지침에 따라 NLB를 공개로 설정하도록 블루프린트를 수정할 수 있습니다.</p>
<p>그런 다음 URL 끝에 쿼리를 추가하여 다음 URL을 사용할 수 있습니다:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">http://\&lt;NLB_DNS_NAME\&gt;/serve/infer?sentence=what is data parallelism and tensor parallelisma and the differences</span><br></span></code></pre></div></div>
<p>브라우저에서 다음과 같은 출력을 볼 수 있습니다:</p>
<p><img decoding="async" loading="lazy" alt="Chat Output" src="/ai-on-eks/ko/assets/images/llama-2-chat-ouput-e291e1c9481febc387222da8c06c1170.png" width="2432" height="1308" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="3단계-gradio-webui-앱-배포">3단계: Gradio WebUI 앱 배포<a href="#3단계-gradio-webui-앱-배포" class="hash-link" aria-label="3단계: Gradio WebUI 앱 배포에 대한 직접 링크" title="3단계: Gradio WebUI 앱 배포에 대한 직접 링크" translate="no">​</a></h3>
<p><a href="https://www.gradio.app/" target="_blank" rel="noopener noreferrer">Gradio</a> Web UI는 inf2 인스턴스를 사용하여 EKS 클러스터에 배포된 Llama2 추론 서비스와 상호 작용하는 데 사용됩니다.
Gradio UI는 서비스 이름과 포트를 사용하여 포트 <code>8000</code>에서 노출되는 Llama2 서비스(<code>llama2-serve-svc.llama2.svc.cluster.local:8000</code>)와 내부적으로 통신합니다.</p>
<p>Gradio 앱을 위한 기본 Docker(<code>ai/inference/gradio-ui/Dockerfile-gradio-base</code>) 이미지를 생성했으며, 이는 모든 모델 추론에 사용할 수 있습니다.
이 이미지는 <a href="https://gallery.ecr.aws/data-on-eks/gradio-web-app-base" target="_blank" rel="noopener noreferrer">Public ECR</a>에 게시되어 있습니다.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="gradio-앱-배포-단계">Gradio 앱 배포 단계:<a href="#gradio-앱-배포-단계" class="hash-link" aria-label="Gradio 앱 배포 단계:에 대한 직접 링크" title="Gradio 앱 배포 단계:에 대한 직접 링크" translate="no">​</a></h4>
<p>다음 YAML 스크립트(<code>ai/inference/llama2-13b-chat-rayserve-inf2/gradio-ui.yaml</code>)는 모델 클라이언트 스크립트가 포함된 전용 네임스페이스, 배포, 서비스 및 ConfigMap을 생성합니다.</p>
<p>이를 배포하려면 다음을 실행합니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> ai-on-eks/blueprints/inference/llama2-13b-chat-rayserve-inf2/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> gradio-ui.yaml</span><br></span></code></pre></div></div>
<p><strong>확인 단계:</strong>
다음 명령을 실행하여 배포, 서비스 및 ConfigMap을 확인합니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get deployments </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> gradio-llama2-inf2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get services </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> gradio-llama2-inf2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get configmaps </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> gradio-llama2-inf2</span><br></span></code></pre></div></div>
<p><strong>서비스 포트 포워딩:</strong></p>
<p>로컬에서 Web UI에 액세스할 수 있도록 포트 포워딩 명령을 실행합니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward service/gradio-service </span><span class="token number" style="color:#36acaa">7860</span><span class="token plain">:7860 </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> gradio-llama2-inf2</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="webui-호출">WebUI 호출<a href="#webui-호출" class="hash-link" aria-label="WebUI 호출에 대한 직접 링크" title="WebUI 호출에 대한 직접 링크" translate="no">​</a></h4>
<p>웹 브라우저를 열고 다음 URL로 이동하여 Gradio WebUI에 액세스합니다:</p>
<p>로컬 URL에서 실행 중:  <a href="http://localhost:7860" target="_blank" rel="noopener noreferrer">http://localhost:7860</a></p>
<p>이제 로컬 머신에서 Gradio 애플리케이션과 상호 작용할 수 있습니다.</p>
<p><img decoding="async" loading="lazy" alt="gradio-llama2-13b-chat" src="/ai-on-eks/ko/assets/images/gradio-llama2-13b-chat-16ad500004cf98f92340e202ce36f89c.png" width="1668" height="1009" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="결론">결론<a href="#결론" class="hash-link" aria-label="결론에 대한 직접 링크" title="결론에 대한 직접 링크" translate="no">​</a></h2>
<p>결론적으로, <strong>Llama-2-13b chat</strong> 모델을 Ray Serve와 함께 EKS에 성공적으로 배포하고 Gradio를 사용하여 chatGPT 스타일의 채팅 웹 UI를 생성했습니다.
이는 자연어 처리 및 챗봇 개발에 흥미로운 가능성을 열어줍니다.</p>
<p>요약하면, Llama-2를 배포하고 확장할 때 AWS Trn1/Inf2 인스턴스는 매력적인 이점을 제공합니다.
GPU 부족과 관련된 문제를 극복하면서 대규모 언어 모델을 효율적이고 접근 가능하게 실행하는 데 필요한 확장성, 비용 최적화 및 성능 향상을 제공합니다.
챗봇, 자연어 처리 애플리케이션 또는 기타 LLM 기반 솔루션을 구축하든 Trn1/Inf2 인스턴스를 통해 AWS 클라우드에서 Llama-2의 잠재력을 최대한 활용할 수 있습니다.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="정리">정리<a href="#정리" class="hash-link" aria-label="정리에 대한 직접 링크" title="정리에 대한 직접 링크" translate="no">​</a></h2>
<p>마지막으로 더 이상 필요하지 않은 리소스를 정리하고 프로비저닝 해제하는 방법을 안내합니다.</p>
<p><strong>1단계:</strong> Gradio 앱 및 Llama2 추론 배포 삭제</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> ai-on-eks/blueprints/inference/llama2-13b-chat-rayserve-inf2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl delete </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> gradio-ui.yaml</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl delete </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> ray-service-llama2.yaml</span><br></span></code></pre></div></div>
<p><strong>2단계:</strong> EKS 클러스터 정리
이 스크립트는 <code>-target</code> 옵션을 사용하여 모든 리소스가 올바른 순서로 삭제되도록 환경을 정리합니다.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> ai-on-eks/infra/trainium-inferentia</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">./cleanup.sh</span><br></span></code></pre></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/awslabs/ai-on-eks/blob/main/website/docs/blueprints/inference/framework-guides/Neuron/llama2-inf2.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>페이지 편집</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="문서 페이지"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama3-inf2"><div class="pagination-nav__sublabel">이전</div><div class="pagination-nav__label">Inferentia2의 Llama-3-8B</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/stablediffusion-inf2"><div class="pagination-nav__sublabel">다음</div><div class="pagination-nav__label">Inferentia2의 Stable Diffusion</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#llama-2란" class="table-of-contents__link toc-highlight">Llama-2란?</a></li><li><a href="#어떤-llama-2-모델-크기를-사용해야-하나요" class="table-of-contents__link toc-highlight"><strong>어떤 Llama-2 모델 크기를 사용해야 하나요?</strong></a></li><li><a href="#trn1inf2-인스턴스에서의-추론-llama-2의-잠재력-극대화" class="table-of-contents__link toc-highlight">Trn1/Inf2 인스턴스에서의 추론: Llama-2의 잠재력 극대화</a><ul><li><a href="#모델-사양" class="table-of-contents__link toc-highlight">모델 사양</a></li><li><a href="#예제-사용-사례" class="table-of-contents__link toc-highlight">예제 사용 사례</a></li></ul></li><li><a href="#솔루션-아키텍처" class="table-of-contents__link toc-highlight">솔루션 아키텍처</a></li><li><a href="#솔루션-배포" class="table-of-contents__link toc-highlight">솔루션 배포</a><ul><li><a href="#배포" class="table-of-contents__link toc-highlight">배포</a></li><li><a href="#리소스-확인" class="table-of-contents__link toc-highlight">리소스 확인</a></li></ul></li><li><a href="#llama-2-chat-모델이-있는-ray-클러스터-배포" class="table-of-contents__link toc-highlight">Llama-2-Chat 모델이 있는 Ray 클러스터 배포</a><ul><li><a href="#1단계-llama-2-chat-모델-배포" class="table-of-contents__link toc-highlight">1단계: Llama-2-Chat 모델 배포</a></li><li><a href="#2단계-llama-2-chat-모델-테스트" class="table-of-contents__link toc-highlight">2단계: Llama-2-Chat 모델 테스트</a></li><li><a href="#3단계-gradio-webui-앱-배포" class="table-of-contents__link toc-highlight">3단계: Gradio WebUI 앱 배포</a></li></ul></li><li><a href="#결론" class="table-of-contents__link toc-highlight">결론</a></li><li><a href="#정리" class="table-of-contents__link toc-highlight">정리</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">참여하기</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/awslabs/ai-on-eks" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Built with ❤️ at AWS  <br> © ${new Date().getFullYear()} Amazon.com, Inc. or its affiliates. All Rights Reserved</div></div></div></footer><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;7fbc7ab02fae4767b1af2588eba0cdf2&quot;}"></script></div>
</body>
</html>
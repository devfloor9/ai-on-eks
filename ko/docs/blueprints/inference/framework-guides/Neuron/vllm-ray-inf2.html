<!doctype html>
<html lang="ko-KR" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-blueprints/inference/framework-guides/Neuron/vllm-ray-inf2" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.0">
<title data-rh="true">Inferentia2에서 vLLM을 사용한 Llama-3-8B | AI on EKS</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/vllm-ray-inf2"><meta data-rh="true" property="og:locale" content="ko_KR"><meta data-rh="true" property="og:locale:alternate" content="en_US"><meta data-rh="true" name="docusaurus_locale" content="ko"><meta data-rh="true" name="docsearch:language" content="ko"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Inferentia2에서 vLLM을 사용한 Llama-3-8B | AI on EKS"><meta data-rh="true" name="description" content="최적화된 추론 성능을 위해 Ray와 vLLM을 사용하여 AWS Inferentia2에서 Meta-Llama-3-8B-Instruct 모델 서빙."><meta data-rh="true" property="og:description" content="최적화된 추론 성능을 위해 Ray와 vLLM을 사용하여 AWS Inferentia2에서 Meta-Llama-3-8B-Instruct 모델 서빙."><link data-rh="true" rel="icon" href="/ai-on-eks/ko/img/header-icon.png"><link data-rh="true" rel="canonical" href="https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/vllm-ray-inf2"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/docs/blueprints/inference/framework-guides/Neuron/vllm-ray-inf2" hreflang="en-US"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/vllm-ray-inf2" hreflang="ko-KR"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/docs/blueprints/inference/framework-guides/Neuron/vllm-ray-inf2" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"EKS에서의 추론","item":"https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/inference/"},{"@type":"ListItem","position":2,"name":"Framework-Specific Deployment Guides","item":"https://awslabs.github.io/ai-on-eks/ko/docs/category/framework-specific-deployment-guides"},{"@type":"ListItem","position":3,"name":"EKS에서의 Neuron 추론","item":"https://awslabs.github.io/ai-on-eks/ko/docs/category/neuron-inference-on-eks"},{"@type":"ListItem","position":4,"name":"Inferentia2에서 vLLM을 사용한 Llama-3-8B","item":"https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/vllm-ray-inf2"}]}</script><link rel="stylesheet" href="/ai-on-eks/ko/assets/css/styles.c270b852.css">
<script src="/ai-on-eks/ko/assets/js/runtime~main.84f685ef.js" defer="defer"></script>
<script src="/ai-on-eks/ko/assets/js/main.62626cce.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><div role="region" aria-label="본문으로 건너뛰기"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">본문으로 건너뛰기</a></div><div class="theme-announcement-bar announcementBar_mb4j" style="background-color:#667eea;color:#ffffff" role="banner"><div class="announcementBarPlaceholder_vyr4"></div><div class="content_knG7 announcementBarContent_xLdY">GenAI on EKS workshop series! <a target="_blank" rel="noopener noreferrer" href="https://aws-experience.com/emea/smb/events/series/get-hands-on-with-amazon-eks?trk=9be4af2e-2339-40ae-b5e9-57b6a7704c36&sc_channel=el" style="color: #ffffff; text-decoration: underline; font-weight: bold; margin-left: 10px;">Register now →</a></div><button type="button" aria-label="닫기" class="clean-btn close closeButton_CVFx announcementBarClose_gvF7"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav aria-label="메인" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="사이드바 펼치거나 접기" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai-on-eks/ko/"><div class="navbar__logo"><img src="/ai-on-eks/ko/img/header-icon.png" alt="AIoEKS 로고" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai-on-eks/ko/img/header-icon.png" alt="AIoEKS 로고" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href="/ai-on-eks/ko/docs/infra">인프라</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/ai-on-eks/ko/docs/blueprints">블루프린트</a><a class="navbar__item navbar__link" href="/ai-on-eks/ko/docs/guidance">가이드</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/awslabs/ai-on-eks" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>한국어</a><ul class="dropdown__menu"><li><a href="/ai-on-eks/docs/blueprints/inference/framework-guides/Neuron/vllm-ray-inf2" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en-US">English</a></li><li><a href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/vllm-ray-inf2" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="ko-KR">한국어</a></li></ul></div><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="어두운 모드와 밝은 모드 전환하기 (현재 system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="맨 위로 스크롤하기" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="문서 사이드바" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai-on-eks/ko/docs/blueprints"><span title="개요" class="linkLabel_WmDU">개요</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai-on-eks/ko/docs/blueprints/inference"><span title="EKS에서의 추론" class="categoryLinkLabel_W154">EKS에서의 추론</span></a><button aria-label="사이드바 분류 &#x27;EKS에서의 추론&#x27; 접기" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai-on-eks/ko/docs/category/framework-specific-deployment-guides"><span title="Framework-Specific Deployment Guides" class="categoryLinkLabel_W154">Framework-Specific Deployment Guides</span></a><button aria-label="사이드바 분류 &#x27;Framework-Specific Deployment Guides&#x27; 접기" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai-on-eks/ko/docs/category/gpu-inference-on-eks"><span title="EKS에서의 GPU 추론" class="categoryLinkLabel_W154">EKS에서의 GPU 추론</span></a><button aria-label="사이드바 분류 &#x27;EKS에서의 GPU 추론&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai-on-eks/ko/docs/category/neuron-inference-on-eks"><span title="EKS에서의 Neuron 추론" class="categoryLinkLabel_W154">EKS에서의 Neuron 추론</span></a><button aria-label="사이드바 분류 &#x27;EKS에서의 Neuron 추론&#x27; 접기" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/vllm-ray-inf2"><span title="Inferentia2에서 vLLM을 사용한 Llama-3-8B" class="linkLabel_WmDU">Inferentia2에서 vLLM을 사용한 Llama-3-8B</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/Mistral-7b-inf2"><span title="Inferentia2의 Mistral-7B" class="linkLabel_WmDU">Inferentia2의 Mistral-7B</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama3-inf2"><span title="Inferentia2의 Llama-3-8B" class="linkLabel_WmDU">Inferentia2의 Llama-3-8B</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama2-inf2"><span title="Inferentia2의 Llama-2" class="linkLabel_WmDU">Inferentia2의 Llama-2</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/stablediffusion-inf2"><span title="Inferentia2의 Stable Diffusion" class="linkLabel_WmDU">Inferentia2의 Stable Diffusion</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/rayserve-ha"><span title="Ray Serve 고가용성" class="linkLabel_WmDU">Ray Serve 고가용성</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama4-trn2"><span title="Llama 4 with vLLM on Trainium" class="linkLabel_WmDU">Llama 4 with vLLM on Trainium</span></a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/inference-charts"><span title="추론 차트" class="linkLabel_WmDU">추론 차트</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai-on-eks/ko/docs/category/training-on-eks"><span title="EKS에서의 학습" class="categoryLinkLabel_W154">EKS에서의 학습</span></a><button aria-label="사이드바 분류 &#x27;EKS에서의 학습&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai-on-eks/ko/docs/blueprints/gateways/envoy-gateway"><span title="게이트웨이" class="categoryLinkLabel_W154">게이트웨이</span></a></div></li></ul></nav><button type="button" title="사이드바 숨기기" aria-label="사이드바 숨기기" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="탐색 경로"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="홈" class="breadcrumbs__link" href="/ai-on-eks/ko/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai-on-eks/ko/docs/blueprints/inference"><span>EKS에서의 추론</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai-on-eks/ko/docs/category/framework-specific-deployment-guides"><span>Framework-Specific Deployment Guides</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai-on-eks/ko/docs/category/neuron-inference-on-eks"><span>EKS에서의 Neuron 추론</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Inferentia2에서 vLLM을 사용한 Llama-3-8B</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">이 페이지에서</button></div><div class="theme-doc-markdown markdown"><div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>경고</div><div class="admonitionContent_BuS1"><p>EKS에 ML 모델을 배포하려면 GPU 또는 Neuron 인스턴스에 대한 액세스가 필요합니다. 배포가 작동하지 않는 경우 이러한 리소스에 대한 액세스가 누락되어 있기 때문인 경우가 많습니다. 또한 일부 배포 패턴은 Karpenter 오토스케일링 및 정적 노드 그룹에 의존합니다. 노드가 초기화되지 않으면 Karpenter 또는 노드 그룹의 로그를 확인하여 문제를 해결하십시오.</p></div></div>
<div class="theme-admonition theme-admonition-danger admonition_xJq3 alert alert--danger"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"></path></svg></span>위험</div><div class="admonitionContent_BuS1"><p>참고: 이 Llama-3 Instruct 모델의 사용은 Meta 라이선스의 적용을 받습니다.
모델 가중치와 토크나이저를 다운로드하려면 <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B" target="_blank" rel="noopener noreferrer">웹사이트</a>를 방문하여 액세스를 요청하기 전에 라이선스에 동의해 주십시오.</p></div></div>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>정보</div><div class="admonitionContent_BuS1"><p>관측성, 로깅 및 확장성 측면의 개선 사항을 포함하기 위해 이 블루프린트를 적극적으로 개선하고 있습니다.</p></div></div>
<header><h1>AWS Neuron에서 RayServe와 vLLM을 사용한 LLM 서빙</h1></header>
<p><a href="https://docs.ray.io/en/latest/serve/index.html" target="_blank" rel="noopener noreferrer">Ray Serve</a>와 AWS Neuron을 사용하여 Amazon Elastic Kubernetes Service (EKS)에 LLM을 배포하는 포괄적인 가이드에 오신 것을 환영합니다.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="aws-neuron이란">AWS Neuron이란?<a href="#aws-neuron이란" class="hash-link" aria-label="AWS Neuron이란?에 대한 직접 링크" title="AWS Neuron이란?에 대한 직접 링크" translate="no">​</a></h3>
<p>이 튜토리얼에서는 AWS Inferentia 및 Trainium 가속기에서 딥러닝 성능을 최적화하는 강력한 SDK인 <a href="https://aws.amazon.com/machine-learning/neuron/" target="_blank" rel="noopener noreferrer">AWS Neuron</a>을 활용합니다. Neuron은 PyTorch 및 TensorFlow와 같은 프레임워크와 원활하게 통합되어 Inf1, Inf2, Trn1 및 Trn1n과 같은 특수 EC2 인스턴스에서 고성능 기계 학습 모델을 개발, 프로파일링 및 배포하기 위한 포괄적인 툴킷을 제공합니다.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="vllm이란">vLLM이  란?<a href="#vllm이란" class="hash-link" aria-label="vLLM이란?에 대한 직접 링크" title="vLLM이란?에 대한 직접 링크" translate="no">​</a></h3>
<p><a href="https://docs.vllm.ai/en/latest/" target="_blank" rel="noopener noreferrer">vLLM</a>은 처리량을 극대화하고 지연 시간을 최소화하도록 설계된 LLM 추론 및 서빙을 위한 고성능 라이브러리입니다. 핵심적으로 vLLM은 GPU 리소스의 최적 활용을 가능하게 하여 메모리 효율성을 크게 개선하는 혁신적인 어텐션 알고리즘인 <a href="https://docs.vllm.ai/en/latest/dev/kernel/paged_attention.html" target="_blank" rel="noopener noreferrer">PagedAttention</a>을 활용합니다. 이 오픈 소스 솔루션은 Python API 및 OpenAI 호환 서버를 통한 원활한 통합을 제공하여 개발자가 프로덕션 환경에서 Llama 3와 같은 대규모 언어 모델을 전례 없는 효율성으로 배포하고 확장할 수 있게 합니다.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="rayserve란">RayServe란?<a href="#rayserve란" class="hash-link" aria-label="RayServe란?에 대한 직접 링크" title="RayServe란?에 대한 직접 링크" translate="no">​</a></h3>
<p>Ray Serve는 Ray 위에 구축된 확장 가능한 모델 서빙 라이브러리로, 프레임워크 불가지론적 배포, 모델 구성 및 내장 확장과 같은 기능을 갖춘 기계 학습 모델 및 AI 애플리케이션을 배포하도록 설계되었습니다. KubeRay 프로젝트의 일부인 Kubernetes 사용자 정의 리소스인 RayService도 접하게 되며, 이는 Kubernetes 클러스터에서 Ray Serve 애플리케이션을 배포하고 관리하는 데 사용됩니다.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="llama-3-8b-instruct란">Llama-3-8B Instruct란?<a href="#llama-3-8b-instruct란" class="hash-link" aria-label="Llama-3-8B Instruct란?에 대한 직접 링크" title="Llama-3-8B Instruct란?에 대한 직접 링크" translate="no">​</a></h3>
<p>Meta는 8B   및 70B 크기의 사전 훈련 및 명령어 조정 생성 텍스트 모델 컬렉션인 Meta Llama 3 대규모 언어 모델(LLM) 제품군을 개발하고 출시했습니다. Llama 3 명령어 조정 모델은 대화 사용 사례에 최적화되어 있으며 일반적인 업계 벤치마크에서 사용 가능한 많은 오픈 소스 채팅 모델을 능가합니다. 또한 이러한 모델을 개발할 때 유용성과 안전성을 최적화하는 데 세심한 주의를 기울였습니다.</p>
<p>Llama3 크기 및 모델 아키텍처에 대한 자세한 정보는 <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct" target="_blank" rel="noopener noreferrer">여기</a>에서 확인할 수 있습니다.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="왜-aws-가속기인가">왜 AWS 가속기인가?<a href="#왜-aws-가속기인가" class="hash-link" aria-label="왜 AWS 가속기인가?에 대한 직접 링크" title="왜 AWS 가속기인가?에 대한 직접 링크" translate="no">​</a></h3>
<p><strong>확장성 및 가용성</strong></p>
<p>Llama-3와 같은 대규모 언어 모델(<code>LLM</code>)을 배포할 때 주요 과제 중 하나는 적절한 하드웨어의 확장성과 가용성입니다. 기존 <code>GPU</code> 인스턴스는 높은 수요로 인해 부족한 경우가 많아 리소스를 효과적으로 프로비저닝하고 확장하기가 어렵습니다.</p>
<p>반면 <code>trn1.32xlarge</code>, <code>trn1n.32xlarge</code>, <code>inf2.24xlarge</code> 및 <code>inf2.48xlarge</code>와 같은 <code>Trn1/Inf2</code> 인스턴스는 LLM을 포함한 생성형 AI 모델의 고성능 딥러닝(DL) 훈련 및 추론을 위해 특별히 구축되었습니다. 확장성과 가용성을 모두 제공하여 리소스 병목 현상이나 지연 없이 필요에 따라 <code>Llama-3</code> 모델을 배포하고 확장할 수 있습니다.</p>
<p><strong>비용 최적화</strong></p>
<p>기존 GPU 인스턴스에서 LLM을 실행하면 GPU의 부족과 경쟁적인 가격으로 인해 비용이 많이 들 수 있습니다. <strong>Trn1/Inf2</strong> 인스턴스는 비용 효율적인 대안을 제공합니다. AI 및 기계 학습 작업에 최적화된 전용 하드웨어를 제공함으로써 Trn1/Inf2 인스턴스를 통해 비용의 일부로 최고 수준의 성능을 달성할 수 있습니다. 이러한 비용 최적화를 통해 예산을 효율적으로 할당하여 LLM 배포를 접근 가능하고 지속 가능하게 만들 수 있습니다.</p>
<p><strong>성능 향상</strong></p>
<p>Llama-3는 GPU에서 고성능 추론을 달성할 수 있지만, Neuron 가속기는 성능을 한 단계 더 끌어올립니다. Neuron 가속기는 기계 학습 워크로드를 위해 특별히 구축되어 Llama-3의 추론 속도를 크게 향상시키는 하드웨어 가속을 제공합니다. 이는 Trn1/Inf2 인스턴스에 Llama-3를 배포할 때 더 빠른 응답 시간과 개선된 사용자 경험으로 이어집니다.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="솔루션-아키텍처">솔루션 아키텍처<a href="#솔루션-아키텍처" class="hash-link" aria-label="솔루션 아키텍처에 대한 직접 링크" title="솔루션 아키텍처에 대한 직접 링크" translate="no">​</a></h2>
<p>이 섹션에서는 Amazon EKS에서 Llama-3 모델, <a href="https://docs.ray.io/en/latest/serve/index.html" target="_blank" rel="noopener noreferrer">Ray Serve</a> 및 <a href="https://aws.amazon.com/ec2/instance-types/inf2/" target="_blank" rel="noopener noreferrer">Inferentia2</a>를 결합한 솔루션의 아키텍처를 자세히 살펴봅니다.</p>
<p><img decoding="async" loading="lazy" alt="Llama-3-inf2" src="/ai-on-eks/ko/assets/images/ray-vllm-inf2-f4743ae82b32bc6cfd823bcd97a29144.png" width="1642" height="834" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="솔루션-배포">솔루션 배포<a href="#솔루션-배포" class="hash-link" aria-label="솔루션 배포에 대한 직접 링크" title="솔루션 배포에 대한 직접   링크" translate="no">​</a></h2>
<p><a href="https://aws.amazon.com/eks/" target="_blank" rel="noopener noreferrer">Amazon EKS</a>에 <code>Llama-3-8B-instruct</code>를 배포하려면 필요한 사전 요구 사항을 다루고 배포 프로세스를 단계별로 안내합니다.</p>
<p>여기에는 AWS Inferentia 인스턴스를 사용한 인프라 설정 및 <strong>Ray 클러스터</strong> 배포가 포함됩니다.</p>
<div class="collapsibleContent_q3kw"><div class="header_QCEw"><h2><span>사전 요구 사항</span></h2><span class="icon_PckA">👈</span></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llama3-모델이-있는-ray-클러스터-배포">Llama3 모델이 있는 Ray 클러스터 배포<a href="#llama3-모델이-있는-ray-클러스터-배포" class="hash-link" aria-label="Llama3 모델이 있는 Ray 클러스터 배포에 대한 직접 링크" title="Llama3 모델이 있는 Ray 클러스터 배포에 대한 직접 링크" translate="no">​</a></h2>
<p>이 튜토리얼에서는 RayCluster, RayJob 및 RayService와 같은 Ray 특정 구성에 대한 사용자 정의 리소스 정의로 Kubernetes를 확장하는 KubeRay operator를 활용합니다. operator는 이러한 리소스와 관련된 사용자 이벤트를 감시하고, Ray 클러스터를 형성하는 데 필요한 Kubernetes 아티팩트를 자동으로 생성하며, 원하는 구성이 실제 상태와 일치하도록 클러스터 상태를 지속적으로 모니터링합니다. 설정, 워커 그룹의 동적 확장 및 해제를 포함한 수명 주기 관리를 처리하여 Kubernetes에서 Ray 애플리케이션을 관리하는 복잡성을 추상화합니다.</p>
<p>각 Ray 클러스터는 헤드 노드 pod와 워커 노드 pod 모음으로 구성되며, 워크로드 요구 사항에 따라 클러스터 크기를 조정하는 선택적 오토스케일링 지원이 있습니다. KubeRay는 이기종 컴퓨팅 노드(GPU 포함) 및 동일한 Kubernetes 클러스터에서 다른 Ray 버전으로 여러 Ray 클  러스터 실행을 지원합니다. 또한 KubeRay는 AWS Inferentia 가속기와 통합되어 특수 하드웨어에서 Llama 3와 같은 대규모 언어 모델을 효율적으로 배포하여 기계 학습 추론 작업의 성능과 비용 효율성을 잠재적으로 개선할 수 있습니다.</p>
<p>필요한 모든 구성 요소와 함께 EKS 클러스터를 배포했으므로 이제 AWS 가속기에서 <code>RayServe</code> 및 <code>vLLM</code>을 사용하여 <code>NousResearch/Meta-Llama-3-8B-Instruct</code>를 배포하는 단계를 진행할 수 있습니다.</p>
<p><strong>1단계:</strong> RayService 클러스터를 배포하려면 <code>vllm-rayserve-deployment.yaml</code> 파일이 있는 디렉토리로 이동하고 터미널에서 <code>kubectl apply</code> 명령을 실행합니다.
이렇게 하면 RayService 구성이 적용되고 EKS 설정에 클러스터가 배포됩니다.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> ai-on-eks/blueprints/inference/vllm-rayserve-inf2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> vllm-rayserve-deployment.yaml</span><br></span></code></pre></div></div>
<p><strong>선택적 구성</strong></p>
<p>기본적으로 <code>inf2.8xlarge</code> 인스턴스가 프로비저닝됩니다. <code>inf2.48xlarge</code>를 사용하려면 <code>vllm-rayserve-deployment.yaml</code> 파일을 수정하여 <code>worker</code> 컨테이너 아래의 <code>resources</code> 섹션을 변경합니다.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">limits:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    cpu: </span><span class="token string" style="color:#e3116c">&quot;30&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    memory: </span><span class="token string" style="color:#e3116c">&quot;110G&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    aws.amazon.com/neuron: </span><span class="token string" style="color:#e3116c">&quot;1&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">requests:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    cpu: </span><span class="token string" style="color:#e3116c">&quot;30&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    memory: </span><span class="token string" style="color:#e3116c">&quot;110G&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    aws.amazon.com/neuron: </span><span class="token string" style="color:#e3116c">&quot;1&quot;</span><br></span></code></pre></div></div>
<p>다음으로 변경:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">limits:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    cpu: </span><span class="token string" style="color:#e3116c">&quot;90&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    memory: </span><span class="token string" style="color:#e3116c">&quot;360G&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    aws.amazon.com/neuron: </span><span class="token string" style="color:#e3116c">&quot;12&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">requests:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    cpu: </span><span class="token string" style="color:#e3116c">&quot;90&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    memory: </span><span class="token string" style="color:#e3116c">&quot;360G&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    aws.amazon.com/neuron: </span><span class="token string" style="color:#e3116c">&quot;12&quot;</span><br></span></code></pre></div></div>
<p><strong>선택 사항: 70B 모델 배포</strong>
<code>inf2.48xlarge</code>에서 llama-70B 모델을 배포하려면 <code>ai-on-eks/blueprints/inference/vllm-rayserve-inf2/vllm-rayserve-deployment-70B.yaml</code>을 참조하십시오. 이 배포는 대형 모델을 다운로드하고 Neuron 코어에서 실행하기 위해 컴파일하는 데 약 60분이 소요됩니 다.</p>
<p><strong>2단계:</strong> 다음 명령을 실행하여 배포 확인</p>
<p>배포가 성공적으로 완료되었는지 확인하려면 다음 명령을 실행합니다:</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>정보</div><div class="admonitionContent_BuS1"><p>배포 프로세스는 최대 <strong>10분</strong>이 소요될 수 있습니다. Head Pod는 5~6분 내에 준비되고, Ray Serve 워커 Pod는 Huggingface에서 이미지 검색 및 모델 배포에 최대 10분이 소요될 수 있습니다.</p></div></div>
<p>RayServe 구성에 따라 <code>x86</code> 인스턴스에서 실행되는 Ray head pod 하나와 <code>inf2</code> 인스턴스에서 실행되는 워커 pod 하나가 있습니다. RayServe YAML 파일을 수정하여 여러 레플리카를 실행할 수 있습니다. 그러나 각 추가 레플리카는 잠재적으로 새 인스턴스를 생성할 수 있음에 유의하십시오.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get pods </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> vllm</span><br></span></code></pre></div></div>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">NAME                                                      READY   STATUS    RESTARTS   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">lm-llama3-inf2-raycluster-ksh7w-worker-inf2-group-dcs5n   1/1     Running   0          2d4h</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">vllm-llama3-inf2-raycluster-ksh7w-head-4ck8f              2/2     Running   0          2d4h</span><br></span></code></pre></div></div>
<p>이 배포는 또한 여러 포트가 구성된 서비스를 구성합니다. 포트 <strong>8265</strong>는 Ray 대시보드용이고 포트 <strong>8000</strong>은 vLLM 추론 서버 엔드포인트용입니다.</p>
<p>다음 명령을 실행하여 서비스를 확인합니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get svc </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> vllm</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME                         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">S</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">                                         AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">vllm                         ClusterIP   </span><span class="token number" style="color:#36acaa">172.20</span><span class="token plain">.23.54    </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">none</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain">        </span><span class="token number" style="color:#36acaa">8080</span><span class="token plain">/TCP,6379/TCP,8265/TCP,10001/TCP,8000/TCP   2d4h</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">vllm-llama3-inf2-head-svc    ClusterIP   </span><span class="token number" style="color:#36acaa">172.20</span><span class="token plain">.18.130   </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">none</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain">        </span><span class="token number" style="color:#36acaa">6379</span><span class="token plain">/TCP,8265/TCP,10001/TCP,8000/TCP,8080/TCP   2d4h</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">vllm-llama3-inf2-serve-svc   ClusterIP   </span><span class="token number" style="color:#36acaa">172.20</span><span class="token plain">.153.10   </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">none</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain">        </span><span class="token number" style="color:#36acaa">8000</span><span class="token plain">/TCP                                        2d4h</span><br></span></code></pre></div></div>
<p>Ray 대시보드에 액세스하려면 관련 포트를 로컬 머신으로 포트 포워딩할 수 있습니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> vllm port-forward svc/vllm </span><span class="token number" style="color:#36acaa">8265</span><span class="token plain">:8265</span><br></span></code></pre></div></div>
<p>그런 다음 Ray 에코시스템 내의 작업 및 액터 배포를 표시하는 <a href="http://localhost:8265" target="_blank" rel="noopener noreferrer">http://localhost:8265</a>에서 웹 UI에 액세스할 수 있습니다.</p>
<p><img decoding="async" loading="lazy" alt="RayServe Deployment" src="/ai-on-eks/ko/assets/images/ray-dashboard-vllm-llama3-inf2-1ec06c016b0ffdf955c973406854f2f0.png" width="3456" height="1202" class="img_ev3q"></p>
<p>배포가 완료되면 Controller 및 Proxy 상태가 <code>HEALTHY</code>이고 Application 상태가 <code>RUNNING</code>이어야 합니다</p>
<p><img decoding="async" loading="lazy" alt="RayServe Deployment Logs" src="/ai-on-eks/ko/assets/images/ray-logs-vllm-llama3-inf2-5c5963506ac6563dae4bd04a4f6a16bf.png" width="3450" height="1224" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="llama3-모델-테스트">Llama3 모델 테스트<a href="#llama3-모델-테스트" class="hash-link" aria-label="Llama3 모델 테스트에 대한 직접 링크" title="Llama3 모델 테스트에 대한 직접 링크" translate="no">​</a></h3>
<p>이제 <code>Meta-Llama-3-8B-Instruct</code> 채팅 모델을 테스트할 시간입니다. Python 클라이언트 스크립트를 사용하여 RayServe 추론 엔드포인트에 프롬프트를 보내고 모델이 생성한 출력을 확인합니다.</p>
<p>먼저 kubectl을 사용하여 <code>vllm-llama3-inf2-serve-svc</code> 서비스로 포트 포워딩을 실행합니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> vllm port-forward svc/vllm-llama3-inf2-serve-svc </span><span class="token number" style="color:#36acaa">8000</span><span class="token plain">:8000</span><br></span></code></pre></div></div>
<p><code>openai-client.py</code>는 HTTP POST 메서드를 사용하여 vllm 서버를 대상으로 텍스트 완성 및 Q&amp;A를 위해 추론 엔드포인트에 프롬프트 목록을 보냅니다.</p>
<p>가상 환경에서 Python 클라이언트 애플리케이션을 실행하려면 다음 단계를 따르십시오:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> ai-on-eks/blueprints/inference/vllm-rayserve-inf2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">python3 </span><span class="token parameter variable" style="color:#36acaa">-m</span><span class="token plain"> venv .venv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token builtin class-name">source</span><span class="token plain"> .venv/bin/activate</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip3 </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> openai</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">python3 openai-client.py</span><br></span></code></pre></div></div>
<p>터미널에서 다음과 유사한 출력을 볼 수 있습니다:</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Python 클라이언트 터미널 출력을 보려면 클릭하십시오</summary><div><div class="collapsibleContent_i85q"><div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Example 1 - Simple chat completion:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Handling connection for 8000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">The capital of India is New Delhi.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Example 2 - Chat completion with different parameters:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">The twin suns of Tatooine set slowly in the horizon, casting a warm orange glow over the bustling spaceport of Anchorhead. Amidst the hustle and bustle, a young farm boy named Anakin Skywalker sat atop a dusty speeder, his eyes fixed on the horizon as he dreamed of adventure beyond the desert planet.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">As the suns dipped below the dunes, Anakin&#x27;s uncle, Owen Lars, called out to him from the doorway of their humble moisture farm. &quot;Anakin, it&#x27;s time to head back! Your aunt and I have prepared a special dinner in your honor.&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">But Anakin was torn. He had received a strange message from an unknown sender, hinting at a great destiny waiting for him. Against his uncle&#x27;s warnings, Anakin decided to investigate further, sneaking away into the night to follow the mysterious clues.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">As he rode his speeder through the desert, the darkness seemed to grow thicker, and the silence was broken only by the distant</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Example 3 - Streaming chat completion:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">I&#x27;d be happy to help you with that. Here we go:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">1...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(Pause)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(Pause)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(Pause)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">4...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(Pause)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">5...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(Pause)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">6...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(Pause)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">7...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(Pause)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">8...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(Pause)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">9...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(Pause)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">10!</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Let me know if you have any other requests!</span><br></span></code></pre></div></div></div></div></details>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="관측성">관측성<a href="#관측성" class="hash-link" aria-label="관측성에 대한 직접 링크" title="관측성에 대한 직접 링크" translate="no">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="aws-cloudwatch-및-neuron-monitor를-통한-관측성">AWS CloudWatch 및 Neuron Monitor를 통한 관측성<a href="#aws-cloudwatch-및-neuron-monitor를-통한-관측성" class="hash-link" aria-label="AWS CloudWatch 및 Neuron Monitor를 통한 관측성에 대한 직접 링크" title="AWS CloudWatch 및 Neuron Monitor를 통한 관측성에 대한 직접 링크" translate="no">​</a></h3>
<p>이 블루프린트는 CloudWatch Observability Agent를 관리형 애드온으로 배포하여 컨테이너화된 워크로드에 대한 포괄적인 모니터링을 제공합니다. CPU 및 메모리 사용률과 같은 주요 성능 메트릭을 추적하기 위한 container insights가 포함됩니다. 또한 애드온은 <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/neuron-sys-tools/neuron-monitor-user-guide.html#neuron-monitor-user-guide" target="_blank" rel="noopener noreferrer">Neuron Monitor 플러그인</a>을 활용하여 Neuron 특정 메트릭을 캡처하고 보고합니다.</p>
<p>container insights 및 Neuron Core 사용률, NeuronCore 메모리 사용량과 같은 Neuron 메트릭을 포함한 모든 메트릭은 Amazon CloudWatch로 전송되어 실시간으로 모니터링하고 분석할 수 있습니다. 배포가 완료되면 CloudWatch 콘솔에서 직접 이러한 메트릭에 액세스하여 워크로드를 효과적으로 관리하고 최적화할 수 있습니다.</p>
<p><img decoding="async" loading="lazy" alt="CloudWatch-neuron-monitor" src="/ai-on-eks/ko/assets/images/neuron-monitor-cwci-4ed152729b7e072f5b1d2f79c25d75b3.png" width="3070" height="1916" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="open-webui-배포">Open WebUI 배포<a href="#open-webui-배포" class="hash-link" aria-label="Open WebUI 배포에 대한 직접 링크" title="Open WebUI 배포에 대한 직접 링크" translate="no">​</a></h2>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>정보</div><div class="admonitionContent_BuS1"><p><a href="https://github.com/open-webui/open-webui" target="_blank" rel="noopener noreferrer">Open WebUI</a>는 OpenAI API 서버 및 Ollama와 호환되는 모델에서만 작동합니다.</p></div></div>
<p><strong>1. WebUI 배포</strong></p>
<p>다음 명령을 실행하여 <a href="https://github.com/open-webui/open-webui" target="_blank" rel="noopener noreferrer">Open WebUI</a>를 배포합니다:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> openai-webui-deployment.yaml</span><br></span></code></pre></div></div>
<p><strong>2. WebUI 접근을 위한 포트 포워딩</strong></p>
<p><strong>참고</strong> Python 클라이언트로 추론을 테스트하기 위해 이미 포트 포워딩을 실행 중인 경우 <code>ctrl+c</code>를 눌러 중단합니다.</p>
<p>kubectl 포트 포워딩을 사용하여 로컬에서 WebUI에 접근합니다:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward svc/open-webui </span><span class="token number" style="color:#36acaa">8081</span><span class="token plain">:80 </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> openai-webui</span><br></span></code></pre></div></div>
<p><strong>3. WebUI 접근</strong></p>
<p>브라우저를 열고 <a href="http://localhost:8081" target="_blank" rel="noopener noreferrer">http://localhost:8081</a> 로 이동합니다</p>
<p><strong>4. 가입</strong></p>
<p>이름, 이메일 및 임의의 비밀번호를 사용하여 가입합니다.</p>
<p><strong>5. 새 채팅 시작</strong></p>
<p>아래 스크린샷과 같이 드롭다운 메뉴에서 모델을 선택하고 New Chat을 클릭합니다:</p>
<p><img decoding="async" loading="lazy" alt="alt text" src="/ai-on-eks/ko/assets/images/openweb-ui-ray-vllm-inf2-1-ca4458ab24555d56156a7a393d81b5eb.png" width="2848" height="1442" class="img_ev3q"></p>
<p><strong>6. 테스트 프롬프트 입력</strong></p>
<p>프롬프트를 입력하면 아래와 같이 스트리밍 결과를 볼 수 있습니다:</p>
<p><img decoding="async" loading="lazy" alt="alt text" src="/ai-on-eks/ko/assets/images/openweb-ui-ray-vllm-inf2-2-bd168c90b7849105772e8332dcad5f86.png" width="1436" height="611" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="llmperf-도구를-사용한-성능-벤치마킹">LLMPerf 도구를 사용한 성능 벤치마킹<a href="#llmperf-도구를-사용한-성능-벤치마킹" class="hash-link" aria-label="LLMPerf 도구를 사용한 성능 벤치마킹에 대한 직접 링크" title="LLMPerf 도구를 사용한 성능 벤치마킹에 대한 직접 링크" translate="no">​</a></h2>
<p><a href="https://github.com/ray-project/llmperf/blob/main/README.md" target="_blank" rel="noopener noreferrer">LLMPerf</a>는 대규모 언어 모델(LLM)의 성능을 벤치마킹하기 위해 설계된 오픈 소스 도구입니다.</p>
<p>LLMPerf 도구는 위에서 <code>kubectl -n vllm port-forward svc/vllm-llama3-inf2-serve-svc 8000:8000</code> 명령을 사용하여 설정한 포트 포워딩을 통해 포트 8000을 통해 vllm 서비스에 연결합니다.</p>
<p>터미널에서 아래 명령을 실행합니다.</p>
<p>LLMPerf 저장소 클론:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">git</span><span class="token plain"> clone https://github.com/ray-project/llmperf.git</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token builtin class-name">cd</span><span class="token plain"> llmperf</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">-e</span><span class="token plain"> </span><span class="token builtin class-name">.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> pandas</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> ray</span><br></span></code></pre></div></div>
<p>아래 명령을 사용하여 <code>vllm_benchmark.sh</code> 파일을 생성합니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">cat</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">&lt;&lt;</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;EOF&#x27;</span><span class="token string bash punctuation" style="color:#393A34"> </span><span class="token string bash punctuation operator" style="color:#393A34">&gt;</span><span class="token string bash punctuation" style="color:#393A34"> vllm_benchmark.sh</span><span class="token string" style="color:#e3116c"></span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">#!/bin/bash</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">model=${1:-NousResearch/Meta-Llama-3-8B-Instruct}</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">vu=${2:-1}</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">export OPENAI_API_KEY=EMPTY</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">export OPENAI_API_BASE=&quot;http://localhost:8000/v1&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">export TOKENIZERS_PARALLELISM=true</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">#if you have more vllm servers, append the below line to the above</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">#;http://localhost:8001/v1;http://localhost:8002/v1&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">max_requests=$(expr ${vu} \* 8 )</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">date_str=$(date &#x27;+%Y-%m-%d-%H-%M-%S&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">python ./token_benchmark_ray.py \</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">       --model ${model} \</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">       --mean-input-tokens 512 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">       --stddev-input-tokens 20 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">       --mean-output-tokens 245 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">       --stddev-output-tokens 20 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">       --max-num-completed-requests ${max_requests} \</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">       --timeout 7200 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">       --num-concurrent-requests ${vu} \</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">       --results-dir &quot;vllm_bench_results/${date_str}&quot; \</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">       --llm-api openai \</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">       --additional-sampling-params &#x27;{}&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">EOF</span><br></span></code></pre></div></div>
<p><code>--mean-input-tokens</code>: 입력 프롬프트의 평균 토큰 수 지정</p>
<p><code>--stddev-input-tokens</code>: 더 현실적인 테스트 환경을 만들기 위한 입력 토큰 길이의 변동성 지정</p>
<p><code>--mean-output-tokens</code>: 현실적인 응답 길이를 시뮬레이션하기 위해 모델 출력에서 예상되는 평균 토큰 수 지정</p>
<p><code>--stddev-output-tokens</code>: 응답 크기의 다양성을 도입하는 출력 토큰 길이의 변동성 지정</p>
<p><code>--max-num-completed-requests</code>: 처리할 최대 요청 수 설정</p>
<p><code>--num-concurrent-requests</code>: 병렬 워크로드를 시뮬레이션하기 위한 동시 요청 수 지정</p>
<p>아래 명령은 지정된 모델 <code>NousResearch/Meta-Llama-3-8B-Instruct</code>로 벤치마킹 스크립트를 실행하고 가상 사용자 수를 2로 설정합니다. 이 결과 벤치마크는 2개의 동시 요청으로 모델 성능을 테스트하고 처리할 최대 16개의 요청을 계산합니다.</p>
<p>아래 명령 실행:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct </span><span class="token number" style="color:#36acaa">2</span><br></span></code></pre></div></div>
<p>다음과 유사한 출력이 표시됩니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct </span><span class="token number" style="color:#36acaa">2</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">None of PyTorch, TensorFlow </span><span class="token operator" style="color:#393A34">&gt;=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2.0</span><span class="token plain">, or Flax have been found. Models won</span><span class="token string" style="color:#e3116c">&#x27;t be available and only tokenizers, configuration and file/data utilities can be used.</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">You are using the default legacy behaviour of the &lt;class &#x27;</span><span class="token plain">transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast&#x27;</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain">. This is expected, and simply means that the </span><span class="token variable" style="color:#36acaa">`</span><span class="token variable" style="color:#36acaa">legacy</span><span class="token variable" style="color:#36acaa">`</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">previous</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> behavior will be used so nothing changes </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> you. If you want to use the new behaviour, </span><span class="token builtin class-name">set</span><span class="token plain"> </span><span class="token variable" style="color:#36acaa">`</span><span class="token variable assign-left variable" style="color:#36acaa">legacy</span><span class="token variable operator" style="color:#393A34">=</span><span class="token variable" style="color:#36acaa">False</span><span class="token variable" style="color:#36acaa">`</span><span class="token builtin class-name">.</span><span class="token plain"> This should only be </span><span class="token builtin class-name">set</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> you understand what it means, and thoroughly </span><span class="token builtin class-name">read</span><span class="token plain"> the reason why this was added as explained </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> https://github.com/huggingface/transformers/pull/24565 - </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> you loaded a llama tokenizer from a GGUF </span><span class="token function" style="color:#d73a49">file</span><span class="token plain"> you can ignore this message.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token number" style="color:#36acaa">2024</span><span class="token plain">-09-03 09:54:45,976	INFO worker.py:1783 -- Started a </span><span class="token builtin class-name">local</span><span class="token plain"> Ray instance.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token number" style="color:#36acaa">0</span><span class="token plain">%</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">                                                                                                                                                                                                                                                    </span><span class="token operator" style="color:#393A34">|</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token plain">/16 </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">00:0</span><span class="token operator file-descriptor important" style="color:#393A34">0</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">?, ?it/s</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain">Handling connection </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">8000</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Handling connection </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">8000</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> </span><span class="token number" style="color:#36acaa">12</span><span class="token plain">%</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">█████████████████████████████▌                                                                                                                                                                                                              </span><span class="token operator" style="color:#393A34">|</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2</span><span class="token plain">/16 </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">00:1</span><span class="token operator file-descriptor important" style="color:#393A34">7</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">02:00,  </span><span class="token number" style="color:#36acaa">8</span><span class="token plain">.58s/it</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain">Handling connection </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">8000</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">..</span><span class="token plain">.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token number" style="color:#36acaa">100</span><span class="token plain">%</span><span class="token operator" style="color:#393A34">|</span><span class="token plain">███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████</span><span class="token operator" style="color:#393A34">|</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">16</span><span class="token plain">/16 </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">02:0</span><span class="token operator file-descriptor important" style="color:#393A34">1</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">00:00,  </span><span class="token number" style="color:#36acaa">7</span><span class="token plain">.58s/it</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain">Results </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> token benchmark </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> NousResearch/Meta-Llama-3-8B-Instruct queried with the openai api.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">inter_token_latency_s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p25 </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.051964785839225695</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p50 </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.053331799814278796</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">..</span><span class="token plain">.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mean </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0.053548951905597324</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">..</span><span class="token plain">.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ttft_s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p25 </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1.5284210312238429</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p50 </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1.7579061459982768</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">..</span><span class="token plain">.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mean </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1.5821313202395686</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">..</span><span class="token plain">.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">end_to_end_latency_s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p25 </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">13.74749460403109</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p50 </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">14.441407957987394</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">..</span><span class="token plain">.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mean </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">14.528114874927269</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">..</span><span class="token plain">.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">request_output_throughput_token_per_s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p25 </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">18.111220396798153</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    p50 </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">18.703139371912407</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">..</span><span class="token plain">.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mean </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">18.682678715983627</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">..</span><span class="token plain">.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Number Of Errored Requests: </span><span class="token number" style="color:#36acaa">0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Overall Output Throughput: </span><span class="token number" style="color:#36acaa">35.827933968528434</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Number Of Completed Requests: </span><span class="token number" style="color:#36acaa">16</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Completed Requests Per Minute: </span><span class="token number" style="color:#36acaa">7.914131755588426</span><br></span></code></pre></div></div>
<p>동시 요청 수를 늘려가며 벤치마킹 결과를 생성하여 동시 요청 수 증가에 따라 성능이 어떻게 변하는지 이해할 수 있습니다:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 4</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 8</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 16</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">.</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="성능-벤치마킹-메트릭">성능 벤치마킹 메트릭<a href="#성능-벤치마킹-메트릭" class="hash-link" aria-label="성능 벤치마킹 메트릭에 대한 직접 링크" title="성능 벤치마킹 메트릭에 대한 직접 링크" translate="no">​</a></h3>
<p><code>llmperf</code> 디렉토리의 <code>vllm_bench_results</code> 디렉토리에서 벤치마킹 스크립트의 결과를 찾을 수 있습니다. 결과는 날짜-시간 명명 규칙을 따르는 폴더에 저장됩니다. 벤치마킹 스크립트가 실행될 때마다 새 폴더가 생성됩니다.</p>
<p>벤치마킹 스크립트의 모든 실행 결과는 아래 형식의 2개 파일로 구성됩니다:</p>
<p><code>NousResearch-Meta-Llama-3-8B-Instruct_512_245_summary_32.json</code> - 모든 요청/응답 쌍에 걸친 성능 메트릭 요약 포함.</p>
<p><code>NousResearch-Meta-Llama-3-8B-Instruct_512_245_individual_responses.json</code> - 각 요청/응답 쌍에 대한 성능 메트릭 포함.</p>
<p>이러한 각 파일에는 다음 성능 벤치마킹 메트릭이 포함됩니다:</p>
<p><code>results_inter_token_latency_s_*</code>: Token generation latency (TPOT)라고도 합니다. Inter-Token 지연 시간은 디코딩 또는 생성 단계에서 대규모 언어 모델(LLM)이 연속 출력 토큰을 생성하는 데 걸리는 평균 시간을 나타냅니다</p>
<p><code>results_ttft_s_*</code>: 첫 번째 토큰 생성까지의 시간(Time to First Token, TTFT)</p>
<p><code>results_end_to_end_s_*</code>: 엔드투엔드 지연 시간 - 사용자가 입력 프롬프트를 제출한 시점부터 LLM이 완전한 출력 응답을 생성하기까지 걸리는 총 시간</p>
<p><code>results_request_output_throughput_token_per_s_*</code>: 모든 사용자 요청 또는 쿼리에서 대규모 언어 모델(LLM)이 초당 생성하는 출력 토큰 수</p>
<p><code>results_number_input_tokens_*</code>: 요청의 입력 토큰 수(입력 길이)</p>
<p><code>results_number_output_tokens_*</code>: 요청의 출력 토큰 수(출력 길이)</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="결론">결론<a href="#결론" class="hash-link" aria-label="결론에 대한 직접 링크" title="결론에 대한 직접 링크" translate="no">​</a></h2>
<p>요약하면, Llama-3를 배포하고 확장할 때 AWS Trn1/Inf2 인스턴스는 매력적인 이점을 제공합니다.
GPU 부족과 관련된 문제를 극복하면서 대규모 언어 모델을 효율적이고 접근 가능하게 실행하는 데 필요한 확장성, 비용 최적화 및 성능 향상을 제공합니다. 챗봇, 자연어 처리 애플리케이션 또는 기타 LLM 기반 솔루션을 구축하든 Trn1/Inf2 인스턴스를 통해 AWS 클라우드에서 Llama-3의 잠재력을 최대한 활용할 수 있습니다.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="정리">정리<a href="#정리" class="hash-link" aria-label="정리에 대한 직접 링크" title="정리에 대한 직접 링크" translate="no">​</a></h2>
<p>마지막으로 더 이상 필요하지 않은 리소스를 정리하고 프로비저닝 해제하는 방법을 안내합니다.</p>
<p>RayCluster 삭제</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> ai-on-eks/blueprints/inference/vllm-rayserve-inf2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl delete </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> vllm-rayserve-deployment.yaml</span><br></span></code></pre></div></div>
<p>EKS 클러스터 및 리소스 삭제</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> ai-on-eks/infra/trainium-inferentia/terraform/_LOCAL</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">./cleanup.sh</span><br></span></code></pre></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/awslabs/ai-on-eks/blob/main/website/docs/blueprints/inference/framework-guides/Neuron/vllm-ray-inf2.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>페이지 편집</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="문서 페이지"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai-on-eks/ko/docs/category/neuron-inference-on-eks"><div class="pagination-nav__sublabel">이전</div><div class="pagination-nav__label">Neuron Inference on EKS</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/Mistral-7b-inf2"><div class="pagination-nav__sublabel">다음</div><div class="pagination-nav__label">Inferentia2의 Mistral-7B</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#aws-neuron이란" class="table-of-contents__link toc-highlight">AWS Neuron이란?</a></li><li><a href="#vllm이란" class="table-of-contents__link toc-highlight">vLLM이란?</a></li><li><a href="#rayserve란" class="table-of-contents__link toc-highlight">RayServe란?</a></li><li><a href="#llama-3-8b-instruct란" class="table-of-contents__link toc-highlight">Llama-3-8B Instruct란?</a></li><li><a href="#왜-aws-가속기인가" class="table-of-contents__link toc-highlight">왜 AWS 가속기인가?</a></li><li><a href="#솔루션-아키텍처" class="table-of-contents__link toc-highlight">솔루션 아키텍처</a></li><li><a href="#솔루션-배포" class="table-of-contents__link toc-highlight">솔루션 배포</a><ul><li><a href="#배포" class="table-of-contents__link toc-highlight">배포</a></li><li><a href="#리소스-확인" class="table-of-contents__link toc-highlight">리소스 확인</a></li><li><a href="#neuron-플러그  인-확인" class="table-of-contents__link toc-highlight">Neuron 플러그인 확인</a></li><li><a href="#neuron-스케줄러-확인" class="table-of-contents__link toc-highlight">Neuron 스케줄러 확인</a></li></ul></li><li><a href="#llama3-모델이-있는-ray-클러스터-배포" class="table-of-contents__link toc-highlight">Llama3 모델이 있는 Ray 클러스터 배포</a><ul><li><a href="#llama3-모델-테스트" class="table-of-contents__link toc-highlight">Llama3 모델 테스트</a></li></ul></li><li><a href="#관측성" class="table-of-contents__link toc-highlight">관측성</a><ul><li><a href="#aws-cloudwatch-및-neuron-monitor를-통한-관측성" class="table-of-contents__link toc-highlight">AWS CloudWatch 및 Neuron Monitor를 통한 관측성</a></li></ul></li><li><a href="#open-webui-배포" class="table-of-contents__link toc-highlight">Open WebUI 배포</a></li><li><a href="#llmperf-도구를-사용한-성능-벤치마킹" class="table-of-contents__link toc-highlight">LLMPerf 도구를 사용한 성능 벤치마킹</a><ul><li><a href="#성능-벤치마킹-메트릭" class="table-of-contents__link toc-highlight">성능 벤치마킹 메트릭</a></li></ul></li><li><a href="#결론" class="table-of-contents__link toc-highlight">결론</a></li><li><a href="#정리" class="table-of-contents__link toc-highlight">정리</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">참여하기</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/awslabs/ai-on-eks" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Built with ❤️ at AWS  <br> © ${new Date().getFullYear()} Amazon.com, Inc. or its affiliates. All Rights Reserved</div></div></div></footer><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;7fbc7ab02fae4767b1af2588eba0cdf2&quot;}"></script></div>
</body>
</html>
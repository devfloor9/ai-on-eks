<!doctype html>
<html lang="ko-KR" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-blueprints/inference/framework-guides/Neuron/llama4-trn2" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.0">
<title data-rh="true">Llama 4 with vLLM on Trainium | AI on EKS</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama4-trn2"><meta data-rh="true" property="og:locale" content="ko_KR"><meta data-rh="true" property="og:locale:alternate" content="en_US"><meta data-rh="true" name="docusaurus_locale" content="ko"><meta data-rh="true" name="docsearch:language" content="ko"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Llama 4 with vLLM on Trainium | AI on EKS"><meta data-rh="true" name="description" content="Deploy Llama 4 models using vLLM on AWS Trainium instances with EKS and Karpenter."><meta data-rh="true" property="og:description" content="Deploy Llama 4 models using vLLM on AWS Trainium instances with EKS and Karpenter."><link data-rh="true" rel="icon" href="/ai-on-eks/ko/img/header-icon.png"><link data-rh="true" rel="canonical" href="https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama4-trn2"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/docs/blueprints/inference/framework-guides/Neuron/llama4-trn2" hreflang="en-US"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama4-trn2" hreflang="ko-KR"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/docs/blueprints/inference/framework-guides/Neuron/llama4-trn2" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"EKSì—ì„œì˜ ì¶”ë¡ ","item":"https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/inference/"},{"@type":"ListItem","position":2,"name":"Framework-Specific Deployment Guides","item":"https://awslabs.github.io/ai-on-eks/ko/docs/category/framework-specific-deployment-guides"},{"@type":"ListItem","position":3,"name":"EKSì—ì„œì˜ Neuron ì¶”ë¡ ","item":"https://awslabs.github.io/ai-on-eks/ko/docs/category/neuron-inference-on-eks"},{"@type":"ListItem","position":4,"name":"Llama 4 with vLLM on Trainium","item":"https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama4-trn2"}]}</script><link rel="stylesheet" href="/ai-on-eks/ko/assets/css/styles.c270b852.css">
<script src="/ai-on-eks/ko/assets/js/runtime~main.84f685ef.js" defer="defer"></script>
<script src="/ai-on-eks/ko/assets/js/main.62626cce.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><div role="region" aria-label="ë³¸ë¬¸ìœ¼ë¡œ ê±´ë„ˆë›°ê¸°"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">ë³¸ë¬¸ìœ¼ë¡œ ê±´ë„ˆë›°ê¸°</a></div><div class="theme-announcement-bar announcementBar_mb4j" style="background-color:#667eea;color:#ffffff" role="banner"><div class="announcementBarPlaceholder_vyr4"></div><div class="content_knG7 announcementBarContent_xLdY">GenAI on EKS workshop series! <a target="_blank" rel="noopener noreferrer" href="https://aws-experience.com/emea/smb/events/series/get-hands-on-with-amazon-eks?trk=9be4af2e-2339-40ae-b5e9-57b6a7704c36&sc_channel=el" style="color: #ffffff; text-decoration: underline; font-weight: bold; margin-left: 10px;">Register now â†’</a></div><button type="button" aria-label="ë‹«ê¸°" class="clean-btn close closeButton_CVFx announcementBarClose_gvF7"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav aria-label="ë©”ì¸" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="ì‚¬ì´ë“œë°” í¼ì¹˜ê±°ë‚˜ ì ‘ê¸°" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai-on-eks/ko/"><div class="navbar__logo"><img src="/ai-on-eks/ko/img/header-icon.png" alt="AIoEKS ë¡œê³ " class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai-on-eks/ko/img/header-icon.png" alt="AIoEKS ë¡œê³ " class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href="/ai-on-eks/ko/docs/infra">ì¸í”„ë¼</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/ai-on-eks/ko/docs/blueprints">ë¸”ë£¨í”„ë¦°íŠ¸</a><a class="navbar__item navbar__link" href="/ai-on-eks/ko/docs/guidance">ê°€ì´ë“œ</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/awslabs/ai-on-eks" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>í•œêµ­ì–´</a><ul class="dropdown__menu"><li><a href="/ai-on-eks/docs/blueprints/inference/framework-guides/Neuron/llama4-trn2" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en-US">English</a></li><li><a href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama4-trn2" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="ko-KR">í•œêµ­ì–´</a></li></ul></div><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="ì–´ë‘ìš´ ëª¨ë“œì™€ ë°ì€ ëª¨ë“œ ì „í™˜í•˜ê¸° (í˜„ì¬ system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="ë§¨ ìœ„ë¡œ ìŠ¤í¬ë¡¤í•˜ê¸°" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="ë¬¸ì„œ ì‚¬ì´ë“œë°”" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai-on-eks/ko/docs/blueprints"><span title="ê°œìš”" class="linkLabel_WmDU">ê°œìš”</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai-on-eks/ko/docs/blueprints/inference"><span title="EKSì—ì„œì˜ ì¶”ë¡ " class="categoryLinkLabel_W154">EKSì—ì„œì˜ ì¶”ë¡ </span></a><button aria-label="ì‚¬ì´ë“œë°” ë¶„ë¥˜ &#x27;EKSì—ì„œì˜ ì¶”ë¡ &#x27; ì ‘ê¸°" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai-on-eks/ko/docs/category/framework-specific-deployment-guides"><span title="Framework-Specific Deployment Guides" class="categoryLinkLabel_W154">Framework-Specific Deployment Guides</span></a><button aria-label="ì‚¬ì´ë“œë°” ë¶„ë¥˜ &#x27;Framework-Specific Deployment Guides&#x27; ì ‘ê¸°" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" tabindex="0" href="/ai-on-eks/ko/docs/category/gpu-inference-on-eks"><span title="EKSì—ì„œì˜ GPU ì¶”ë¡ " class="categoryLinkLabel_W154">EKSì—ì„œì˜ GPU ì¶”ë¡ </span></a><button aria-label="ì‚¬ì´ë“œë°” ë¶„ë¥˜ &#x27;EKSì—ì„œì˜ GPU ì¶”ë¡ &#x27; í¼ì¹˜ê¸°" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai-on-eks/ko/docs/category/neuron-inference-on-eks"><span title="EKSì—ì„œì˜ Neuron ì¶”ë¡ " class="categoryLinkLabel_W154">EKSì—ì„œì˜ Neuron ì¶”ë¡ </span></a><button aria-label="ì‚¬ì´ë“œë°” ë¶„ë¥˜ &#x27;EKSì—ì„œì˜ Neuron ì¶”ë¡ &#x27; ì ‘ê¸°" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/vllm-ray-inf2"><span title="Inferentia2ì—ì„œ vLLMì„ ì‚¬ìš©í•œ Llama-3-8B" class="linkLabel_WmDU">Inferentia2ì—ì„œ vLLMì„ ì‚¬ìš©í•œ Llama-3-8B</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/Mistral-7b-inf2"><span title="Inferentia2ì˜ Mistral-7B" class="linkLabel_WmDU">Inferentia2ì˜ Mistral-7B</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama3-inf2"><span title="Inferentia2ì˜ Llama-3-8B" class="linkLabel_WmDU">Inferentia2ì˜ Llama-3-8B</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama2-inf2"><span title="Inferentia2ì˜ Llama-2" class="linkLabel_WmDU">Inferentia2ì˜ Llama-2</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/stablediffusion-inf2"><span title="Inferentia2ì˜ Stable Diffusion" class="linkLabel_WmDU">Inferentia2ì˜ Stable Diffusion</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/rayserve-ha"><span title="Ray Serve ê³ ê°€ìš©ì„±" class="linkLabel_WmDU">Ray Serve ê³ ê°€ìš©ì„±</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/llama4-trn2"><span title="Llama 4 with vLLM on Trainium" class="linkLabel_WmDU">Llama 4 with vLLM on Trainium</span></a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/inference/inference-charts"><span title="ì¶”ë¡  ì°¨íŠ¸" class="linkLabel_WmDU">ì¶”ë¡  ì°¨íŠ¸</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai-on-eks/ko/docs/category/training-on-eks"><span title="EKSì—ì„œì˜ í•™ìŠµ" class="categoryLinkLabel_W154">EKSì—ì„œì˜ í•™ìŠµ</span></a><button aria-label="ì‚¬ì´ë“œë°” ë¶„ë¥˜ &#x27;EKSì—ì„œì˜ í•™ìŠµ&#x27; í¼ì¹˜ê¸°" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai-on-eks/ko/docs/blueprints/gateways/envoy-gateway"><span title="ê²Œì´íŠ¸ì›¨ì´" class="categoryLinkLabel_W154">ê²Œì´íŠ¸ì›¨ì´</span></a></div></li></ul></nav><button type="button" title="ì‚¬ì´ë“œë°” ìˆ¨ê¸°ê¸°" aria-label="ì‚¬ì´ë“œë°” ìˆ¨ê¸°ê¸°" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="íƒìƒ‰ ê²½ë¡œ"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="í™ˆ" class="breadcrumbs__link" href="/ai-on-eks/ko/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai-on-eks/ko/docs/blueprints/inference"><span>EKSì—ì„œì˜ ì¶”ë¡ </span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai-on-eks/ko/docs/category/framework-specific-deployment-guides"><span>Framework-Specific Deployment Guides</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai-on-eks/ko/docs/category/neuron-inference-on-eks"><span>EKSì—ì„œì˜ Neuron ì¶”ë¡ </span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Llama 4 with vLLM on Trainium</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">ì´ í˜ì´ì§€ì—ì„œ</button></div><div class="theme-doc-markdown markdown"><div class="theme-admonition theme-admonition-danger admonition_xJq3 alert alert--danger"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"></path></svg></span>ìœ„í—˜</div><div class="admonitionContent_BuS1"><p>Use of Llama 4 models is governed by the <a href="https://www.llama.com/llama4/license/" target="_blank" rel="noopener noreferrer">Meta Llama License</a>.
Please visit <a href="https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct" target="_blank" rel="noopener noreferrer">Hugging Face</a> and accept the license before requesting access.</p></div></div>
<header><h1>Llama 4 Inference with vLLM on AWS Trainium</h1></header>
<p>This guide covers deploying <a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/" target="_blank" rel="noopener noreferrer">Llama 4</a> models using <a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">vLLM</a> with <a href="https://huggingface.co/docs/optimum-neuron/index" target="_blank" rel="noopener noreferrer">optimum-neuron</a> on AWS Trainium instances.</p>
<div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>Model Compilation Required</div><div class="admonitionContent_BuS1"><p>Llama 4 inference on Neuron is supported via <strong>optimum-neuron &gt;= 0.4.0</strong> with the <code>Llama4NeuronModelForCausalLM</code> class. However, the first deployment requires <strong>Neuron model compilation</strong>, which happens automatically when <code>vllm serve</code> runs but can take <strong>30-60+ minutes</strong>. Pre-compiled artifacts may not yet be available in the <a href="https://huggingface.co/aws-neuron/optimum-neuron-cache" target="_blank" rel="noopener noreferrer">optimum-neuron-cache</a> for all configurations.</p><p>The <code>optimum-cli export neuron</code> command does <strong>not</strong> support Llama 4. Use <code>vllm serve</code> directly, which invokes the inference-path compilation internally.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-trainium-for-llama-4">Why Trainium for Llama 4?<a href="#why-trainium-for-llama-4" class="hash-link" aria-label="Why Trainium for Llama 4?ì— ëŒ€í•œ ì§ì ‘ ë§í¬" title="Why Trainium for Llama 4?ì— ëŒ€í•œ ì§ì ‘ ë§í¬" translate="no">â€‹</a></h2>
<p>AWS Trainium provides large HBM memory capacity, making it an excellent choice for large MoE models like Llama 4:</p>
<table><thead><tr><th>Instance</th><th>Chips</th><th>NeuronCores</th><th>HBM Memory</th><th>Karpenter</th><th>EKS Auto Mode</th></tr></thead><tbody><tr><td>trn1.32xlarge</td><td>16 Trainium v1</td><td>32</td><td>512 GiB</td><td>Supported</td><td>Supported</td></tr><tr><td>trn2.48xlarge</td><td>16 Trainium v2</td><td>64</td><td>1.5 TiB</td><td>Supported</td><td>Not yet supported</td></tr></tbody></table>
<table><thead><tr><th>Advantage</th><th>Detail</th></tr></thead><tbody><tr><td><strong>No quantization needed</strong></td><td>Both trn1 (512 GiB) and trn2 (1.5 TiB) support Scout (~220 GiB) in native BF16</td></tr><tr><td><strong>Karpenter auto-provisioning</strong></td><td>Neuron NodePool provisions Trainium nodes on-demand when workloads are scheduled</td></tr><tr><td><strong>trn2 for Maverick</strong></td><td>trn2.48xlarge (1.5 TiB) supports Maverick (~800 GiB) in BF16 without quantization</td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="memory-comparison-gpu-vs-trainium">Memory Comparison: GPU vs Trainium<a href="#memory-comparison-gpu-vs-trainium" class="hash-link" aria-label="Memory Comparison: GPU vs Trainiumì— ëŒ€í•œ ì§ì ‘ ë§í¬" title="Memory Comparison: GPU vs Trainiumì— ëŒ€í•œ ì§ì ‘ ë§í¬" translate="no">â€‹</a></h3>
<table><thead><tr><th>Model</th><th>BF16 Memory</th><th>GPU (FP8 required?)</th><th>trn1.32xlarge (512 GiB)</th><th>trn2.48xlarge (1.5 TiB)</th></tr></thead><tbody><tr><td>Scout 17B-16E</td><td>~220 GiB</td><td>p4d.24xlarge (320 GiB) - No</td><td>Fits in BF16</td><td>Fits in BF16</td></tr><tr><td>Maverick 17B-128E</td><td>~800 GiB</td><td>p5.48xlarge (640 GiB) - <strong>Yes, FP8</strong></td><td>Does not fit</td><td>Fits in BF16</td></tr></tbody></table>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>ì •ë³´</div><div class="admonitionContent_BuS1"><p>For Maverick, only <code>trn2.48xlarge</code> has sufficient memory (1.5 TiB) for BF16. GPU deployment requires FP8 quantization, and <code>trn1.32xlarge</code> (512 GiB) is insufficient.</p></div></div>
<div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>ê²½ê³ </div><div class="admonitionContent_BuS1"><p>Trainium instance availability varies by region:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">aws ec2 describe-instance-type-offerings </span><span class="token parameter variable" style="color:#36acaa">--region</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">REGION</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --location-type availability-zone </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">--filters</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Name=instance-type,Values=trn*&quot;</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">--query</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;InstanceTypeOfferings[].{Type:InstanceType,Zone:Location}&#x27;</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">--output</span><span class="token plain"> table</span><br></span></code></pre></div></div><ul>
<li><strong>trn2.48xlarge</strong>: Limited availability (<code>us-east-2</code>). <strong>Not supported by EKS Auto Mode</strong> â€” use Karpenter with the inference-ready cluster.</li>
<li><strong>trn1.32xlarge</strong>: Available in more regions (<code>us-west-2</code>, <code>us-east-1</code>, <code>us-east-2</code>, etc.).</li>
</ul></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="model-compilation">Model Compilation<a href="#model-compilation" class="hash-link" aria-label="Model Compilationì— ëŒ€í•œ ì§ì ‘ ë§í¬" title="Model Compilationì— ëŒ€í•œ ì§ì ‘ ë§í¬" translate="no">â€‹</a></h2>
<p>The AWS Neuron DLC uses <strong>optimum-neuron</strong> to run vLLM on Trainium. Models must be pre-compiled for Neuron before serving. The DLC checks the <a href="https://huggingface.co/aws-neuron/optimum-neuron-cache" target="_blank" rel="noopener noreferrer">optimum-neuron-cache</a> on Hugging Face for pre-compiled model artifacts matching your configuration (model, batch size, sequence length, tensor parallelism, dtype).</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>ì •ë³´</div><div class="admonitionContent_BuS1"><p>The <code>optimum-cli export neuron</code> command does <strong>not</strong> support <code>llama4</code> as a model type. However, <code>vllm serve</code> uses a separate inference code path (<code>optimum.neuron.models.inference.llama4</code>) that includes full MoE support via <code>Llama4NeuronModelForCausalLM</code>. Compilation is triggered automatically on first serve.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="software-versions">Software Versions<a href="#software-versions" class="hash-link" aria-label="Software Versionsì— ëŒ€í•œ ì§ì ‘ ë§í¬" title="Software Versionsì— ëŒ€í•œ ì§ì ‘ ë§í¬" translate="no">â€‹</a></h2>
<table><thead><tr><th>Component</th><th>Version</th><th>Notes</th></tr></thead><tbody><tr><td>Neuron SDK</td><td>2.26.1</td><td>Required</td></tr><tr><td>optimum-neuron</td><td>&gt;= 0.4.0</td><td>Llama 4 inference support added in v0.4.0</td></tr><tr><td>vLLM</td><td>0.11.0</td><td>With optimum-neuron Neuron platform plugin</td></tr><tr><td>neuronx-distributed</td><td>0.15</td><td>MoE module used by Llama 4 inference</td></tr><tr><td>DLC Image</td><td><code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/huggingface-vllm-inference-neuronx:0.11.0-optimum0.4.5-neuronx-py310-sdk2.26.1-ubuntu22.04</code></td><td>Latest available</td></tr></tbody></table>
<div class="collapsibleContent_q3kw"><div class="header_QCEw"><h2><span>Deploying the Inference-Ready EKS Cluster</span></h2><span class="icon_PckA">ğŸ‘ˆ</span></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-llama-4-scout-on-trainium">Deploy Llama 4 Scout on Trainium<a href="#deploy-llama-4-scout-on-trainium" class="hash-link" aria-label="Deploy Llama 4 Scout on Trainiumì— ëŒ€í•œ ì§ì ‘ ë§í¬" title="Deploy Llama 4 Scout on Trainiumì— ëŒ€í•œ ì§ì ‘ ë§í¬" translate="no">â€‹</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-1-create-hugging-face-token-secret">Step 1: Create Hugging Face Token Secret<a href="#step-1-create-hugging-face-token-secret" class="hash-link" aria-label="Step 1: Create Hugging Face Token Secretì— ëŒ€í•œ ì§ì ‘ ë§í¬" title="Step 1: Create Hugging Face Token Secretì— ëŒ€í•œ ì§ì ‘ ë§í¬" translate="no">â€‹</a></h3>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl create secret generic hf-token --from-literal</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">token</span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">your-huggingface-token</span><span class="token operator" style="color:#393A34">&gt;</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-2-deploy-with-helm">Step 2: Deploy with Helm<a href="#step-2-deploy-with-helm" class="hash-link" aria-label="Step 2: Deploy with Helmì— ëŒ€í•œ ì§ì ‘ ë§í¬" title="Step 2: Deploy with Helmì— ëŒ€í•œ ì§ì ‘ ë§í¬" translate="no">â€‹</a></h3>
<p>For <strong>trn2.48xlarge</strong> (Scout):</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">helm repo </span><span class="token function" style="color:#d73a49">add</span><span class="token plain"> ai-on-eks https://awslabs.github.io/ai-on-eks-charts/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">helm repo update</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">helm </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> llama4-scout-neuron ai-on-eks/inference-charts </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">--values</span><span class="token plain"> https://raw.githubusercontent.com/awslabs/ai-on-eks-charts/refs/heads/main/charts/inference-charts/values-llama-4-scout-17b-vllm-neuron.yaml</span><br></span></code></pre></div></div>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>ì •ë³´</div><div class="admonitionContent_BuS1"><p>Key deployment parameters:</p><ul>
<li><strong>tensor_parallel_size: 16</strong> (one per Trainium chip, not per NeuronCore)</li>
<li><strong>Docker image</strong>: AWS Neuron DLC from private ECR (<code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/huggingface-vllm-inference-neuronx</code>)</li>
<li><strong>Neuron device requests</strong>: <code>aws.amazon.com/neuron: 16</code> for all 16 chips</li>
<li><strong>CPU memory</strong>: <code>384Gi</code> minimum (weight sharding requires loading the full model into CPU memory)</li>
<li><strong>Instance type</strong>: <code>trn2.48xlarge</code> (default for both Scout and Maverick)</li>
<li><strong>Environment variable</strong>: <code>VLLM_NEURON_FRAMEWORK=optimum</code> is required for on-the-fly Neuron compilation</li>
</ul></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-3-monitor-deployment">Step 3: Monitor Deployment<a href="#step-3-monitor-deployment" class="hash-link" aria-label="Step 3: Monitor Deploymentì— ëŒ€í•œ ì§ì ‘ ë§í¬" title="Step 3: Monitor Deploymentì— ëŒ€í•œ ì§ì ‘ ë§í¬" translate="no">â€‹</a></h3>
<p>After deploying, Karpenter will automatically provision a Trainium node:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Watch node provisioning</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get nodeclaims </span><span class="token parameter variable" style="color:#36acaa">-w</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Check pod status</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get pods </span><span class="token parameter variable" style="color:#36acaa">-w</span><br></span></code></pre></div></div>
<p>During deployment, the pod will go through these stages:</p>
<ol>
<li><strong>Pending</strong> - waiting for Trainium node provisioning (~5 minutes)</li>
<li><strong>ContainerCreating</strong> - pulling the Neuron DLC image (~2.9 GiB)</li>
<li><strong>Running</strong> - Neuron model compilation (30-60+ minutes on first run)</li>
<li><strong>Ready</strong> - vLLM server is serving requests</li>
</ol>
<div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>CPU Memory Requirements</div><div class="admonitionContent_BuS1"><p>The pod requires <strong>at least 384 GiB of CPU memory</strong> for model weight sharding across 16 Neuron devices. With insufficient memory (e.g., 64 GiB), the pod will be OOMKilled during weight loading. The trn2.48xlarge instance provides ~2 TiB of system memory, so this is well within capacity.</p></div></div>
<div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>ê²½ê³ </div><div class="admonitionContent_BuS1"><p>The first deployment takes significantly longer due to Neuron model compilation. Subsequent deployments with the same configuration will use cached artifacts. Monitor the compilation progress in the logs:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl logs </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">-l</span><span class="token plain"> app.kubernetes.io/instance</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">llama4-scout-neuron</span><br></span></code></pre></div></div></div></div>
<p><strong>Tested deployment timeline on trn2.48xlarge (Scout):</strong></p>
<table><thead><tr><th>Phase</th><th>Duration</th><th>Description</th></tr></thead><tbody><tr><td>Node provisioning</td><td>~5 min</td><td>Karpenter provisions trn2.48xlarge</td></tr><tr><td>Image pull</td><td>~30 sec</td><td>DLC image (~2.9 GiB, cached after first pull)</td></tr><tr><td>HLO generation</td><td>~60 sec</td><td>Generates HLOs for context_encoding and token_generation</td></tr><tr><td>Neuron compilation</td><td>~200 sec</td><td>neuronx-cc compiles HLOs to NEFFs (target=trn2)</td></tr><tr><td>Model build</td><td>~650 sec</td><td>Weight layout transformation</td></tr><tr><td>Weight loading</td><td>~5 min</td><td>Download, shard, and load weights to 16 Neuron devices</td></tr><tr><td><strong>Total (first deploy)</strong></td><td><strong>~20 min</strong></td><td>Subsequent deploys reuse cached compilation artifacts</td></tr></tbody></table>
<p>Once complete, the vLLM server will start:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">INFO:     Application startup complete.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">INFO:     Uvicorn running on http://0.0.0.0:8000</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-llama-4-maverick-on-trainium2">Deploy Llama 4 Maverick on Trainium2<a href="#deploy-llama-4-maverick-on-trainium2" class="hash-link" aria-label="Deploy Llama 4 Maverick on Trainium2ì— ëŒ€í•œ ì§ì ‘ ë§í¬" title="Deploy Llama 4 Maverick on Trainium2ì— ëŒ€í•œ ì§ì ‘ ë§í¬" translate="no">â€‹</a></h2>
<p>Maverick requires <code>trn2.48xlarge</code> (1.5 TiB HBM) and runs in native BF16 without quantization. Ensure your cluster has the <code>trn2-neuron</code> Karpenter NodePool configured (see cluster setup above).</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">helm </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> llama4-maverick-neuron ai-on-eks/inference-charts </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">--values</span><span class="token plain"> https://raw.githubusercontent.com/awslabs/ai-on-eks-charts/refs/heads/main/charts/inference-charts/values-llama-4-maverick-17b-vllm-neuron.yaml</span><br></span></code></pre></div></div>
<div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>ê²½ê³ </div><div class="admonitionContent_BuS1"><ul>
<li><code>trn2.48xlarge</code> is available in limited regions (<code>us-east-2</code>). Verify availability before deploying.</li>
<li>Ensure your AWS account has sufficient <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html" target="_blank" rel="noopener noreferrer">service quota</a> for Trainium instances (Maverick requires 192 vCPUs).</li>
</ul></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="test-the-model">Test the Model<a href="#test-the-model" class="hash-link" aria-label="Test the Modelì— ëŒ€í•œ ì§ì ‘ ë§í¬" title="Test the Modelì— ëŒ€í•œ ì§ì ‘ ë§í¬" translate="no">â€‹</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="port-forward">Port Forward<a href="#port-forward" class="hash-link" aria-label="Port Forwardì— ëŒ€í•œ ì§ì ‘ ë§í¬" title="Port Forwardì— ëŒ€í•œ ì§ì ‘ ë§í¬" translate="no">â€‹</a></h3>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward svc/llama4-scout-neuron </span><span class="token number" style="color:#36acaa">8000</span><span class="token plain">:8000</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="chat-completion-request">Chat Completion Request<a href="#chat-completion-request" class="hash-link" aria-label="Chat Completion Requestì— ëŒ€í•œ ì§ì ‘ ë§í¬" title="Chat Completion Requestì— ëŒ€í•œ ì§ì ‘ ë§í¬" translate="no">â€‹</a></h3>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">curl</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">-X</span><span class="token plain"> POST http://localhost:8000/v1/chat/completions </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">-H</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Content-Type: application/json&quot;</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">-d</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;{</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;model&quot;: &quot;meta-llama/Llama-4-Scout-17B-16E-Instruct&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;messages&quot;: [</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">      {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Explain the benefits of Mixture of Experts architecture in large language models.&quot;}</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    ],</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;max_tokens&quot;: 512,</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;temperature&quot;: 0.7</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">  }&#x27;</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="list-available-models">List Available Models<a href="#list-available-models" class="hash-link" aria-label="List Available Modelsì— ëŒ€í•œ ì§ì ‘ ë§í¬" title="List Available Modelsì— ëŒ€í•œ ì§ì ‘ ë§í¬" translate="no">â€‹</a></h3>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">curl</span><span class="token plain"> http://localhost:8000/v1/models </span><span class="token operator" style="color:#393A34">|</span><span class="token plain"> python3 </span><span class="token parameter variable" style="color:#36acaa">-m</span><span class="token plain"> json.tool</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="multimodal-request-text--image">Multimodal Request (Text + Image)<a href="#multimodal-request-text--image" class="hash-link" aria-label="Multimodal Request (Text + Image)ì— ëŒ€í•œ ì§ì ‘ ë§í¬" title="Multimodal Request (Text + Image)ì— ëŒ€í•œ ì§ì ‘ ë§í¬" translate="no">â€‹</a></h3>
<p>Llama 4 supports multimodal inference. Send image URLs alongside text:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">curl</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">-X</span><span class="token plain"> POST http://localhost:8000/v1/chat/completions </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">-H</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Content-Type: application/json&quot;</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">-d</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;{</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;model&quot;: &quot;meta-llama/Llama-4-Scout-17B-16E-Instruct&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;messages&quot;: [</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">      {</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">        &quot;role&quot;: &quot;user&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">        &quot;content&quot;: [</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">          {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe what you see in this image.&quot;},</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">          {&quot;type&quot;: &quot;image_url&quot;, &quot;image_url&quot;: {&quot;url&quot;: &quot;https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg&quot;}}</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">        ]</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">      }</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    ],</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;max_tokens&quot;: 256</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">  }&#x27;</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-open-webui">Deploy Open WebUI<a href="#deploy-open-webui" class="hash-link" aria-label="Deploy Open WebUIì— ëŒ€í•œ ì§ì ‘ ë§í¬" title="Deploy Open WebUIì— ëŒ€í•œ ì§ì ‘ ë§í¬" translate="no">â€‹</a></h2>
<p><a href="https://github.com/open-webui/open-webui" target="_blank" rel="noopener noreferrer">Open WebUI</a> provides a ChatGPT-style interface for interacting with the model.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">helm repo </span><span class="token function" style="color:#d73a49">add</span><span class="token plain"> open-webui https://helm.openwebui.com/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">helm repo update</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">helm </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> open-webui open-webui/open-webui </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">--namespace</span><span class="token plain"> open-webui --create-namespace </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">--set</span><span class="token plain"> </span><span class="token assign-left variable" style="color:#36acaa">ollama.enabled</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">false </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">--set</span><span class="token plain"> </span><span class="token assign-left variable" style="color:#36acaa">env.OPENAI_API_BASE_URL</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">http://llama4-scout-neuron.default.svc.cluster.local:8000/v1 </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">--set</span><span class="token plain"> </span><span class="token assign-left variable" style="color:#36acaa">env.OPENAI_API_KEY</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">dummy</span><br></span></code></pre></div></div>
<p>Access the UI:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward svc/open-webui </span><span class="token number" style="color:#36acaa">8080</span><span class="token plain">:80 </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> open-webui</span><br></span></code></pre></div></div>
<p>Open <a href="http://localhost:8080" target="_blank" rel="noopener noreferrer">http://localhost:8080</a> in your browser and register a new account. The model will appear in the model selector.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="monitoring">Monitoring<a href="#monitoring" class="hash-link" aria-label="Monitoringì— ëŒ€í•œ ì§ì ‘ ë§í¬" title="Monitoringì— ëŒ€í•œ ì§ì ‘ ë§í¬" translate="no">â€‹</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="check-inference-logs">Check Inference Logs<a href="#check-inference-logs" class="hash-link" aria-label="Check Inference Logsì— ëŒ€í•œ ì§ì ‘ ë§í¬" title="Check Inference Logsì— ëŒ€í•œ ì§ì ‘ ë§í¬" translate="no">â€‹</a></h3>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># View vLLM Neuron logs</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl logs </span><span class="token parameter variable" style="color:#36acaa">-l</span><span class="token plain"> app.kubernetes.io/instance</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">llama4-scout-neuron </span><span class="token parameter variable" style="color:#36acaa">--tail</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">100</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Monitor token generation throughput</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl logs </span><span class="token parameter variable" style="color:#36acaa">-l</span><span class="token plain"> app.kubernetes.io/instance</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">llama4-scout-neuron </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">|</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">grep</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;tokens/s&quot;</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="observability-dashboard">Observability Dashboard<a href="#observability-dashboard" class="hash-link" aria-label="Observability Dashboardì— ëŒ€í•œ ì§ì ‘ ë§í¬" title="Observability Dashboardì— ëŒ€í•œ ì§ì ‘ ë§í¬" translate="no">â€‹</a></h3>
<p>If the observability stack is enabled on your cluster, access Grafana:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward svc/kube-prometheus-stack-grafana </span><span class="token number" style="color:#36acaa">3000</span><span class="token plain">:80 </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> monitoring</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="cleanup">Cleanup<a href="#cleanup" class="hash-link" aria-label="Cleanupì— ëŒ€í•œ ì§ì ‘ ë§í¬" title="Cleanupì— ëŒ€í•œ ì§ì ‘ ë§í¬" translate="no">â€‹</a></h2>
<p>Remove the model deployment:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Remove Scout</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">helm uninstall llama4-scout-neuron</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Remove Maverick (if deployed)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">helm uninstall llama4-maverick-neuron</span><br></span></code></pre></div></div>
<p>To destroy the entire cluster infrastructure:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> ai-on-eks/infra/solutions/inference-ready-cluster</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">./cleanup.sh</span><br></span></code></pre></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/awslabs/ai-on-eks/blob/main/website/docs/blueprints/inference/framework-guides/Neuron/llama4-trn2.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>í˜ì´ì§€ í¸ì§‘</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="ë¬¸ì„œ í˜ì´ì§€"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai-on-eks/ko/docs/blueprints/inference/framework-guides/Neuron/rayserve-ha"><div class="pagination-nav__sublabel">ì´ì „</div><div class="pagination-nav__label">Ray Serve ê³ ê°€ìš©ì„±</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai-on-eks/ko/docs/blueprints/inference/inference-charts"><div class="pagination-nav__sublabel">ë‹¤ìŒ</div><div class="pagination-nav__label">ì¶”ë¡  ì°¨íŠ¸</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#why-trainium-for-llama-4" class="table-of-contents__link toc-highlight">Why Trainium for Llama 4?</a><ul><li><a href="#memory-comparison-gpu-vs-trainium" class="table-of-contents__link toc-highlight">Memory Comparison: GPU vs Trainium</a></li></ul></li><li><a href="#model-compilation" class="table-of-contents__link toc-highlight">Model Compilation</a></li><li><a href="#software-versions" class="table-of-contents__link toc-highlight">Software Versions</a><ul><li><a href="#prerequisites" class="table-of-contents__link toc-highlight">Prerequisites</a></li><li><a href="#deploy-the-cluster" class="table-of-contents__link toc-highlight">Deploy the Cluster</a></li><li><a href="#configure-kubectl" class="table-of-contents__link toc-highlight">Configure kubectl</a></li><li><a href="#verify-karpenter-resources" class="table-of-contents__link toc-highlight">Verify Karpenter Resources</a></li><li><a href="#install-neuron-device-plugin" class="table-of-contents__link toc-highlight">Install Neuron Device Plugin</a></li><li><a href="#neuron-resource-names" class="table-of-contents__link toc-highlight">Neuron Resource Names</a></li></ul></li><li><a href="#deploy-llama-4-scout-on-trainium" class="table-of-contents__link toc-highlight">Deploy Llama 4 Scout on Trainium</a><ul><li><a href="#step-1-create-hugging-face-token-secret" class="table-of-contents__link toc-highlight">Step 1: Create Hugging Face Token Secret</a></li><li><a href="#step-2-deploy-with-helm" class="table-of-contents__link toc-highlight">Step 2: Deploy with Helm</a></li><li><a href="#step-3-monitor-deployment" class="table-of-contents__link toc-highlight">Step 3: Monitor Deployment</a></li></ul></li><li><a href="#deploy-llama-4-maverick-on-trainium2" class="table-of-contents__link toc-highlight">Deploy Llama 4 Maverick on Trainium2</a></li><li><a href="#test-the-model" class="table-of-contents__link toc-highlight">Test the Model</a><ul><li><a href="#port-forward" class="table-of-contents__link toc-highlight">Port Forward</a></li><li><a href="#chat-completion-request" class="table-of-contents__link toc-highlight">Chat Completion Request</a></li><li><a href="#list-available-models" class="table-of-contents__link toc-highlight">List Available Models</a></li><li><a href="#multimodal-request-text--image" class="table-of-contents__link toc-highlight">Multimodal Request (Text + Image)</a></li></ul></li><li><a href="#deploy-open-webui" class="table-of-contents__link toc-highlight">Deploy Open WebUI</a></li><li><a href="#monitoring" class="table-of-contents__link toc-highlight">Monitoring</a><ul><li><a href="#check-inference-logs" class="table-of-contents__link toc-highlight">Check Inference Logs</a></li><li><a href="#observability-dashboard" class="table-of-contents__link toc-highlight">Observability Dashboard</a></li></ul></li><li><a href="#cleanup" class="table-of-contents__link toc-highlight">Cleanup</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">ì°¸ì—¬í•˜ê¸°</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/awslabs/ai-on-eks" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Built with â¤ï¸ at AWS  <br> Â© ${new Date().getFullYear()} Amazon.com, Inc. or its affiliates. All Rights Reserved</div></div></div></footer><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;7fbc7ab02fae4767b1af2588eba0cdf2&quot;}"></script></div>
</body>
</html>
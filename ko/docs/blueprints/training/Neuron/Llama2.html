<!doctype html>
<html lang="ko-KR" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-blueprints/training/Neuron/Llama2" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.0">
<title data-rh="true">Trn1에서 Nemo-Megatron을 활용한 Llama-2 | AI on EKS</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/training/Neuron/Llama2"><meta data-rh="true" property="og:locale" content="ko_KR"><meta data-rh="true" property="og:locale:alternate" content="en_US"><meta data-rh="true" name="docusaurus_locale" content="ko"><meta data-rh="true" name="docsearch:language" content="ko"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Trn1에서 Nemo-Megatron을 활용한 Llama-2 | AI on EKS"><meta data-rh="true" name="description" content="Trainium, Neuronx-Nemo-Megatron 및 MPI operator를 사용한 Llama-2 모델 훈련"><meta data-rh="true" property="og:description" content="Trainium, Neuronx-Nemo-Megatron 및 MPI operator를 사용한 Llama-2 모델 훈련"><link data-rh="true" rel="icon" href="/ai-on-eks/ko/img/header-icon.png"><link data-rh="true" rel="canonical" href="https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/training/Neuron/Llama2"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/docs/blueprints/training/Neuron/Llama2" hreflang="en-US"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/training/Neuron/Llama2" hreflang="ko-KR"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/docs/blueprints/training/Neuron/Llama2" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"EKS에서의 학습","item":"https://awslabs.github.io/ai-on-eks/ko/docs/category/training-on-eks"},{"@type":"ListItem","position":2,"name":"Trn1에서 Nemo-Megatron을 활용한 Llama-2","item":"https://awslabs.github.io/ai-on-eks/ko/docs/blueprints/training/Neuron/Llama2"}]}</script><link rel="stylesheet" href="/ai-on-eks/ko/assets/css/styles.c270b852.css">
<script src="/ai-on-eks/ko/assets/js/runtime~main.84f685ef.js" defer="defer"></script>
<script src="/ai-on-eks/ko/assets/js/main.62626cce.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><div role="region" aria-label="본문으로 건너뛰기"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">본문으로 건너뛰기</a></div><div class="theme-announcement-bar announcementBar_mb4j" style="background-color:#667eea;color:#ffffff" role="banner"><div class="announcementBarPlaceholder_vyr4"></div><div class="content_knG7 announcementBarContent_xLdY">GenAI on EKS workshop series! <a target="_blank" rel="noopener noreferrer" href="https://aws-experience.com/emea/smb/events/series/get-hands-on-with-amazon-eks?trk=9be4af2e-2339-40ae-b5e9-57b6a7704c36&sc_channel=el" style="color: #ffffff; text-decoration: underline; font-weight: bold; margin-left: 10px;">Register now →</a></div><button type="button" aria-label="닫기" class="clean-btn close closeButton_CVFx announcementBarClose_gvF7"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav aria-label="메인" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="사이드바 펼치거나 접기" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai-on-eks/ko/"><div class="navbar__logo"><img src="/ai-on-eks/ko/img/header-icon.png" alt="AIoEKS 로고" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai-on-eks/ko/img/header-icon.png" alt="AIoEKS 로고" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href="/ai-on-eks/ko/docs/infra">인프라</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/ai-on-eks/ko/docs/blueprints">블루프린트</a><a class="navbar__item navbar__link" href="/ai-on-eks/ko/docs/guidance">가이드</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/awslabs/ai-on-eks" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>한국어</a><ul class="dropdown__menu"><li><a href="/ai-on-eks/docs/blueprints/training/Neuron/Llama2" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en-US">English</a></li><li><a href="/ai-on-eks/ko/docs/blueprints/training/Neuron/Llama2" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="ko-KR">한국어</a></li></ul></div><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="어두운 모드와 밝은 모드 전환하기 (현재 system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="맨 위로 스크롤하기" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="문서 사이드바" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai-on-eks/ko/docs/blueprints"><span title="개요" class="linkLabel_WmDU">개요</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/ai-on-eks/ko/docs/blueprints/inference"><span title="EKS에서의 추론" class="categoryLinkLabel_W154">EKS에서의 추론</span></a><button aria-label="사이드바 분류 &#x27;EKS에서의 추론&#x27; 펼치기" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/ai-on-eks/ko/docs/category/training-on-eks"><span title="EKS에서의 학습" class="categoryLinkLabel_W154">EKS에서의 학습</span></a><button aria-label="사이드바 분류 &#x27;EKS에서의 학습&#x27; 접기" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/training/GPUs/bionemo"><span title="GPU" class="categoryLinkLabel_W154">GPU</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/training/Neuron/RayTrain-Llama2"><span title="Neuron" class="categoryLinkLabel_W154">Neuron</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/training/Neuron/RayTrain-Llama2"><span title="RayTrain을 활용한 Trn1에서의 Llama-2" class="linkLabel_WmDU">RayTrain을 활용한 Trn1에서의 Llama-2</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/training/Neuron/Llama2"><span title="Trn1에서 Nemo-Megatron을 활용한 Llama-2" class="linkLabel_WmDU">Trn1에서 Nemo-Megatron을 활용한 Llama-2</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/training/Neuron/BERT-Large"><span title="Trainium에서의 BERT-Large" class="linkLabel_WmDU">Trainium에서의 BERT-Large</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/ko/docs/blueprints/training/Neuron/Llama-LoRA-Finetuning"><span title="LoRA를 활용한 Llama 3 파인튜닝" class="linkLabel_WmDU">LoRA를 활용한 Llama 3 파인튜닝</span></a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ai-on-eks/ko/docs/blueprints/gateways/envoy-gateway"><span title="게이트웨이" class="categoryLinkLabel_W154">게이트웨이</span></a></div></li></ul></nav><button type="button" title="사이드바 숨기기" aria-label="사이드바 숨기기" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="탐색 경로"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="홈" class="breadcrumbs__link" href="/ai-on-eks/ko/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai-on-eks/ko/docs/category/training-on-eks"><span>EKS에서의 학습</span></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Neuron</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Trn1에서 Nemo-Megatron을 활용한 Llama-2</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">이 페이지에서</button></div><div class="theme-doc-markdown markdown"><div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>경고</div><div class="admonitionContent_BuS1"><p>EKS에서 ML 모델을 배포하려면 GPU 또는 Neuron 인스턴스에 대한 접근 권한이 필요합니다. 배포가 작동하지 않는 경우 이러한 리소스에 대한 접근 권한이 없기 때문인 경우가 많습니다. 또한 일부 배포 패턴은 Karpenter 자동 스케일링과 정적 노드 그룹에 의존합니다. 노드가 초기화되지 않으면 Karpenter 또는 노드 그룹의 로그를 확인하여 문제를 해결하세요.</p></div></div>
<div class="theme-admonition theme-admonition-danger admonition_xJq3 alert alert--danger"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"></path></svg></span>위험</div><div class="admonitionContent_BuS1"><p>참고: 이 Llama-2 모델의 사용은 Meta 라이선스의 적용을 받습니다.
모델 가중치와 토크나이저를 다운로드하려면 <a href="https://ai.meta.com/" target="_blank" rel="noopener noreferrer">웹사이트</a>를 방문하여 접근 권한을 요청하기 전에 라이선스에 동의해야 합니다.</p></div></div>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>정보</div><div class="admonitionContent_BuS1"><p>이 블루프린트는 관측성, 로깅 및 확장성 측면의 개선 사항을 통합하기 위해 적극적으로 개선 중입니다.</p></div></div>
<header><h1>Trainium, Neuronx-Nemo-Megatron 및 MPI operator를 사용한 Llama-2 모델 훈련</h1></header>
<p>AWS Trainium, Neuronx-Nemo-Megatron 및 MPI Operator를 사용하여 Amazon Elastic Kubernetes Service (EKS)에서 <a href="https://ai.meta.com/llama/#inside-the-model" target="_blank" rel="noopener noreferrer">Meta Llama-2-7b</a> 모델을 훈련하는 종합 가이드에 오신 것을 환영합니다.</p>
<p>이 튜토리얼에서는 Amazon EKS에서 <a href="https://aws.amazon.com/machine-learning/trainium/" target="_blank" rel="noopener noreferrer">AWS Trainium</a> 가속기를 사용하여 다중 노드 훈련 작업을 실행하는 방법을 배웁니다. 구체적으로 <a href="https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample" target="_blank" rel="noopener noreferrer">RedPajama 데이터셋의 하위 집합</a>을 사용하여 4개의 AWS EC2 trn1.32xlarge 인스턴스에서 Llama-2-7b를 사전 훈련합니다.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="llama-2란">Llama-2란?<a href="#llama-2란" class="hash-link" aria-label="Llama-2란?에 대한 직접 링크" title="Llama-2란?에 대한 직접 링크" translate="no">​</a></h3>
<p>Llama-2는 2조 개의 텍스트 및 코드 토큰으로 훈련된 대규모 언어 모델(LLM)입니다. 현재 사용 가능한 가장 크고 강력한 LLM 중 하나입니다. Llama-2는 자연어 처리, 텍스트 생성, 번역 등 다양한 작업에 사용할 수 있습니다.</p>
<p>Llama-2는 사전 훈련된 모델로 제공되지만, 이 튜토리얼에서는 모델을 처음부터 사전 훈련하는 방법을 보여드립니다.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="llama-2-chat">Llama-2-chat<a href="#llama-2-chat" class="hash-link" aria-label="Llama-2-chat에 대한 직접 링크" title="Llama-2-chat에 대한 직접 링크" translate="no">​</a></h4>
<p>Llama-2는 엄격한 훈련 과정을 거친 뛰어난 언어 모델입니다. 공개적으로 이용 가능한 온라인 데이터를 사용한 사전 훈련으로 시작합니다.</p>
<p>Llama-2는 세 가지 다른 모델 크기로 제공됩니다:</p>
<ul>
<li><strong>Llama-2-70b:</strong> 700억 개의 파라미터를 가진 가장 큰 Llama-2 모델입니다. 가장 강력한 Llama-2 모델이며 가장 까다로운 작업에 사용할 수 있습니다.</li>
<li><strong>Llama-2-13b:</strong> 130억 개의 파라미터를 가진 중간 크기의 Llama-2 모델입니다. 성능과 효율성 사이의 좋은 균형을 제공하며 다양한 작업에 사용할 수 있습니다.</li>
<li><strong>Llama-2-7b:</strong> 70억 개의 파라미터를 가진 가장 작은 Llama-2 모델입니다. 가장 효율적인 Llama-2 모델이며 최고 수준의 성능이 필요하지 않은 작업에 사용할 수 있습니다.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="어떤-llama-2-모델-크기를-사용해야-하나요"><strong>어떤 Llama-2 모델 크기를 사용해야 하나요?</strong><a href="#어떤-llama-2-모델-크기를-사용해야-하나요" class="hash-link" aria-label="어떤-llama-2-모델-크기를-사용해야-하나요에 대한 직접 링크" title="어떤-llama-2-모델-크기를-사용해야-하나요에 대한 직접 링크" translate="no">​</a></h3>
<p>최적의 Llama-2 모델 크기는 특정 요구사항에 따라 달라지며, 최고 성능을 달성하기 위해 항상 가장 큰 모델이 필요한 것은 아닙니다. 적절한 Llama-2 모델 크기를 선택할 때 컴퓨팅 리소스, 응답 시간, 비용 효율성과 같은 요소를 평가하고 고려하는 것이 좋습니다. 결정은 애플리케이션의 목표와 제약 조건에 대한 종합적인 평가를 기반으로 해야 합니다.</p>
<p><strong>성능 향상</strong>
Llama-2는 GPU에서 고성능 추론을 달성할 수 있지만, Neuron 가속기는 성능을 한 단계 더 끌어올립니다. Neuron 가속기는 머신 러닝 워크로드를 위해 특별히 설계되어 Llama-2의 추론 속도를 크게 향상시키는 하드웨어 가속을 제공합니다. 이는 Trn1/Inf2 인스턴스에서 Llama-2를 배포할 때 더 빠른 응답 시간과 개선된 사용자 경험으로 이어집니다.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="솔루션-아키텍처">솔루션 아키텍처<a href="#솔루션-아키텍처" class="hash-link" aria-label="솔루션 아키텍처에 대한 직접 링크" title="솔루션 아키텍처에 대한 직접 링크" translate="no">​</a></h2>
<p>이 섹션에서는 솔루션의 아키텍처를 자세히 살펴봅니다.</p>
<p><strong>Trn1.32xl 인스턴스:</strong> 머신 러닝 훈련 워크로드에 최적화된 EC2 Trn1 (Trainium) 인스턴스 패밀리의 일부인 EC2 가속 인스턴스 유형입니다.</p>
<p><strong>MPI Worker Pods:</strong> MPI (Message Passing Interface) 작업을 실행하도록 구성된 Kubernetes 파드입니다. MPI는 분산 메모리 병렬 컴퓨팅을 위한 표준입니다. 각 워커 파드는 16개의 Trainium 가속기와 8개의 Elastic Fabric Adapters (EFA)가 장착된 trn1.32xlarge 인스턴스에서 실행됩니다. EFA는 Amazon EC2 인스턴스에서 실행되는 고성능 컴퓨팅 애플리케이션을 지원하는 네트워크 장치입니다.</p>
<p><strong>MPI Launcher Pod:</strong> 워커 파드 전체에서 MPI 작업을 조정하는 역할을 담당하는 파드입니다. 훈련 작업이 클러스터에 처음 제출되면 MPI 런처 파드가 생성되어 워커들이 온라인 상태가 되기를 기다리고, 각 워커에 연결한 다음 훈련 스크립트를 호출합니다.</p>
<p><strong>MPI Operator:</strong> Kubernetes에서 오퍼레  이터는 Kubernetes 애플리케이션을 패키징, 배포 및 관리하는 방법입니다. MPI Operator는 MPI 워크로드의 배포 및 관리를 자동화합니다.</p>
<p><strong>FSx for Lustre:</strong> 머신 러닝, 고성능 컴퓨팅(HPC), 비디오 처리, 금융 모델링과 같은 워크로드에 적합한 공유 고성능 파일 시스템입니다. FSx for Lustre 파일 시스템은 훈련 작업의 워커 파드 간에 공유되어 훈련 데이터에 접근하고 모델 아티팩트 및 로그를 저장하기 위한 중앙 저장소를 제공합니다.</p>
<p><img decoding="async" loading="lazy" alt="Llama-2-trn1" src="/ai-on-eks/ko/assets/images/llama2-trainium-076066f4fecbd832a016e76f75269187.png" width="885" height="614" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="솔루션-배포">솔루션 배포<a href="#솔루션-배포" class="hash-link" aria-label="솔루션 배포에 대한 직접 링크" title="솔루션 배포에 대한 직접 링크" translate="no">​</a></h2>
<p><strong>Amazon EKS에서 AWS Trainium을 사용하여 Llama-2를 훈련하는 단계</strong></p>
<p>참고: 이 게시물은 Meta의 Llama 토크나이저를 사용하며, 토크나이저 파일을 다운로드하기 전에 수락해야 하는 사용자 라이선스로 보호됩니다. 여기에서 접근 권한을 요청하여 Llama 파일에 대한 접근 권한이 있는지 확인하세요.</p>
<div class="collapsibleContent_q3kw"><div class="header_QCEw"><h2><span>사전 요구사항</span></h2><span class="icon_PckA">👈</span></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="분산-훈련">분산 훈련<a href="#분산-훈련" class="hash-link" aria-label="분산 훈련에 대한 직접 링크" title="분산 훈련에 대한 직접 링크" translate="no">​</a></h2>
<p>EKS 클러스터가 배포되면 neuronx-nemo-megatron 컨테이너 이미지를 빌드하고 이미지를 ECR에 푸시하는 다음 단계를 진행할 수 있습니다.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="neuronx-nemo-megatron-컨테이너-이미지-빌드">neuronx-nemo-megatron 컨테이너 이미지 빌드<a href="#neuronx-nemo-megatron-컨테이너-이미지-빌드" class="hash-link" aria-label="neuronx-nemo-megatron 컨테이너 이미지 빌드에 대한 직접 링크" title="neuronx-nemo-megatron 컨테이너 이미지 빌드에 대한 직접 링크" translate="no">​</a></h3>
<p>examples/llama2 디렉토리로 이동</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> examples/llama2/</span><br></span></code></pre></div></div>
<p><code>1-llama2-neuronx-pretrain-build-image.sh</code> 스크립트를 실행하여 neuronx-nemo-megatron 컨테이너 이미지를 빌드하고 ECR에 이미지를 푸시합니다.</p>
<p>리전을 입력하라는 메시지가 표시되면 위에서 EKS 클러스터를 시작한 리전을 입력하세요.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">./1-llama2-neuronx-pretrain-build-image.sh</span><br></span></code></pre></div></div>
<p>참고: 이미지 빌드 및 ECR 푸시에는 약 10분이 소요됩니다</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cli-파드-시작-및-연결">CLI 파드 시작 및 연결<a href="#cli-파드-시작-및-연결" class="hash-link" aria-label="CLI 파드 시작  및 연결에 대한 직접 링크" title="CLI 파드 시작 및 연결에 대한 직접 링크" translate="no">​</a></h3>
<p>이 단계에서는 공유 FSx 스토리지에 대한 접근이 필요합니다. 이 스토리지에 파일을 복사하려면 먼저 위에서 생성한 neuronx-nemo-megatron Docker 이미지를 실행하는 CLI 파드를 시작하고 연결합니다.</p>
<p>다음 스크립트를 실행하여 CLI 파드를 시작합니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">./2-launch-cmd-shell-pod.sh</span><br></span></code></pre></div></div>
<p>다음으로, CLI 파드가 &#x27;Running&#x27; 상태가 될 때까지 다음 명령을 주기적으로 실행합니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get pod </span><span class="token parameter variable" style="color:#36acaa">-w</span><br></span></code></pre></div></div>
<p>CLI 파드가 &#x27;Running&#x27; 상태가 되면 다음 명령을 사용하여 연결합니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl </span><span class="token builtin class-name">exec</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">-it</span><span class="token plain"> cli-cmd-shell -- /bin/bash</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="llama-토크나이저-및-redpajama-데이터셋을-fsx에-다운로드">Llama 토크나이저 및 Redpajama 데이터셋을 FSx에 다운로드<a href="#llama-토크나이저-및-redpajama-데이터셋을-fsx에-다운로드" class="hash-link" aria-label="Llama 토크나이저 및 Redpajama 데이터셋을 FSx에 다운로드에 대한 직접 링크" title="Llama 토크나이저 및 Redpajama 데이터셋을 FSx에 다운로드에 대한 직접 링크" translate="no">​</a></h3>
<p>CLI 파드 내에서 Llama 토크나이저 파일을 다운로드합니다. 이 파일들은 Meta의 Llama 라이선스로 보호되므로 <code>huggingface-cli login</code> 명령을 실행하여 접근 토큰으로 Hugging Face에 로그인해야 합니다. 접근 토큰은 Hugging Face 웹사이트의 Settings -&gt; Access Tokens에서 찾을 수 있습니다.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">huggingface-cli login</span><br></span></code></pre></div></div>
<p>토큰을 입력하라는 메시지가 표시되면 접근 토큰을 붙여넣고 <code>ENTER</code>를 누릅니다.</p>
<p>다음으로, 다음 python 코드를 실행하여 llama7-7b 토크나이저 파일을 /shared/llama7b_tokenizer에 다운로드합니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python3 </span><span class="token operator" style="color:#393A34">&lt;&lt;</span><span class="token string" style="color:#e3116c">EOF</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">import transformers</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">tok = transformers.AutoTokenizer.from_pretrained(&quot;meta-llama/Llama-2-7b-hf&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">tok.save_pretrained(&quot;/shared/llama7b_tokenizer&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">EOF</span><br></span></code></pre></div></div>
<p>다음으로, RedPajama-Data-1T-Sample 데이터셋(10억 개의 토큰을 포함하는 전체 RedPajama 데이터셋의 작은 하위 집합)을 다운로드합니다.</p>
<p>CLI 파드에 연결된 상태에서 git을 사용하여 데이터셋을 다운로드합니다</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">cd /shared</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">git clone https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    data/RedPajama-Data-1T-Sample</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="데이터셋-토큰화">데이터셋 토큰화<a href="#데이터셋-토큰화" class="hash-link" aria-label="데이터셋 토큰화에 대한 직접 링크" title="데이터셋 토큰화에 대한 직접 링크" translate="no">​</a></h3>
<p>neuronx-nemo-megatron에 포함된 전처리 스크립트를 사용하여 데이터셋을 토큰화합니다. 이 전처리 단계는 trn1.32xl 인스턴스에서 약 60분이 소요됩니다.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> /shared</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 필요한 스크립트가 포함된 neuronx-nemo-megatron 저장소 클론</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token function" style="color:#d73a49">git</span><span class="token plain"> clone https://github.com/aws-neuron/neuronx-nemo-megatron.git</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 개별 redpajama 파일을 단일 jsonl 파일로   결합</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token function" style="color:#d73a49">cat</span><span class="token plain"> /shared/data/RedPajama-Data-1T-Sample/*.jsonl </span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> /shared/redpajama_sample.jsonl</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># llama 토크나이저를 사용하여 전처리 스크립트 실행</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">python3 neuronx-nemo-megatron/nemo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token parameter variable" style="color:#36acaa">--input</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">/shared/redpajama_sample.jsonl </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --json-keys</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">text </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --tokenizer-library</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">huggingface </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --tokenizer-type</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">/shared/llama7b_tokenizer </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --dataset-impl</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">mmap </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --output-prefix</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">/shared/data/redpajama_sample </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --append-eod </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    --need-pad-id </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token parameter variable" style="color:#36acaa">--workers</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">32</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="훈련-스크립트에서-데이터셋-및-토크나이저-경로-수정">훈련 스크립트에서 데이터셋 및 토크나이저 경로 수정<a href="#훈련-스크립트에서-데이터셋-및-토크나이저-경로-수정" class="hash-link" aria-label="훈련 스  크립트에서 데이터셋 및 토크나이저 경로 수정에 대한 직접 링크" title="훈련 스크립트에서 데이터셋 및 토크나이저 경로 수정에 대한 직접 링크" translate="no">​</a></h3>
<p>참고: 나중에 EKS에서 훈련 작업을 시작할 때 훈련 파드는 FSx의 neuronx-nemo-megatron/nemo/examples 디렉토리에서 훈련 스크립트를 실행합니다. 이는 모든 변경 사항에 대해 neuronx-nemo-megatron 컨테이너를 다시 빌드하지 않고도 FSx에서 직접 훈련 스크립트를 수정할 수 있어 편리합니다.</p>
<p>test_llama.sh 스크립트 <code>/shared/neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/test_llama.sh</code>를 수정하여 다음 두 줄을 업데이트합니다. 이 줄들은 훈련 파드 워커에게 FSx 파일 시스템에서 Llama 토크나이저와 데이터셋을 찾을 위치를 알려줍니다.</p>
<p>실행:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">sed</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">-i</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;s#^\(: ${TOKENIZER_PATH=\).*#\1/shared/llama7b_tokenizer}#&#x27;</span><span class="token plain"> /shared/neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/test_llama.sh</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token function" style="color:#d73a49">sed</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">-i</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;s#^\(: ${DATASET_PATH=\).*#\1/shared/data/redpajama_sample_text_document}#&#x27;</span><span class="token plain"> /shared/neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/test_llama.sh</span><br></span></code></pre></div></div>
<p>변경 전:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">: ${TOKENIZER_PATH=$HOME/llamav2_weights/7b-hf}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">: ${DATASET_PATH=$HOME/examples_datasets/llama_7b/book.jsonl-processed_text_document}</span><br></span></code></pre></div></div>
<p>변경 후:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">: ${TOKENIZER_PATH=/shared/llama7b_tokenizer}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">: ${DATASET_PATH=/shared/data/redpajama_sample_text_document}</span><br></span></code></pre></div></div>
<p>nano에서 변경 사항을 저장하려면 <code>CTRL-X</code>를 누른 다음 <code>y</code>를 누르고 <code>ENTER</code>를 누릅니다.</p>
<p>완료되면 <code>exit</code>를 입력하거나 <code>CTRL-d</code>를 눌러 CLI 파드를 종료합니다.</p>
<p>CLI 파드가 더 이상 필요하지 않으면 다음을 실행하여 제거할 수 있습니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl delete pod cli-cmd-shell</span><br></span></code></pre></div></div>
<p>이제 사전 컴파일 및 훈련 작업을 시작할 준비가 되었습니다!</p>
<p>먼저 다음 명령을 실행하여 MPI 오퍼레이터가 제대로 작동하는지 확인합니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get all </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> mpi-operator</span><br></span></code></pre></div></div>
<p>MPI Operator가 설치되어 있지 않으면 진행하기 전에 <a href="https://github.com/kubeflow/mpi-operator#installation" target="_blank" rel="noopener noreferrer">MPI Operator 설치 지침</a>을 따르세요.</p>
<p>훈련 작업을 실행하기 전에 먼저 모델 아티팩트를 준비하기 위해 사전 컴파일 작업을 실행합니다. 이 단계는 Llama-2-7b 모델의 기본 컴퓨트 그래프를 추출하고 컴파일하여 Trainium 가속기에서 실행할 수 있는 Neuron 실행 파일(NEFF)을 생성합니다. 이러한 NEFF는 FSx의 영구 Neuron 캐시에 저장되어 나중에 훈련 작업에서 접근할 수 있습니다.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="사전-컴파일-작업-실행">사전 컴파일 작업 실행<a href="#사전-컴파일-작업-실행" class="hash-link" aria-label="사전 컴파일 작업 실행에 대한 직접 링크" title="사전 컴파일 작업 실행에 대한 직접 링크" translate="no">​</a></h3>
<p>사전 컴파일 스크립트 실행</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">./3-llama2-neuronx-mpi-compile.sh</span><br></span></code></pre></div></div>
<p>4개의 trn1.32xlarge 노드를 사용할 때 사전 컴파일은 약 10분이 소요됩니다.</p>
<p><code>kubectl get pods | grep compile</code>을 주기적으로 실행하고 컴파일 작업이 &#x27;Completed&#x27;로 표시될 때까지 기다립니다.</p>
<p>사전 컴파일이 완료되면 다음 스크립트를 실행하여 4개의 trn1.32xl 노드에서 사전 훈련 작업을 시작할 수 있습니다:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="훈련-작업-실행">훈련 작업 실행<a href="#훈련-작업-실행" class="hash-link" aria-label="훈련 작업 실행에 대한 직접 링크" title="훈련 작업 실행에 대한 직접 링크" translate="no">​</a></h3>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">./4-llama2-neuronx-mpi-train.sh</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="훈련-작업-출력-보기">훈련 작업 출력 보기<a href="#훈련-작업-출력-보기" class="hash-link" aria-label="훈련 작업 출력 보기에 대한 직접 링크" title="훈련 작업 출력 보기에 대한 직접 링크" translate="no">​</a></h3>
<p>훈련 작업 출력을 모니터링하려면 먼저 훈련 작업과 연결된 런처 파드의 이름을 찾습니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get pods </span><span class="token operator" style="color:#393A34">|</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">grep</span><span class="token plain"> launcher</span><br></span></code></pre></div></div>
<p>런처 파드의 이름을 확인하고 &#x27;Running&#x27; 상태인 것을 확인한 후 다음 단계는 UID를 확인하는 것입니다. 다음 명령에서 test-mpi-train-launcher-xxx를 실제 런처 파드 이름으로 대체하면 UID가 출력됩니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get pod test-mpi-train-launcher-xxx </span><span class="token parameter variable" style="color:#36acaa">-o</span><span class="token plain"> json </span><span class="token operator" style="color:#393A34">|</span><span class="token plain"> jq </span><span class="token parameter variable" style="color:#36acaa">-r</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;.metadata.uid&quot;</span><br></span></code></pre></div></div>
<p>UID를 사용하여 로그 경로를 확인하고 훈련 로그를 tail할 수 있습니다. 다음 명령에서 <code>UID</code>를 위의 값으로 대체하세요.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl </span><span class="token builtin class-name">exec</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">-it</span><span class="token plain"> test-mpi-train-worker-0 -- </span><span class="token function" style="color:#d73a49">tail</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> /shared/nemo_experiments/</span><span class="token environment constant" style="color:#36acaa">UID</span><span class="token plain">/0/log</span><br></span></code></pre></div></div>
<p>로그 확인이 완료되면 <code>CTRL-C</code>를 눌러 tail 명령을 종료할 수 있습니다.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="trainium-가속기-활용률-모니터링">Trainium 가속기 활용률 모니터링<a href="#trainium-가속기-활용률-모니터링" class="hash-link" aria-label="Trainium 가속기 활용률 모니터링에 대한 직접 링크" title="Trainium 가속기 활용률 모니터링에 대한 직접 링크" translate="no">​</a></h3>
<p>Trainium 가속기 활용률을 모니터링하려면 neuron-top 명령을 사용할 수 있습니다. Neuron-top은 trn1/inf2/inf1 인스턴스에서 Neuron 및 시스템 관련 성능 메트릭을 모니터링하기 위한 콘솔 기반 도구입니다. 다음과 같이 워커 파드 중 하나에서 neuron-top을 시작 할 수 있습니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl </span><span class="token builtin class-name">exec</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">-it</span><span class="token plain"> test-mpi-train-worker-0 -- /bin/bash </span><span class="token parameter variable" style="color:#36acaa">-l</span><span class="token plain"> neuron-top</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="tensorboard에서-훈련-작업-메트릭-보기">TensorBoard에서 훈련 작업 메트릭 보기<a href="#tensorboard에서-훈련-작업-메트릭-보기" class="hash-link" aria-label="TensorBoard에서 훈련 작업 메트릭 보기에 대한 직접 링크" title="TensorBoard에서 훈련 작업 메트릭 보기에 대한 직접 링크" translate="no">​</a></h3>
<p><a href="https://www.tensorflow.org/tensorboard" target="_blank" rel="noopener noreferrer">TensorBoard</a>는 훈련 작업을 모니터링하고 탐색하는 데 일반적으로 사용되는 웹 기반 시각화 도구입니다. 훈련 메트릭을 빠르게 모니터링할 수 있으며 서로 다른 훈련 실행 간의 메트릭을 쉽게 비교할 수도 있습니다.</p>
<p>TensorBoard 로그는 FSx for Lustre 파일 시스템의 /shared/nemo_experiments/ 디렉토리에서 사용할 수 있습니다.</p>
<p>다음 스크립트를 실행하여 Llama-2 훈련 작업 진행 상황을 시각화할 수 있는 TensorBoard 배포를 생성합니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">./5-deploy-tensorboard.sh</span><br></span></code></pre></div></div>
<p>배포가 준비되면 스크립트는 새 TensorBoard 배포에 대한 암호로 보호된 URL을 출력합니다.</p>
<p>URL을 실행하여 훈련 진행 상황을 확인합니다.</p>
<p>TensorBoard 인터페이스를 열면 왼쪽 메뉴에서 훈련 작업 UID를 선택한 다음 메인 애플리케이션 창에서 다양한 훈련 메트릭(예: reduced-train-loss, throughput, grad-norm)을 탐색합니다.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="훈련-작업-중지">훈련 작업 중지<a href="#훈련-작업-중지" class="hash-link" aria-label="훈련 작업 중지에 대한 직접 링크" title="훈련 작업 중지에 대한 직접 링크" translate="no">​</a></h3>
<p>훈련 작업을 중지하고 런처/워커 파드를 제거하려면 다음 명령을 실행합니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl delete mpijob test-mpi-train</span><br></span></code></pre></div></div>
<p>그런 다음 <code>kubectl get pods</code>를 실행하여 런처/워커 파드가 제거되었는지 확인할 수 있습니다.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="정리">정리<a href="#정리" class="hash-link" aria-label="정리에 대한 직접 링크" title="정리에 대한 직접 링크" translate="no">​</a></h3>
<p>이 솔루션을 사용하여 생성된 리소스를 제거하려면 정리 스크립트를 실행합니다:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> ai-on-eks/infra/trainium-inferentia</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">./cleanup.sh</span><br></span></code></pre></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/awslabs/ai-on-eks/blob/main/website/docs/blueprints/training/Neuron/Llama2.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>페이지 편집</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="문서 페이지"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai-on-eks/ko/docs/blueprints/training/Neuron/RayTrain-Llama2"><div class="pagination-nav__sublabel">이전</div><div class="pagination-nav__label">RayTrain을 활용한 Trn1에서의 Llama-2</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai-on-eks/ko/docs/blueprints/training/Neuron/BERT-Large"><div class="pagination-nav__sublabel">다음</div><div class="pagination-nav__label">Trainium에서의 BERT-Large</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#llama-2란" class="table-of-contents__link toc-highlight">Llama-2란?</a></li><li><a href="#어떤-llama-2-모델-크기를-사용해야-하나요" class="table-of-contents__link toc-highlight"><strong>어떤 Llama-2 모델 크기를 사용해야 하나요?</strong></a></li><li><a href="#솔루션-아키텍처" class="table-of-contents__link toc-highlight">솔루션 아키텍처</a></li><li><a href="#솔루션-배포" class="table-of-contents__link toc-highlight">솔루션 배포</a><ul><li><a href="#리소스-확인" class="table-of-contents__link toc-highlight">리소스 확인</a></li></ul></li><li><a href="#분산-훈련" class="table-of-contents__link toc-highlight">분산 훈련</a><ul><li><a href="#neuronx-nemo-megatron-컨테이너-이미지-빌드" class="table-of-contents__link toc-highlight">neuronx-nemo-megatron 컨테이너 이미지 빌드</a></li><li><a href="#cli-파드-시작-및-연결" class="table-of-contents__link toc-highlight">CLI 파드 시작 및 연결</a></li><li><a href="#llama-토크나이저-및-redpajama-데이터셋을-fsx에-다운로드" class="table-of-contents__link toc-highlight">Llama 토크나이저 및 Redpajama 데이터셋을 FSx에 다운로드</a></li><li><a href="#데이터셋-토큰화" class="table-of-contents__link toc-highlight">데이터셋 토큰화</a></li><li><a href="#훈련-스크립트에서-데이터셋-및-토크나이저-경로-수정" class="table-of-contents__link toc-highlight">훈련 스크립트에서 데이터셋 및 토크나이저 경로 수정</a></li><li><a href="#사전-컴파일-작업-실행" class="table-of-contents__link toc-highlight">사전 컴파일 작업 실행</a></li><li><a href="#훈련-작업-실행" class="table-of-contents__link toc-highlight">훈련 작업 실행</a></li><li><a href="#훈련-작업-출력-보기" class="table-of-contents__link toc-highlight">훈련 작업 출력 보기</a></li><li><a href="#trainium-가속기-활용률- 모니터링" class="table-of-contents__link toc-highlight">Trainium 가속기 활용률 모니터링</a></li><li><a href="#tensorboard에서-훈련-작업-메트릭-보기" class="table-of-contents__link toc-highlight">TensorBoard에서 훈련 작업 메트릭 보기</a></li><li><a href="#훈련-작업-중지" class="table-of-contents__link toc-highlight">훈련 작업 중지</a></li><li><a href="#정리" class="table-of-contents__link toc-highlight">정리</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">참여하기</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/awslabs/ai-on-eks" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Built with ❤️ at AWS  <br> © ${new Date().getFullYear()} Amazon.com, Inc. or its affiliates. All Rights Reserved</div></div></div></footer><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;7fbc7ab02fae4767b1af2588eba0cdf2&quot;}"></script></div>
</body>
</html>
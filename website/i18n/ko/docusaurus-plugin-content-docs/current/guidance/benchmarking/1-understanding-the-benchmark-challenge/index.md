---
sidebar_label: 벤치마크 과제 이해하기
---

# 벤치마크 과제 이해하기

더 많은 조직이 자체 인프라에 대규모 언어 모델(LLM)을 배포함에 따라 성능 측정 방법을 이해하는 것이 공통적인 과제가 되었습니다.

고객들은 종종 다음과 같은 질문을 합니다: 셀프 호스팅 LLM을 어떻게 벤치마킹해야 하나요? 어떤 메트릭이 중요한가요? 그리고 그 숫자들이 실제 워크로드에 대해 무엇을 의미하나요? LLM 벤치마킹은 기존 AI 모델을 테스트하는 것처럼 간단하지 않습니다. LLM은 수십억 개의 매개변수를 가지고 있으며 성능은 하드웨어 설정, 메모리 대역폭, 양자화, KV 캐시 동작, 병렬화 전략 등 많은 요인에 따라 달라집니다. 구성, 프롬프트 길이 또는 사용자 동작의 작은 변화도 처리량과 지연 시간에 큰 차이를 만들 수 있습니다. 프로덕션 워크로드는 예기치 않게 변할 수 있으며, 테스트하지 않았던 상황에서 사용자가 갑자기 컨텍스트 길이를 최대로 사용하면 성능이 크게 저하됩니다.

사용 사례에 대해 허용 가능한 응답 품질을 달성한 후에는 성능이 다음으로 중요한 관심사가 됩니다. 모델이 사용자 기대를 충족할 만큼 빠르게 응답할 수 있나요? 그리고 10명의 동시 사용자든 10,000명의 동시 사용자든 실제 워크로드를 처리할 수 있도록 확장할 수 있나요? 이러한 질문은 사용자 경험, 인프라 비용 및 배포의 실행 가능성에 직접적인 영향을 미칩니다.

명확한 벤치마킹 프레임워크가 없으면 팀은 하드웨어 옵션을 비교하고, 모델을 효율적으로 튜닝하거나, 배포 비용을 예측하는 데 어려움을 겪습니다. 이 가이드는 추론 성능 벤치마킹에 초점을 맞추고 있으며, 고객이 배포 구성을 최적화하고, 주요 메트릭을 실제로 이해하며, 추론 성능을 분석하고 개선하기 위한 체계적인 접근 방식을 구현할 수 있도록 처리량, 지연 시간 및 리소스 활용도를 측정합니다.
